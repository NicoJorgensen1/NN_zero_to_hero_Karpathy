{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# makemore: part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dataset and i2s <-> s2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import math \n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt         # for making figures\n",
    "%matplotlib inline\n",
    "from typing import Union, Dict, Tuple, List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have read the 32033 words from the file. Here we display the first 8 words\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# Here we will import the list with all the words/names \n",
    "import os \n",
    "words_txt_file_path = os.path.join(os.path.expanduser(\"~\"), \"NN_zero_to_hero\", \"Lectures\", \"Makemore_repo\", \"names.txt\")\n",
    "words = open(words_txt_file_path, 'r').read().splitlines()\n",
    "\n",
    "print(f\"Now we have read the {len(words)} words from the file. Here we display the first 8 words\\n{words[:8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have created the mappings between chars and integers and vice versa, which yields to 27 different, possible characters (i.e. classes) in our dataset:\n",
      "s_to_i={'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "i_to_s={1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "### Here we are building the vocabulary and mapping from char to integer and vice versa \n",
    "chars = sorted(list(set(''.join(words))))           # Read all unique characters in all words \n",
    "s_to_i = {s:i+1 for i,s in enumerate(chars)}        # Create a mapping from char to index integer\n",
    "s_to_i['.'] = 0                                     # Assign the index of our special start/end token '.' to 0\n",
    "i_to_s = {i:s for s,i in s_to_i.items()}            # Reverse the mapping from integer to char \n",
    "num_classes = len(s_to_i)                           # Read the different number of characters available in our dataset \n",
    "vocab_size = num_classes                            # Vocabulary = number of different tokens available for the model \n",
    "\n",
    "print(f\"Now we have created the mappings between chars and integers and vice versa, which yields to {num_classes} different, possible characters (i.e. classes) in our dataset:\\n{s_to_i=}\\n{i_to_s=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build the dataset\n",
    "\n",
    "# Create the dataset arrays from the words list\n",
    "def build_dataset(words, s2i: Dict = s_to_i, block_size: int = 3, num_words: Union[int, None] = None, ):  \n",
    "  X, Y = [], []\n",
    "  for w in (words if num_words is None else words[:num_words] if isinstance(words[:num_words], list) else [words[:num_words]]): \n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = s2i[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix]                              # crop and append the next character to the context\n",
    "  return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "# Print metadata \n",
    "random.seed(42)                                                 # Choose a seed for deterministic shuffling \n",
    "random.shuffle(words)                                           # inplace shuffling of the words list \n",
    "n1 = int(0.8*len(words))                                        # Extract an integer for the 80% of the dataset \n",
    "n2 = int(0.9*len(words))                                        # Extract an integer for the 90% of the dataset \n",
    "\n",
    "block_size = 3                                                  # context length: how many characters do we take to predict the next one?\n",
    "Xtr,  Ytr  = build_dataset(words[:n1], block_size=block_size)   # Build the training split \n",
    "Xdev, Ydev = build_dataset(words[n1:n2], block_size=block_size) # Build the validation (dev) split \n",
    "Xte,  Yte  = build_dataset(words[n2:], block_size=block_size)   # Build the test split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for generating model parameters and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This means that with the embedding dimension 2 and a two layer MLP with 100 in each layer, we end up at 3681 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "##### Generate parameters function \n",
    "\n",
    "# Generate the parameters of the model \n",
    "def generate_parameters(random_seed: int = 2147483648, emb_dim: int = 2, num_hidden_neurons: int = 100, block_size: int = 3, num_classes: int = num_classes, \n",
    "                        return_generator: bool = False, emb_scale: float = 1.0, W1_scale: float = 1.0, W2_scale: float = 1.0, b1_scale: float = 1.0, b2_scale: float = 1.0,\n",
    "                        emb_offset: float = 0.0, W1_offset: float = 0.0, W2_offset: float = 0.0, b1_offset: float = 0.0, b2_offset: float = 0.0, use_BN: bool = True) -> Tuple[torch.tensor]:\n",
    "    g = torch.Generator().manual_seed(random_seed)          # for reproducibility and deterministic generation \n",
    "    \n",
    "    # Create all the layers and trainable parameters. Scale them according to the chosen hyperparameters parameters\n",
    "    C = torch.randn((num_classes, emb_dim), generator=g) * emb_scale + emb_offset\n",
    "    W1 = torch.randn((emb_dim*block_size, num_hidden_neurons), generator=g) * W1_scale + W1_offset \n",
    "    b1 = torch.randn(num_hidden_neurons, generator=g) * b1_scale + b1_offset \n",
    "    W2 = torch.randn((num_hidden_neurons, num_classes), generator=g) * W2_scale + W2_offset \n",
    "    b2 = torch.randn(num_classes, generator=g) * b2_scale + b2_offset \n",
    "\n",
    "    # BatchNorm parameters\n",
    "    bn_gain = torch.ones((1, num_hidden_neurons))                       # Initiate the running gain to be ones \n",
    "    bn_bias = torch.zeros((1, num_hidden_neurons))                      # Initiate the running bias to be zeros \n",
    "    bn_mean_running = torch.zeros((1, num_hidden_neurons))              # Initiate the running mean to be zeros \n",
    "    bn_std_running = torch.ones((1, num_hidden_neurons))                # Initiate the running std to be ones \n",
    "\n",
    "    # Assure all parameters have set \"required_grad\" to True and return the parameters \n",
    "    for p in [C, W1, b1, W2, b2] if not use_BN else [C, W1, b1, W2, b2, bn_gain, bn_bias]:\n",
    "        p.requires_grad = True\n",
    "    return_list = [C, W1, b1, W2, b2]\n",
    "    if use_BN:\n",
    "        return_list.extend([bn_gain, bn_bias, bn_mean_running, bn_std_running])\n",
    "    if return_generator:\n",
    "        return_list.append(g)\n",
    "    return tuple(return_list)\n",
    "\n",
    "\n",
    "# Compute the total number of parameters in our network \n",
    "C, W1, b1, W2, b2, bn_gain, bn_bias, bn_mean_running, bn_std_running = generate_parameters()\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "num_parameters = sum(p.nelement() for p in parameters)          # number of parameters in total\n",
    "print(f\"This means that with the embedding dimension {C.shape[1]} and a two layer MLP with {W1.shape[1]} in each layer, we end up at {num_parameters} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define the training function\n",
    "\n",
    "# This we will use to both showcase three examples:\n",
    "# 1) How to perform a learning rate search? i.e. we will search for the proper learning rate \n",
    "# 2) What happens when we overfit with just the 32 sequences from our X array, i.e. to check that the model is capable of learning\n",
    "# 3) Training the model on the entire dataset \n",
    "\n",
    "\n",
    "\n",
    "def train_func(inp_data: torch.tensor, targets: torch.tensor, emb_matrix: torch.tensor = C, batch_size: int = 32, num_iterations: int = 20,\n",
    "        lr_scheduler: Union[torch.tensor, None] = None, lr: Union[torch.tensor, None] = 1e-4, regenerate_parameters: bool = True, use_batch_norm: bool = False, **kwargs) -> Tuple[List, List, List, List]:\n",
    "    # Initiating lists to store the data that we want to keep \n",
    "    lri = []\n",
    "    lossi = []\n",
    "    stepi = []\n",
    "\n",
    "    # Regenerate the parameters of the network at random state \n",
    "    emb_matrix, W1, b1, W2, b2, g_and_BN = generate_parameters(return_generator=True, use_BN=use_batch_norm, **kwargs)\n",
    "    parameters = [emb_matrix, W1, b1, W2, b2]\n",
    "    if use_batch_norm:\n",
    "        g, bn_gain, bn_bias, bn_mean_running, bn_std_running = g_and_BN\n",
    "        parameters.extend([bn_gain, bn_bias])\n",
    "    else:\n",
    "        g = g_and_BN \n",
    "\n",
    "    # Iterating through all the selected iterations \n",
    "    if lr_scheduler is None:\n",
    "        lr_scheduler = torch.full((num_iterations,), lr)                        # Repeat the provided learning rate for all iterations \n",
    "    for i in range(num_iterations):\n",
    "        ### Forward pass \n",
    "        # Create a single mini batch from the dataset \n",
    "        idx = torch.randint(0, inp_data.shape[0], (batch_size,), generator=g)   # Create integer index array for creating stochastic mini-batches \n",
    "        emb = emb_matrix[inp_data[idx]]                                         # (32, 3, 10) --> Read the embedding vectors of the randomly selected examples for the current iteration \n",
    "        emb_cat = emb.view(-1, emb.shape[1]*emb.shape[2])                       # Concatenate the last 'block_size' characters together in an array of sequences \n",
    "\n",
    "        # Linear layer \n",
    "        h_pre_act_func = emb_cat @ W1 + b1                                      # (32, 200)   --> Compute the output of the first hidden layer of our MLP model \n",
    "\n",
    "        # BatchNorm layer\n",
    "        # -------------------------------------------------------------\n",
    "        if use_batch_norm:\n",
    "            bn_mean_i = h_pre_act_func.mean(0, keepdim=True) \n",
    "            bn_std_i = h_pre_act_func.std(0, keepdim=True)\n",
    "            h_pre_act_func = bn_gain * (h_pre_act_func-bn_mean_i) / bn_std_i + bn_bias \n",
    "\n",
    "            with torch.no_grad():\n",
    "                bn_mean_running = 0.999 * bn_mean_running + 0.001 * bn_mean_i\n",
    "                bn_std_running = 0.999 * bn_std_running + 0.001 * bn_std_i \n",
    "\n",
    "        # Non-linearity \n",
    "        h_act_func = torch.tanh(h_pre_act_func)                                 # (32, 200)   --> Compute the\n",
    "        logits = h_act_func @ W2 + b2                                                    # (32, 27)    --> Compute the logits of the forward pass \n",
    "        loss = F.cross_entropy(logits, targets[idx])                            # Compute the NLL loss (equal to the cross entropy) using the efficient pytorch implementation \n",
    "\n",
    "        \n",
    "        ### Backward pass \n",
    "        # According to Karparthy, setting the gradient to None is more efficient than p.grad = 0, when using PyTorch .backward() method \n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()                                 # Compute the new gradients for all parameters \n",
    "\n",
    "        # Update all the parameters \n",
    "        lr = lr_scheduler[i]\n",
    "        for p in parameters:\n",
    "            p.data += -lr *p.grad \n",
    "        \n",
    "        # Track the stats\n",
    "        lri.append(lr)\n",
    "        stepi.append(i)\n",
    "        lossi.append(loss.log10().item())\n",
    "    \n",
    "    # Return the lists of stats\n",
    "    return lri, stepi, lossi, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train again, with different setups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10                 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200              # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "\n",
    "# Generate initial parameters for the model \n",
    "C, W1, b1, W2, b2 = generate_parameters(emb_dim=10, num_hidden_neurons=200)\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnstd = hpreact.std(0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The split loss will be computed without gradients \n",
    "## The decorator will make these computations much more efficient, as we are telling PyTorch to not create computational graphs under the hood, for later gradient calculations, as we are not going to use them ... \n",
    "\n",
    "@torch.no_grad()        # this decorator disables gradient tracking => similar to \"with torch.no_grad():\" ...\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss log\n",
    "\n",
    "### original:\n",
    "train 2.1245384216308594\n",
    "val   2.168196439743042\n",
    "\n",
    "### fix softmax confidently wrong:\n",
    "train 2.07\n",
    "val   2.13\n",
    "\n",
    "### fix tanh layer too saturated at init:\n",
    "train 2.0355966091156006\n",
    "val   2.1026785373687744\n",
    "\n",
    "### use semi-principled \"kaiming init\" instead of hacky init:\n",
    "train 2.0376641750335693\n",
    "val   2.106989622116089\n",
    "\n",
    "### add batch norm layer\n",
    "train 2.0668270587921143\n",
    "val 2.104844808578491\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY + PYTORCHIFYING -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a deeper network\n",
    "# The classes we create here are the same API as nn.Module in PyTorch\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "# layers = [\n",
    "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size),\n",
    "# ]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  layers[-1].gamma *= 0.1\n",
    "  #layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for layer in layers:\n",
    "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  with torch.no_grad():\n",
    "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "  if i >= 1000:\n",
    "    break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out.grad\n",
    "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  t = p.grad\n",
    "  if p.ndim == 2:\n",
    "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "# put layers into eval mode\n",
    "for layer in layers:\n",
    "  layer.training = False\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "      for layer in layers:\n",
    "        x = layer(x)\n",
    "      logits = x\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(i_to_s[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE; BONUS content below, not covered in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm forward pass as a widget\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def normshow(x0):\n",
    "  \n",
    "  g = torch.Generator().manual_seed(2147483647+1)\n",
    "  x = torch.randn(5, generator=g) * 5\n",
    "  x[0] = x0 # override the 0th example with the slider\n",
    "  mu = x.mean()\n",
    "  sig = x.std()\n",
    "  y = (x - mu)/sig\n",
    "\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  # plot 0\n",
    "  plt.plot([-6,6], [0,0], 'k')\n",
    "  # plot the mean and std\n",
    "  xx = np.linspace(-6, 6, 100)\n",
    "  plt.plot(xx, stats.norm.pdf(xx, mu, sig), 'b')\n",
    "  xx = np.linspace(-6, 6, 100)\n",
    "  plt.plot(xx, stats.norm.pdf(xx, 0, 1), 'r')\n",
    "  # plot little lines connecting input and output\n",
    "  for i in range(len(x)):\n",
    "    plt.plot([x[i],y[i]], [1, 0], 'k', alpha=0.2)\n",
    "  # plot the input and output values\n",
    "  plt.scatter(x.data, torch.ones_like(x).data, c='b', s=100)\n",
    "  plt.scatter(y.data, torch.zeros_like(y).data, c='r', s=100)\n",
    "  plt.xlim(-6, 6)\n",
    "  # title\n",
    "  plt.title('input mu %.2f std %.2f' % (mu, sig))\n",
    "\n",
    "interact(normshow, x0=(-30,30,0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear: activation statistics of forward and backward pass\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "a = torch.randn((1000,1), requires_grad=True, generator=g)          # a.grad = b.T @ c.grad\n",
    "b = torch.randn((1000,1000), requires_grad=True, generator=g)       # b.grad = c.grad @ a.T\n",
    "c = b @ a\n",
    "loss = torch.randn(1000, generator=g) @ c\n",
    "a.retain_grad()\n",
    "b.retain_grad()\n",
    "c.retain_grad()\n",
    "loss.backward()\n",
    "print('a std:', a.std().item())\n",
    "print('b std:', b.std().item())\n",
    "print('c std:', c.std().item())\n",
    "print('-----')\n",
    "print('c grad std:', c.grad.std().item())\n",
    "print('a grad std:', a.grad.std().item())\n",
    "print('b grad std:', b.grad.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear + BatchNorm: activation statistics of forward and backward pass\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "n = 1000\n",
    "# linear layer ---\n",
    "inp = torch.randn(n, requires_grad=True, generator=g)\n",
    "w = torch.randn((n, n), requires_grad=True, generator=g) # / n**0.5\n",
    "x = w @ inp\n",
    "# bn layer ---\n",
    "xmean = x.mean()\n",
    "xvar = x.var()\n",
    "out = (x - xmean) / torch.sqrt(xvar + 1e-5)\n",
    "# ----\n",
    "loss = out @ torch.randn(n, generator=g)\n",
    "inp.retain_grad()\n",
    "x.retain_grad()\n",
    "w.retain_grad()\n",
    "out.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "print('inp std: ', inp.std().item())\n",
    "print('w std: ', w.std().item())\n",
    "print('x std: ', x.std().item())\n",
    "print('out std: ', out.std().item())\n",
    "print('------')\n",
    "print('out grad std: ', out.grad.std().item())\n",
    "print('x grad std: ', x.grad.std().item())\n",
    "print('w grad std: ', w.grad.std().item())\n",
    "print('inp grad std: ', inp.grad.std().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
