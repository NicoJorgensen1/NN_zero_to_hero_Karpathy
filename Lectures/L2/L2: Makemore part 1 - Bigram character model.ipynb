{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will simply import the list with all the words \n",
    "import os \n",
    "words_txt_file_path = os.path.join(os.path.expanduser(\"~\"), \"NN_zero_to_hero\", \"Lectures\", \"Makemore_repo\", \"names.txt\")\n",
    "words = open(words_txt_file_path, 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 32033 different names. The shortest is 2 characters, where as the longest is 15 characters.\n",
      "The 10 first names in the list are: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 first words (=names) in the list \n",
    "print(f\"We have {len(words)} different names. The shortest is {min(len(w) for w in words)} characters, where as the longest is {max(len(w) for w in words)} characters.\")\n",
    "print(f\"The 10 first names in the list are: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a dictionary that will store all our bigrams\n",
    "b = {}                                      # Initiate a dictionary to store the bigrams \n",
    "for w in words:                             # Iterate over all the words ...\n",
    "  chs = ['<S>'] + list(w) + ['<E>']         # Create a character list, that will be initiated with both the start and end tokens\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):        # Iterating over all characters in the current word ...\n",
    "    bigram = (ch1, ch2)                     # Create the bigram with the pairs of characters\n",
    "    b[bigram] = b.get(bigram, 0) + 1        # Get the value of the b[bigram] (or create it with value 0, if it doesn't exist) and increase by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 32033 names we have created 627 bigrams with 228146 training examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<E>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<S>', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '<E>'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('<S>', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '<E>'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('<S>', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('<S>', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('<S>', 'l'), 1572),\n",
       " (('<S>', 'c'), 1542),\n",
       " (('<S>', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '<E>'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '<E>'), 1314),\n",
       " (('<S>', 't'), 1308),\n",
       " (('<S>', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '<E>'), 1169),\n",
       " (('<S>', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('<S>', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('<S>', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '<E>'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('<S>', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('<S>', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('<S>', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '<E>'), 516),\n",
       " (('d', '<E>'), 516),\n",
       " (('<S>', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '<E>'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('<S>', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('<S>', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('<S>', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '<E>'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('<S>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '<E>'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<E>'), 160),\n",
       " (('u', '<E>'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('<S>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<E>'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '<E>'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '<E>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('<S>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '<E>'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<E>'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<S>', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<E>'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '<E>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '<E>'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<E>'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the dictionary as a sorted dictionary, sorted by the values (counts)\n",
    "print(f\"From the {len(words)} names we have created {len(b)} bigrams with {sum(b.values())} training examples\")\n",
    "sorted(b.items(), key = lambda kv: kv[1], reverse=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an array of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function capable of plotting our array of counts \n",
    "from typing import Union, List, Optional, Tuple, Dict \n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_counts(\n",
    "        N_matrix_list: Union[List[torch.tensor], torch.tensor],\n",
    "        i2s: Optional[Dict[int, str]] = None,\n",
    "        figsize: Tuple[int, int] = (16, 16),\n",
    "        fontsize: int = 10,\n",
    "        fontcolor: str = \"gray\",\n",
    "        colormap: str = \"Blues\",\n",
    "        draw: bool = True,\n",
    "        offset: bool = False,\n",
    "        print_txt: bool = True,\n",
    "        title_list: Optional[List[str]] = None,\n",
    "        tit_fontsize: Optional[int] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a matrix showcasing bigrams using the provided N_matrix or a list of N_matrices.\n",
    "\n",
    "    This function visualizes bigram counts stored in the N_matrix or a list of N_matrices by plotting a heatmap for each matrix.\n",
    "    The function allows customizations such as adjusting the figure size, font size, font color, colormap, and displaying text on the heatmap cells.\n",
    "\n",
    "    Parameters:\n",
    "        N_matrix_list (Union[List[np.ndarray], np.ndarray]): A list of NumPy arrays or a single NumPy array containing bigram counts.\n",
    "            Each matrix represents bigram counts, where rows correspond to the first character and columns correspond to the second character.\n",
    "        i2s (Optional[Dict[int, str]], default=None): A dictionary mapping integer indices to character strings.\n",
    "            It is used to convert integer indices to character bigrams for displaying text on the heatmap cells.\n",
    "            If not provided, the function will use default integer-to-character mapping.\n",
    "        figsize (Tuple[int, int], default=(16, 16)): A tuple representing the size of the figure in inches (width, height).\n",
    "            It controls the overall size of the plot containing all the individual heatmaps.\n",
    "        fontsize (int, default=10): The font size for the text displayed on the heatmap cells.\n",
    "        fontcolor (str, default=\"gray\"): The color of the text displayed on the heatmap cells.\n",
    "        colormap (str, default=\"Blues\"): The name of the colormap to use for the heatmaps. It should be a valid matplotlib colormap name.\n",
    "        draw (bool, default=True): A flag indicating whether to draw the plot. If True, the plot will be displayed; if False, the plot will be closed.\n",
    "        offset (bool, default=False): A flag indicating whether the integer-to-character mapping starts from 1 (offset=True) or 0 (offset=False).\n",
    "            If offset=True, the mapping is assumed to have keys starting from 1; otherwise, it starts from 0.\n",
    "        print_txt (bool, default=True): A flag indicating whether to display text on the heatmap cells.\n",
    "            If True, the function will display the character bigrams and their corresponding counts on the heatmap cells.\n",
    "            If False, no text will be displayed on the heatmap cells.\n",
    "        title_list (Optional[List[str]], default=None): A list of strings representing the titles for each individual heatmap.\n",
    "            If provided, the list should have the same length as the N_matrix_list.\n",
    "            Each title represents the label for the corresponding heatmap.\n",
    "        tit_fontsize (Optional[int], default=None): The font size for the heatmap titles.\n",
    "            If provided, it overrides the default font size for the titles.\n",
    "\n",
    "    Returns:\n",
    "        None: This function plots the bigram heatmaps and does not return any value.\n",
    "\n",
    "    Note:\n",
    "        - The function assumes that the input N_matrix_list is either a list of NumPy arrays or a single NumPy array.\n",
    "        - Each matrix in N_matrix_list represents bigram counts, where rows correspond to the first character and columns correspond to the second character.\n",
    "        - The function uses matplotlib to plot the heatmaps, so make sure to have matplotlib installed to use this function.\n",
    "        - If i2s is not provided, the function will use a default integer-to-character mapping, where integers are directly converted to character bigrams (e.g., 0 -> 'a', 1 -> 'b', ...).\n",
    "        - If print_txt is True, the function will display the character bigrams and their corresponding counts on the heatmap cells.\n",
    "        - If title_list is provided, the function will display individual heatmap titles with the specified titles for each subplot.\n",
    "        - If tit_fontsize is provided, it will override the default font size for the heatmap titles.\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)                                       # Create the figure instance \n",
    "    if not isinstance(N_matrix_list, list):                                 # If the provided N_matrix is not a list ...\n",
    "        N_matrix_list = [N_matrix_list]                                     # ... insert it into a list \n",
    "    for idx, N_matrix in enumerate(N_matrix_list):                          # Iterating through the list of matrices \n",
    "        fig.add_subplot(1,len(N_matrix_list),idx+1)                         # Add the first subplot \n",
    "        plt.imshow(N_matrix, cmap=colormap, vmin=0, vmax=N_matrix.max() if N_matrix.max()>1 else 1) # PLot the N_matrix showcasing the bigram as a heatmap \n",
    "        # Print the count/probabilities on each of the heatmap cells \n",
    "        if print_txt:\n",
    "            for i in range(N_matrix.shape[1]):\n",
    "                for j in range(N_matrix.shape[0]):\n",
    "                    chstr = i2s[i+1 if offset else i] + i2s[j+1 if offset else j]\n",
    "                    num_to_write = f\"{N_matrix[i, j].item():d}\" if isinstance(N_matrix[i, j].item(), int) else f\"{N_matrix[i, j].item():.2f}\"\n",
    "                    plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=fontcolor, fontsize=fontsize)\n",
    "                    plt.text(j, i, num_to_write, ha=\"center\", va=\"top\", color=fontcolor, fontsize=fontsize)\n",
    "        plt.axis('off')\n",
    "        # Add the title for the heatmap \n",
    "        if title_list is not None:\n",
    "            plt.title(title_list[idx], fontsize=tit_fontsize)\n",
    "    fig.tight_layout()\n",
    "    if draw:\n",
    "        plt.draw()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have the initial character bigram count matrix of size torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# At first we will count how many unique characters are present in our words_list \n",
    "words_list_in_string = \"\".join(words)                                       # This will be a massive string of all lower case characters from all 32000 words\n",
    "unique_character_list = sorted(list(set(words_list_in_string)))             # This will convert the string into a list of unique characters\n",
    "num_special_tokens = 2                                                      # We start with our 2 special tokens <S> and <E> for \"start\" and \"end\" of sequence \n",
    "N_init_dim = len(unique_character_list) + num_special_tokens                # Compute the side dimension of our character count matrix \n",
    "N_init = torch.zeros((N_init_dim, N_init_dim), dtype=torch.int32)           # Initiate a 28x28 matrix of zeros, each will \n",
    "print(f\"We now have the initial character bigram count matrix of size {N_init.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the dictionary mapping from character to index:\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '<S>': 27, '<E>': 28}\n"
     ]
    }
   ],
   "source": [
    "### As we just cannot insert characters into our model, we need some way of quantizing the characters\n",
    "# That we will do by making a dictionary mapping from character to index\n",
    "s_to_i_init = {s:i+1 for i,s in enumerate(unique_character_list)}           # Enumerate and create a dictionary mapping each character into a index parameter \n",
    "s_to_i_init[\"<S>\"] = len(s_to_i_init)+1                                     # Assign the special start token to the dictionary \n",
    "s_to_i_init[\"<E>\"] = len(s_to_i_init)+1                                     # Assign the special end token to the dictionary \n",
    "i_to_s_init = {i:s for s,i in s_to_i_init.items()}                          # Reverse the dictionary, so we now have a index_2_char mapping\n",
    "print(f\"This is the dictionary mapping from character to index:\\n{s_to_i_init}\")\n",
    "\n",
    "### Now we will fill out the N_init matrix array \n",
    "for bigram, count in b.items():\n",
    "    row_index = s_to_i_init[bigram[0]] -1 \n",
    "    col_index = s_to_i_init[bigram[1]] -1\n",
    "    N_init[row_index][col_index] = count\n",
    "\n",
    "### Now we are going to plot our N_init array matrix \n",
    "# When drawing the N_init matrix, notice that the bottom <E> row is negative, as no bigram will every start with the end token, as well as the second last column, with <S> is all zeroes, as no bigram will end with the start token\n",
    "plot_counts(N_matrix_list=N_init, i2s=i_to_s_init, figsize=(45,45), fontsize=30, fontcolor=\"gray\", colormap=\"jet\", draw=False, offset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will therefore remove the special <S> and <E> tokens in favour of a single token describing both the start and end token.\n",
    "# In order to get nice plottings we will simply use the '.' token \n",
    "s_to_i = {s:i+1 for i,s in enumerate(unique_character_list)}                # Enumerate and create a dictionary mapping each character into a index parameter \n",
    "s_to_i[\".\"] = 0                                                             # Assign now only the '.' as our special token, which will denote both start and end of our sequences and put it at the BEGINNING of our dict \n",
    "i_to_s = {i:s for s,i in s_to_i.items()}                                    # Reverse the dictionary to create a mapping from indices to characters \n",
    "\n",
    "# Create a new N_array matrix with our counts and the '.' as the special token \n",
    "N_dim = len(s_to_i)\n",
    "N = torch.zeros((N_dim, N_dim), dtype=torch.int32)\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = s_to_i[ch1] \n",
    "    ix2 = s_to_i[ch2] \n",
    "    N[ix1, ix2] += 1\n",
    "\n",
    "\n",
    "### Now we are plotting the new, better N matrix \n",
    "plot_counts(N_matrix_list=N, i2s=i_to_s, figsize=(45,45), fontsize=35, fontcolor=\"gray\", colormap=\"jet\", draw=False, offset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we start with the probability vector: ['0.6064', '0.3033', '0.0903']\n",
      "Hence, we expect 60% of our samples to be 0s, 30% of our samples to be 1s and 10% of our samples to be 2s\n",
      "We sample this vector using the multinomial function:\n",
      "tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,\n",
      "        0, 1, 1, 1])\n",
      "Here we have 61 zeros, 33 ones and 6 twos\n"
     ]
    }
   ],
   "source": [
    "# Define the generator and visualize the use of torch.multinomial()\n",
    "g = torch.Generator().manual_seed(2147483647)                                           # Create a random generator with a deterministic seed \n",
    "p = torch.rand(3, generator=g)                                                          # Randomly create a vector of three samples \n",
    "p = p / p.sum()                                                                         # Normalize the vector, i.e. converting it into a probability vector \n",
    "tensor_sampled = torch.multinomial(p, num_samples=100, replacement=True, generator=g)   # Sample 'num_samples' from the range [0, len(p)] with probabilities given from p. Replacement=True allows to sample the same value more than once \n",
    "\n",
    "# Print sampled results \n",
    "print(f\"Now we start with the probability vector: {[f'{x.item():.4f}' for x in p]}\")\n",
    "print(f\"Hence, we expect 60% of our samples to be 0s, 30% of our samples to be 1s and 10% of our samples to be 2s\")\n",
    "print(f\"We sample this vector using the multinomial function:\\n{tensor_sampled}\")\n",
    "print(f\"Here we have {(tensor_sampled==0).sum()} zeros, {(tensor_sampled==1).sum()} ones and {(tensor_sampled==2).sum()} twos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we want to convert the rows of the first row of N into a vector of probabilities\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "p = N[0].float() / N[0].sum()                                                   # Convert the first row of N into a probability vector\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()  # Sample 'num_samples' of the range [1, len(p)] using the probabilities from the probability vector p. \n",
    "i_to_s[ix]                                                                      # Convert the sampled index into a character string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "janasah\n",
      "p\n",
      "cony\n",
      "a\n",
      "\n",
      "nzqfjiirltozcogsjgwzvudlhnpauyjbilevhajkdbduinrwibtlzsnjyievyvaftbzffvmumthyfodtumjrpfytszwjhrjagq\n",
      "coreaysezocfkyjjabdywejfmoifmwyfinwagaasnhsvfihofszxhddgosfmptpagicz\n",
      "rjpiufmthdt\n",
      "rkrrsru\n",
      "iyumuyfy\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Union\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "\n",
    "### Now we can start to sample from our \"model\" \n",
    "def sample_from_model(num_samples: int = 10, uniform_probs: bool = False, i2s: Dict = i_to_s, N_matrix: Union[torch.tensor, None] = N, P_matrix: Union[torch.tensor, None] = None) -> None: \n",
    "    \"\"\"\n",
    "    Sample words from a model output using random integer sampling and a character mapping dictionary.\n",
    "\n",
    "    This function generates random words by sampling random integers between 0 and 26 (both inclusive) from the model's output probabilities.\n",
    "    The sampled integers are then converted into character strings using a predefined mapping dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        num_samples (int, optional): The number of words to sample. Defaults to 10.\n",
    "        uniform_probs (bool, optional):     A flag indicating whether to use uniform probabilities for sampling.\n",
    "                                                If True, each integer is equally likely to be sampled, ignoring model output probabilities.\n",
    "                                                If False, the integers are sampled based on the model's output probabilities. Defaults to False.\n",
    "        N_matrix (torch.tensor, optional):  The model's output probability matrix. Defaults to a predefined tensor `N`.\n",
    "                                                Ensure `N` is accessible and contains the probability distribution for each integer (0 to 26).\n",
    "        i2s (Dict, optional):               The character mapping dictionary. Defaults to a predefined dictionary `i_to_s`.\n",
    "                                                Ensure `i_to_s` is accessible and maps integers to character strings.\n",
    "\n",
    "    Returns:\n",
    "        None: This function prints the sampled words and does not return any value.\n",
    "\n",
    "    Note:\n",
    "        - The function assumes the existence of a predefined mapping dictionary `i_to_s`, which maps integers to character strings.\n",
    "        - The variable `N` must be accessible, and it represents the model's output probabilities. It is assumed to be a PyTorch tensor.\n",
    "        - The variable `g` must be accessible and represent a PyTorch random generator for reproducible results.\n",
    "        - The function uses the provided `N_matrix` and `i2s` parameters to override default values if specified.\n",
    "        - Make sure to provide appropriate values for `N_matrix` and `i2s` if overriding the default ones.\n",
    "        - The function uses a `while True` loop to sample integers until it encounters the integer 0. This ensures that the function generates words of varying lengths, as the sampling process stops when 0 is sampled.\n",
    "        - If `uniform_probs` is set to True, the function uses uniform probabilities for sampling, meaning that each integer is equally likely to be sampled, regardless of its probability in the model's output.\n",
    "        - If `uniform_probs` is set to False (default), the function samples integers based on the model's output probabilities.\n",
    "    \"\"\"\n",
    "    for i in range(num_samples):\n",
    "        out = list()                                            \n",
    "        ix = 0\n",
    "        while True:\n",
    "            if N_matrix is not None:\n",
    "                p = N_matrix[ix].float() / N_matrix[ix].sum()           # This is very inefficient, as we compute probabilities and divide every time we sample a new integer \n",
    "            if P_matrix is not None:\n",
    "                p = P_matrix[ix]                                        # This is way more efficient as we use the already computed probabilities \n",
    "            if uniform_probs:\n",
    "                p = torch.ones(N_matrix.shape[0]) / N_matrix.shape[0]\n",
    "            ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            if ix==0:\n",
    "                break \n",
    "            out.append(i2s[ix])\n",
    "        print(\"\".join(out))\n",
    "\n",
    "# Now we can sample from the model using the probabilities provided \n",
    "sample_from_model(num_samples=5, N_matrix=N, uniform_probs=False); print()\n",
    "sample_from_model(num_samples=5, N_matrix=N, uniform_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just using P.sum() would yield a tensor with size: torch.Size([]) and value: 228875.0\n",
      "Using P.sum(dim=1, keepdim=False) would yield a tensor with size: torch.Size([27])\n",
      "Using P.sum(dim=1, keepdim=True) would yield a tensor with size: torch.Size([27, 1])\n",
      "\n",
      "This is the shape of the P_sum_notkeepdim: torch.Size([27]) with values in a 1D column vector:\n",
      "tensor([32060., 33912.,  2672.,  3559.,  5523., 20450.,   932.,  1954.,  7643.,\n",
      "        17728.,  2927.,  5067., 13985.,  6669., 18354.,  7961.,  1053.,   299.,\n",
      "        12727.,  8133.,  5597.,  3162.,  2600.,   956.,   724.,  9803.,  2425.])\n",
      "\n",
      "This is the shape of the P_sum_keepdim: torch.Size([27, 1]) with values in a 2D row vector:\n",
      "tensor([[32060.],\n",
      "        [33912.],\n",
      "        [ 2672.],\n",
      "        [ 3559.],\n",
      "        [ 5523.],\n",
      "        [20450.],\n",
      "        [  932.],\n",
      "        [ 1954.],\n",
      "        [ 7643.],\n",
      "        [17728.],\n",
      "        [ 2927.],\n",
      "        [ 5067.],\n",
      "        [13985.],\n",
      "        [ 6669.],\n",
      "        [18354.],\n",
      "        [ 7961.],\n",
      "        [ 1053.],\n",
      "        [  299.],\n",
      "        [12727.],\n",
      "        [ 8133.],\n",
      "        [ 5597.],\n",
      "        [ 3162.],\n",
      "        [ 2600.],\n",
      "        [  956.],\n",
      "        [  724.],\n",
      "        [ 9803.],\n",
      "        [ 2425.]])\n"
     ]
    }
   ],
   "source": [
    "######################################################### EFFICIENCY UPDATE OF THE SAMPLING FUNCTION #########################################################\n",
    "# Section from the lecture -> https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2&t=2177s \n",
    "\n",
    "\"\"\"\n",
    "Instead of sampling into the N matrix at each step and then re-compute the probabilities, we want to compute all the probabilities at once, before entering the function \n",
    "In order to stabilize our probability vectors and avoid impossible (=zero probability) sampling, we add a small factor of 1 to the counts matrix \n",
    "\n",
    "Here we have to be careful with the summation, due to later broadcasting semantics -> https://pytorch.org/docs/stable/notes/broadcasting.html \n",
    "The documentation states -> torch.sum(input, dim, keepdim=False) ->  https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \n",
    "The input is our tensor N+1.\n",
    "The dimension is along the first dimension (i.e. left to right) as we want the sum of each row in order to normalize the probabilities\n",
    "The keepdim has to be True in order to keep the dimension of the output Tensor. If we input a 27x27 and sum along the first dimension, we simply get a 1D vector of shape 27. However, with keepdim=True we get 27x1 \n",
    "\"\"\"\n",
    "\n",
    "P = (N+1).float()\n",
    "\n",
    "\n",
    "# The entries for P.sum(dim=1, keepdim=False) and P.sum(dim=1, keepdim=True) are equal, it is only the shape of the output that is different \n",
    "print(f\"Just using P.sum() would yield a tensor with size: {P.sum().size()} and value: {P.sum()}\")\n",
    "print(f\"Using P.sum(dim=1, keepdim=False) would yield a tensor with size: {P.sum(dim=1,keepdim=False).size()}\")\n",
    "print(f\"Using P.sum(dim=1, keepdim=True) would yield a tensor with size: {P.sum(dim=1,keepdim=True).size()}\")\n",
    "\n",
    "\"\"\"\n",
    "Now, in order to normalize the P matrix along the rows (i.e. let the sum of each row be 1), we have to divide each element in each row by the sum of that corresponding row\n",
    "This we can do in Python using what is called broadcasting.\n",
    "From the broadcast semantics we can see that two tensors are broadcastable if:\n",
    "    - Each tensor has at least one dimension\n",
    "    - When iterating over the dimension sizes of the two tensors, starting at the trailing dimension, the dimension sizes must either:\n",
    "            * be equal\n",
    "            * one of them is 1 \n",
    "            * one of them does not exist\n",
    "This means that for our example, where we have\n",
    "P_orig = 27 x 27\n",
    "P_sum  = 27 x  1\n",
    "We are fulfilling the criteria, i.e. the P / P_sum is broadcastable\n",
    "However, math wise we cannot divide a 27x27 with a 27x1.\n",
    "Hence, in order to be able to broadcast Python will internally copy all the columns along the dimension which is 1 and then perform an elementwise division:\n",
    "i.e. what happens is:\n",
    "P_orig: 27 x 27                                                           P_orig stays: 27 x 27\n",
    "                copy each column along the first dimension which is 1 ->                         elementwise division -> output is 27 x 27 \n",
    "P_sum:  27 x  1                                                           P_sum is now: 27 x 27\n",
    "\n",
    "\n",
    "If we had set keepdim==False, then the output would have been only \"not exist\" x 27, and then copied into 27x27, and then that  \n",
    "would have resulted in a copy upside down (along rows) instead of the copy along columns, i.e. a normalization in the wrong direction\n",
    "i.e. that would have been:\n",
    "P_orig:  27 x 27   ->   27 x 27   ->   27 x 27 \n",
    "P_sum:        27   ->   1  x 27   ->   27 x 27\n",
    "This is because the rules of broadcasting are as follows:\n",
    "1) Align dimensions on the right\n",
    "2) If a dimension does not exist, then create it by unsqueezing \n",
    "3) If any dimension is only 1 on one of the two tensors, copy the smallest tensor in the direction of that dimension in order to make the tensors have the same shapes \n",
    "\"\"\"\n",
    "P = (N+1).float()\n",
    "P_sum_keepdim = P.sum(dim=1, keepdim=True)\n",
    "P_sum_notkeepdim = P.sum(dim=1, keepdim=False)\n",
    "print(f\"\\nThis is the shape of the P_sum_notkeepdim: {P_sum_notkeepdim.size()} with values in a 1D column vector:\\n{P_sum_notkeepdim}\")\n",
    "print(f\"\\nThis is the shape of the P_sum_keepdim: {P_sum_keepdim.size()} with values in a 2D row vector:\\n{P_sum_keepdim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the two probability matrices \n",
    "P_keep = P / P_sum_keepdim          \n",
    "P_not_keep = P/P_sum_notkeepdim\n",
    "P = P_keep\n",
    "\n",
    "# Actually, the preferred would have been\n",
    "P = (N+1).float()\n",
    "P /= P.sum(dim=1, keepdim=True) \n",
    "# as in_place operations (like /=) are more efficient, as they don't create \n",
    "# new memory \"under the hood\", they simply replace the variable already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the two figures where the normalization has happened either from left to right (as wanted) or upside down \n",
    "plot_counts(N_matrix_list=[P_keep, P_not_keep], i2s=i_to_s, figsize=(60,120), fontsize=30, colormap=\"jet\", draw=False, title_list=[\"Correct broadcast & bigram probabilities\", \"Wrong broadcast & bigram probabilities\"], tit_fontsize=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "\n",
      "nzqfjiirltozcogsjgwzvudlhnpauyjbilevhajkdbduinrwibtlzsnjyievyvaftbzffvmumthyfodtumjrpfytszwjhrjagq.\n",
      "coreaysezocfkyjjabdywejfmoifmwyfinwagaasnhsvfihofszxhddgosfmptpagicz.\n",
      "rjpiufmthdt.\n",
      "rkrrsru.\n",
      "iyumuyfy.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Now we re-sample from the model again using the probability P-matrix\n",
    "# This almost requires a re-write of the sample_from_model function from before \n",
    "\n",
    "def sample_from_model_new(num_samples: int = 5, P_matrix: torch.tensor = P, uniform_probs: bool = False):\n",
    "  for _ in range(num_samples):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "      if not uniform_probs:\n",
    "        p = P_matrix[ix]\n",
    "      else:\n",
    "        p = torch.ones(P_matrix.shape[0]) / P_matrix.shape[0]\n",
    "      ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "      out.append(i_to_s[ix])\n",
    "      if ix == 0:\n",
    "        break\n",
    "    print(''.join(out))\n",
    " \n",
    "\n",
    "# We can see here that we are sampling the exact same five new words as before, but now we are using the probability matrix \n",
    "# Hence, we \n",
    "sample_from_model_new(num_samples=5, P_matrix=P, uniform_probs=False); print()\n",
    "sample_from_model_new(num_samples=5, P_matrix=P, uniform_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bigram: (., e) has a p=0.0478, which gives a log(p)=-3.0410\n",
      "The bigram: (e, m) has a p=0.0377, which gives a log(p)=-3.2793\n",
      "The bigram: (m, m) has a p=0.0253, which gives a log(p)=-3.6753\n",
      "The bigram: (m, a) has a p=0.3885, which gives a log(p)=-0.9454\n",
      "The bigram: (a, .) has a p=0.1958, which gives a log(p)=-1.6305\n",
      "The bigram: (., o) has a p=0.0123, which gives a log(p)=-4.3965\n",
      "The bigram: (o, l) has a p=0.0779, which gives a log(p)=-2.5526\n",
      "The bigram: (l, i) has a p=0.1774, which gives a log(p)=-1.7293\n",
      "The bigram: (i, v) has a p=0.0152, which gives a log(p)=-4.1845\n",
      "The bigram: (v, i) has a p=0.3508, which gives a log(p)=-1.0476\n",
      "The bigram: (i, a) has a p=0.1380, which gives a log(p)=-1.9807\n",
      "The bigram: (a, .) has a p=0.1958, which gives a log(p)=-1.6305\n",
      "The bigram: (., a) has a p=0.1376, which gives a log(p)=-1.9835\n",
      "The bigram: (a, v) has a p=0.0246, which gives a log(p)=-3.7041\n",
      "The bigram: (v, a) has a p=0.2473, which gives a log(p)=-1.3971\n",
      "The bigram: (a, .) has a p=0.1958, which gives a log(p)=-1.6305\n",
      "This means that the log likelihood for all these bigrams are -38.8086\n",
      "This gives a negative log likelihood (nll) loss of 2.4255\n"
     ]
    }
   ],
   "source": [
    "### Now we have a model from which we are capable of sampling (i.e. predicting new words)\n",
    "# From this sampling we can see that even though sampling with bigram probabilities doesn't provide real, new words/names, \n",
    "# it is still way better than sampling with the uniform probabilities.\n",
    "# Hence, this means, that we can conclude, that our model has learned something from the bigram probabilities\n",
    "# For example, we can see that the bigram (m, a) has a probability of around 39%. \n",
    "# That means that the model knows that in 39% of the times we have an 'm', then it will be successed by an 'a'.\n",
    "# Hence, in 39% of the situations, where the model has sampled an 'm', an 'a' will be sampled afterwards \n",
    "\n",
    "### Here we can visualize the bigrams from the first three words in our list of words, including the probabilities for each of these bigrams\n",
    "\n",
    "log_likelihood, n_count = 0, 0\n",
    "for w in words[:3]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = s_to_i[ch1]\n",
    "    ix2 = s_to_i[ch2]\n",
    "    prob = P[ix1,ix2]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n_count += 1\n",
    "    print(f\"The bigram: ({ch1}, {ch2}) has a p={prob:.4f}, which gives a log(p)={logprob:.4f}\")\n",
    "mean_nll_loss_bigram_model = -log_likelihood/n_count\n",
    "print(f\"This means that the log likelihood for all these bigrams are {log_likelihood:.4f}\")\n",
    "print(f\"This gives a negative log likelihood (nll) loss of {mean_nll_loss_bigram_model:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the 228146 bigrams we compute a NLL loss of 2.4544\n",
      "This loss will then be considered our lower bound for such a bigram model\n"
     ]
    }
   ],
   "source": [
    "# Now we will simply compute the NLL loss for the entire dataset\n",
    "# by running through the probabilities of all the bigrams in the dataset\n",
    "log_likelihood, n_count = 0, 0\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    prob = P[s_to_i[ch1],s_to_i[ch2]]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n_count += 1\n",
    "mean_nll_loss_bigram_model = -log_likelihood/n_count\n",
    "print(f\"For the {n_count} bigrams we compute a NLL loss of {mean_nll_loss_bigram_model:.4f}\")\n",
    "print(f\"This loss will then be considered our lower bound for such a bigram model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average selected probability for all 228146 bigrams is 0.08592\n"
     ]
    }
   ],
   "source": [
    "### As we can see the lower bound of this bigram model is 2.45 we can compute what probability leads to that result \n",
    "# We have 2.45 = -log(x)  ->  exp(-2.45) = x  ->  x  0.086 \n",
    "print(f\"The average selected probability for all {n_count} bigrams is {torch.exp(-mean_nll_loss_bigram_model):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the natural logarithm for this log likelihood:\n",
      "\n",
      "torch.log(torch.tensor(math.exp(1)))    = 1.0\n",
      "torch.log10(torch.tensor(math.exp(1)))  = 0.4343\n",
      "torch.log(torch.tensor(10))             = 2.3026\n",
      "torch.log10(torch.tensor(10))           = 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9QAAAF2CAYAAACPn9BIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB2mklEQVR4nO3dd3hTZfsH8G/SpulMB920lLZAKWVPKSIgUIaCqK9bGSounPCqoK+yVEBQ9HXgQlBfEMUB/li2MkSUTcsspYXSQhfd6UzT5Pn9URqJHTRtktM23891cdFzcsadu+OcO89znkcmhBAgIiIiIiIiIpPIpQ6AiIiIiIiIqC1iQU1ERERERETUDCyoiYiIiIiIiJqBBTURERERERFRM7CgJiIiIiIiImoGFtREREREREREzcCCmoiIiIiIiKgZWFATERERERERNQMLaiIiIiIiIqJmYEFNdNWePXsgk8nwww8/WPQ8Fy9ehEwmw9q1ay16noZ888036N69OxQKBTw8PCSJ4XqkzhEREbUv1rrGt0Y7duxA37594ejoCJlMhqKiIqlDqpdMJsOCBQukDoPIZPZSB0BkSTKZrEnb7d6928KRtA5nz57F9OnTMX78eMydOxfOzs6SxrN+/XpcuXIFzz//vKRxEBFR28Nr/PXl5+fj7rvvRlRUFD766CMolUq4uLhIFs+2bdtw6NAhFs7UrrCgpnbtm2++MVr++uuvERcXV2d9ZGQkEhMTrRmaJPbs2QO9Xo/3338fXbp0kTocrF+/HqdOnapTUIeEhKCiogIKhUKawIiIqNXjNf76Dh8+jJKSEixevBhjxoyROhxs27YNH330Ub0FdUVFBeztWZpQ28OfWmrXHnzwQaPlAwcOIC4urs56ADZxsb1y5QoAtNqu3rVkMhkcHR2lDoOIiFoxXuOvr61c9wHwuk9tFp+hJvoHvV6PN998E0FBQXB0dMTo0aORkpJSZ7uDBw9i/PjxcHd3h7OzM0aMGIE///yz2efdtWsXhg8fDhcXF3h4eOC2226r9wZgz549GDhwIBwdHREeHo5PP/0UCxYsuG7Xt86dO2P+/PkAAB8fH6NnlRp6bqlz586YPn26YXnt2rWQyWT4888/MXv2bPj4+MDFxQW33347cnNz6+y/fft2jBgxAm5ublCpVBg0aBDWr18PABg5ciS2bt2KtLQ0yGQyyGQydO7cGUDDz1A3JUe1uUhJScH06dPh4eEBd3d3zJgxA+Xl5Y3miIiI2jdrXOM1Gg3mz5+PLl26QKlUIjg4GC+99BI0Go1hm2nTpsHR0bHONWzcuHHw9PREZmYmgL+vu3v37sXjjz+ODh06QKVSYerUqSgsLGw0jpEjR2LatGkAgEGDBkEmkxmu6f+8vl+7z8iRIw3Ltc+ef//9903O28SJE+Hp6QkXFxf07t0b77//PgBg+vTp+OijjwDAcN2/9t6lvnuR+Ph4TJgwASqVCq6urhg9ejQOHDhgtI2p9yZE5sYWaqJ/WLp0KeRyOf7973+juLgYb7/9Nh544AEcPHjQsM2uXbswYcIEDBgwAPPnz4dcLseaNWtw8803448//sDgwYNNOudvv/2GCRMmICwsDAsWLEBFRQU++OADDBs2DMeOHTMUmvHx8Rg/fjwCAgKwcOFC6HQ6LFq0CD4+Ptc9x3vvvYevv/4aP//8M1atWgVXV1f07t3bpDhrPfPMM/D09MT8+fNx8eJFvPfee3j66afx3XffGbZZu3YtHn74YURFRWHevHnw8PBAfHw8duzYgfvvvx+vvvoqiouLcfnyZaxcuRIA4Orq2uIc1br77rsRGhqKJUuW4NixY/jiiy/g6+uLZcuWNes9ExFR22fpa7xer8fkyZOxb98+PPbYY4iMjMTJkyexcuVKnDt3Dps2bQIAvP/++9i1axemTZuG/fv3w87ODp9++iliY2PxzTffIDAw0Oi4Tz/9NDw8PLBgwQIkJSVh1apVSEtLMxS89Xn11VcRERGBzz77DIsWLUJoaCjCw8Mtlre4uDjceuutCAgIwHPPPQd/f38kJiZiy5YteO655/D4448jMzOz3m759Tl9+jSGDx8OlUqFl156CQqFAp9++ilGjhyJ33//HUOGDDHavin3JkQWIYhsyKxZs0RDP/a7d+8WAERkZKTQaDSG9e+//74AIE6ePCmEEEKv14uuXbuKcePGCb1eb9iuvLxchIaGirFjxzYaQ2pqqgAg1qxZY1jXt29f4evrK/Lz8w3rjh8/LuRyuZg6daph3aRJk4Szs7PIyMgwrEtOThb29vYNvq9rzZ8/XwAQubm5RusBiPnz59fZPiQkREybNs2wvGbNGgFAjBkzxui9v/DCC8LOzk4UFRUJIYQoKioSbm5uYsiQIaKiosLomNfud8stt4iQkJA6521Jjmrf48MPP2x0zNtvv1106NChblKIiKhdaA3X+G+++UbI5XLxxx9/GK3/5JNPBADx559/Gtb9+uuvAoB44403xIULF4Srq6uYMmWK0X61190BAwaIqqoqw/q3335bABCbN29uNJ7a/Q8fPmy0/p/X91ojRowQI0aMMCw3NW/V1dUiNDRUhISEiMLCQqNjXpvHxr5H/7wXmTJlinBwcBDnz583rMvMzBRubm7ipptuqvMer3dvQmQp7PJN9A8zZsyAg4ODYXn48OEAgAsXLgAAEhISkJycjPvvvx/5+fnIy8tDXl4eysrKMHr0aOzduxd6vb7J58vKykJCQgKmT58OLy8vw/revXtj7Nix2LZtGwBAp9Pht99+w5QpU4w+ue7SpQsmTJjQovdsqscee8zoE/Hhw4dDp9MhLS0NQM2n1CUlJZg7d26dZ6KaOirrtZqao2s98cQTRsvDhw9Hfn4+1Gq1yecnIqL2wdLX+I0bNyIyMhLdu3c37JuXl4ebb74ZgPGI4zExMXj88cexaNEi3HHHHXB0dMSnn35a73Efe+wxo4E6n3zySdjb29d7/bOE6+UtPj4eqampeP755+s8r92c675Op0NsbCymTJmCsLAww/qAgADcf//92LdvX53r+fXuTYgshV2+if6hU6dORsuenp4AYHhWKTk5GQAMzyXVp7i42LDf9dT+oY+IiKjzWmRkJH799VeUlZVBrVajoqKi3tG5rT1i9/VydP78eQBAz549zXK+pubo2qlAGotRpVKZJS4iImpbLH2NT05ORmJiYoOPYtUOElZrxYoV2Lx5MxISErB+/Xr4+vrWu1/Xrl2Nll1dXREQEICLFy82GKc5Wfu6n5ubi/Ly8gav+3q9HpcuXUJUVFSTYySyFBbURP9gZ2dX73ohBAAYPplevnw5+vbtW++2jT0L3JbodLp6118vR61BW4iRiIisy9LXeL1ej169euHdd9+t9/Xg4GCj5fj4eEORffLkSdx3332Nxm8uDbUa63S6enPUFq6pbSFGap9YUBOZqHZAD5VKZZY5HUNCQgAASUlJdV47e/YsvL294eLiAkdHRzg6OtY7qmZ960zh6emJoqIio3VVVVXIyspq1vFqc3Tq1KlGW8+b2g2sqTkiIiJqiZZe48PDw3H8+HGMHj36ute4srIyzJgxAz169EB0dDTefvtt3H777Rg0aFCdbZOTkzFq1CjDcmlpKbKysjBx4kSTYwTqv+4DNT3Cru1i3VTXXvcby1tTr/s+Pj5wdnZu8Lovl8vrfDhBJBU+Q01kogEDBiA8PBwrVqxAaWlpnddNnaIhICAAffv2xVdffWV0cTt16hRiY2MNF0s7OzuMGTMGmzZtMkynAdQU09u3b2/em7kqPDwce/fuNVr32WefNdhCfT0xMTFwc3PDkiVLUFlZafTatZ8Uu7i4oLi4+LrHa2qOiIiIWqKl1/i7774bGRkZ+Pzzz+u8VlFRgbKyMsPyyy+/jPT0dHz11Vd499130blzZ0ybNs1oeq1an332GbRarWF51apVqK6ubvYYKuHh4Thw4ACqqqoM67Zs2YJLly4163j9+/dHaGgo3nvvvTqF+j+v+wDqLeavZWdnh5iYGGzevNmoW3tOTg7Wr1+PG2+8kY9vUavBFmoiE8nlcnzxxReYMGECoqKiMGPGDHTs2BEZGRnYvXs3VCoV/u///s+kYy5fvhwTJkzA0KFD8cgjjximhHJ3dzeak3HBggWIjY3FsGHD8OSTT0Kn0+HDDz9Ez549kZCQ0Oz39Oijj+KJJ57AnXfeibFjx+L48eP49ddf4e3t3azjqVQqrFy5Eo8++igGDRqE+++/H56enjh+/DjKy8vx1VdfAai5cfnuu+8we/ZsDBo0CK6urpg0aVK9x2xqjoiIiJqrpdf4hx56CN9//z2eeOIJ7N69G8OGDYNOp8PZs2fx/fff49dff8XAgQOxa9cufPzxx5g/fz769+8PAFizZg1GjhyJ1157DW+//bbRcauqqjB69GjcfffdSEpKwscff4wbb7wRkydPbtb7fPTRR/HDDz9g/PjxuPvuu3H+/Hn873//a/a0WnK5HKtWrcKkSZPQt29fzJgxAwEBATh79ixOnz6NX3/9FUDNdR8Ann32WYwbNw52dna499576z3mG2+8gbi4ONx444146qmnYG9vj08//RQajaZOfogkJd0A40TW15QpNTZu3Gi0vr4pnIQQIj4+Xtxxxx2iQ4cOQqlUipCQEHH33XeLnTt3NhpDQ8f77bffxLBhw4STk5NQqVRi0qRJ4syZM3X237lzp+jXr59wcHAQ4eHh4osvvhBz5swRjo6O133/DU2bpdPpxMsvvyy8vb2Fs7OzGDdunEhJSWlw2qx/Tr9Rm7vdu3cbrf/ll19EdHS04T0NHjxYfPvtt4bXS0tLxf333y88PDwEAMMUWi3JUUPvsTb21NTU6+aJiIjantZwjRdCiKqqKrFs2TIRFRUllEql8PT0FAMGDBALFy4UxcXFQq1Wi5CQENG/f3+h1WqN9n3hhReEXC4X+/fvF0L8fe36/fffxWOPPSY8PT2Fq6ureOCBB4ymkWxIQ9dtIYR45513RMeOHYVSqRTDhg0TR44caXDarKbmbd++fWLs2LHCzc1NuLi4iN69e4sPPvjA8Hp1dbV45plnhI+Pj5DJZEbfL9QzheexY8fEuHHjhKurq3B2dhajRo0Sf/31V5PeY0P3JkTmJhOCT+oTtXVTpkzB6dOnDaOTEhERUdu3du1azJgxA4cPH8bAgQOlDoeI6sFnqInamIqKCqPl5ORkbNu2DSNHjpQmICIiIiIiG8VnqInamLCwMEyfPh1hYWFIS0vDqlWr4ODggJdeeknq0IiIiIiIbAoLaqI2Zvz48fj222+RnZ0NpVKJoUOH4q233kLXrl2lDo2IiIiIyKbwGWoiIiIiIiKiZuAz1ERERERERETNwIKaiIiIiIiIqBla9TPUer0emZmZcHNzg0wmkzocIiIiCCFQUlKCwMBAyOX8XLqleK0nIqLWxpRrfasuqDMzMxEcHCx1GERERHVcunQJQUFBUofR5vFaT0RErVVTrvWtuqB2c3MDUPNGVCpVi46l1WoRGxuLmJgYKBQKc4TX7jFnpmPOTMecmY45M505c6ZWqxEcHGy4RlHLmPNaD7T93w/GLy3GLy3GLy3G/zdTrvWtuqCu7fqlUqnMUlA7OztDpVK1yR8QKTBnpmPOTMecmY45M50lcsbuyeZhzms90PZ/Pxi/tBi/tBi/tBh/XU251vPhLyIiIiIiIqJmYEFNRERERERE1AwsqImIiIiIiIiagQU1ERERERERUTOwoCYiIiIiIiJqBhbURERERERERM3AgpqIiIiIiIioGaxSUH/00Ufo3LkzHB0dMWTIEBw6dMgapyUiIiIiIiKyGIsX1N999x1mz56N+fPn49ixY+jTpw/GjRuHK1euWPrURERERERERBZj8YL63XffxcyZMzFjxgz06NEDn3zyCZydnfHll19a+tRERERERERkAzTVeknOa2/Jg1dVVeHo0aOYN2+eYZ1cLseYMWOwf//+OttrNBpoNBrDslqtBgBotVpotdoWxVK7f0uPY0uYM9MxZ6ZjzkzHnNVPpxfIL6tCdnElctQaZKsra/4Va5BVXAGHSjnGmiFnzDsREVHrIoTAzG+OoUotx6ASDQK9FFY7t0UL6ry8POh0Ovj5+Rmt9/Pzw9mzZ+tsv2TJEixcuLDO+tjYWDg7O5slpri4OLMcx5YwZ6ZjzkzHnJnOlnJWrQfUWqBIAxRVyVBcVfN/URVQpKlZLtYCeiFr8BhBLjKz5Ky8vLzFxyAiIiLz2XIiC/svFEAhk1m9pdqiBbWp5s2bh9mzZxuW1Wo1goODERMTA5VK1aJja7VaxMXFYezYsVAorPeJRVvGnJmOOTMdc2a69pYzrU6PHLUGmcUVyCqqRHZt63Jxzdc56krklVVBiOsfSy4DfNyU8Fc5wl+lhJ/KEf7uSvi4KJB57oRZclbbe4qIiIikV6apxptbEwEAY4P0CPJ0sur5LVpQe3t7w87ODjk5OUbrc3Jy4O/vX2d7pVIJpVJZZ71CoTDbTaM5j2UrmDPTMWemY85M1xZyJkRNN+ysokpkFFUgs6gCWcUVyCyqRGZxzfKVEk2TimUHOzn83JUIUDnB392x5p/KEQFXvw5wd4K3qwPs7eoOD6LVarEt64RZctbac05ERGRLPtiVgmx1JYI9nXBzYInVz2/RgtrBwQEDBgzAzp07MWXKFACAXq/Hzp078fTTT1vy1EREZAVlmuq/C+SiCmQWX/2/qAJZV79uStcrB3s5Aq8WxQEetUWyEwJUtcWyI7xcHCCTNdylm4iIiGxLck4JvvjjAgDgP7d0R+X5w1aPweJdvmfPno1p06Zh4MCBGDx4MN577z2UlZVhxowZlj41ERG1gBACeaVVuFxYjsuFFbhcWNu6XIGMokpkFVegqLxpA3T5uikR6OGEQA9HBLo7/f21R83XHVgsExERkQmEEPjPplOo1guM7eGHmyN8sO289eOweEF9zz33IDc3F6+//jqys7PRt29f7Nixo85AZUREZF1CCOSWagzF8rWF8+XCcmQUNq112VVpj44eNS3LgR5ONV+7//21n8oRDvYWn6WRiIiIbMimhAwcTC2Ao0KO+ZN6SBaHVQYle/rpp9nFm4jIyvR6gbxSDS41UCxnNKE7tkwG+KscEeRZUxx39HRCgHvN14FXi2iVI58pJiIiIusprtAaBiJ7dnRXBHk6SzatZasa5ZuIiJpOCIHCsiqkF5QjraAclwrKDYVzRmEFLhdVoOo6BbPcUDA7I8jT6eo/Z8P//u5sXSYiIqLW5d3YJOSVViHcxwWP3hgmaSwsqImIWjGtTo+sokqkFZQhvaAc6QXluJhbitNpdnj12G6Uaqob3V8uQ02Lcp1i2QnBVwtmRT2jYhMRERG1RicvF+ObA2kAgMVTekr+wT8LaiIiiakrtUjPrymW067+f6mgHGkFZcgsqoROX9+cUjIANcW0n0qJTl7OCPZyRvA1rctBnk4smImIiKjd0OkF/rPpJPQCuK1vIKLDvaUOiQU1EZGl1Q7+dTGvHBfzynAx/+/W5vSC8uuOlK20lyPYyxkhV4vmjh5KXLlwBrePHY5QHxWcHOys9E6IiIiIpLPhcDqOXy6Gm9Ier06MlDocACyoiYjMQgiB/LIqXMwrQ+rVovliXjlS88qQll+Gsipdo/t7uyrRycsJnbyc0amDS83/Xs4I6eAMH1cl5PK/p5TSarXYVngaXX1doVCwmCYiIqL2L69Ug7d3JAEA5sR0g6/KUeKIarCgJiIyQWFZFVLzy2pamvPKkJpfbvi6pJHnmeUyoKOnEzp3cEHnDi4I6VDT2hzSoaabtouSf46JiIiIGrJ0+1kUV2gRFajCgzeESB2OAe/giIj+oaJKhwt5pTifW4YLuaVGhXNxRcPds2UyINDdCZ29ndG5gwtCvWuK587eLgj2coLSnq3JRERERKY6lFqAH45ehkwGvDGlJ+xb0fgwLKiJyCYJIZBbokFKbk3hfP5KKS7k1fyfUVTR6L7+Kkd09nY2KphDvWu6aTuyCzYRERGR2WiqdXjl55MAgHsHBaNfJ0+JIzLGgpqI2jVNtQ5p+eW4cE3hfP7q141NOeXhrEC4jyvCvF0Q6uOC0KuFc0gHZzg78E8nERERkTV8+vsFpFwphberEnPHt46ByK7Fu0IiahdKNdVIzilBck5pTavz1cI5vaAc9c46hZrnmjt5OSPMxxXhPi4I93FFuK8rwn1c4eXiYN03QERERERGzueW4sNdKQCA1yf1gLuzQuKI6mJBTURtSpmmGslXSnEupwTJOSU4l1OKlOt003ZV2hsK5rBrCueQDs58rpmIiIioFRJC4JWfTqJKp8fICB9M6h0gdUj1YkFNRK1SmaYaKbWF85VSQ/HcWOHs46ZENz9XdPV1q2lp9nZBuK8rfN2UkMlkDe5HRERERK3LxiOXcTC1AE4KOyy+rWervZdjQU1Ekqqq1iOzDNh8PAvJuWVIzqkpoi8XNlw4e7vWFM7d/NzQtfZ/X1d4OLObNhEREVFbl1uiwZvbEgEAs8d2Q7CXs8QRNYwFNRFZRe2o2onZJTibpcbZ7BIkZqlxPrcUWp09cOJknX28XR3Q1detptXZz81QOHvy+WYiIiKidmvxljOGOadnDOssdTiNYkFNRGZXqdUh5UopEq8Wzmez1UjMKkFBWVW92zvZCUQFeSIywB3d/N3QzbemgObAYERERES2ZU/SFfxyPBNyGbD0jt6tas7p+rCgJqIWySvV4HSmGqcyig0FdGpeGXT1DK0tlwGh3i7oHqBCpL8buvur0MXHCQl/7sYttwyGQtH6Rm4ksiVLlizBTz/9hLNnz8LJyQnR0dFYtmwZIiIiGtxn7dq1mDFjhtE6pVKJyspKS4dLRETtTHlVNf6z6RQAYMawUPQKcpc4outjQU1ETSKEQGZxJU5nFONUphpnMotxKkONbHX9N82ezgpEBqjQ3V+F7gFuiPRXoaufKxwVxqNqa7VaHG+dY0wQ2Zzff/8ds2bNwqBBg1BdXY1XXnkFMTExOHPmDFxcXBrcT6VSISkpybDcWgeOISKi1u2935JxubACHT2cMHtsN6nDaRIW1ERUh14vcDG/rKblObMYZ662QBeWa+tsK7va6hwV6I4eASpEBrghMkDFkbWJ2qAdO3YYLa9duxa+vr44evQobrrppgb3k8lk8Pf3t3R4RETUjp3KKMbqfakAgMVTouCibBulatuIkogsprZ4Pn65CCcuF+N0hhpnstQo1VTX2dZeLkNXPzdEBarQM1CFnh3dERmgajN/8IjINMXFxQAALy+vRrcrLS1FSEgI9Ho9+vfvj7feegtRUVH1bqvRaKDRaAzLarUaQE1vFa227od2pqo9hjmOJQXGLy3GLy3GLy0p49fq9Hhx43Ho9AITovwwPNzL5DjMGb8px+BdMJENEUIgW12J45eKceJykaGILqmsWzwr7eWIDFDVFM8d3REVqEI3P7c6XbaJqH3S6/V4/vnnMWzYMPTs2bPB7SIiIvDll1+id+/eKC4uxooVKxAdHY3Tp08jKCiozvZLlizBwoUL66yPjY2Fs7P5pkWJi4sz27GkwPilxfilxfilJUX8sZdlSMy2g7O9QLRjBrZty2j2scwRf3l5eZO3ZUFN1I4VlVfhxOViHL9UhOOXa4roKyWaOtsp7eWIClShd5AHege5IyrQHeE+Lq1+VEUispxZs2bh1KlT2LdvX6PbDR06FEOHDjUsR0dHIzIyEp9++ikWL15cZ/t58+Zh9uzZhmW1Wo3g4GDExMRApVK1OG6tVou4uDiMHTu2TQ50yPilxfilxfilJVX8yVdK8e9D+wEILJrSG7f1CWjWccwZf23vqaZgQU3UTmh1epzJVONYeiHi02tan9Py6366ZieXoauvK/oGexgK6Ah/NyhYPBPRVU8//TS2bNmCvXv31tvK3BiFQoF+/fohJSWl3teVSiWUSmW9+5nzBs7cx7M2xi8txi8txi8ta8av0wu8sukMtDqBm7v74s4BwS0eg8cc8ZuyPwtqojYqr1SDY2mFOJZehGNphTiRUYRKrb7OdiEdnNHnauHcN9gDUYHucHJgt20iqksIgWeeeQY///wz9uzZg9DQUJOPodPpcPLkSUycONECERIRUXuy5s9UJFwqgpvSHm/e3rNNDmjLgpqoDdDpBZKyS3A0vRDxaYU4ml5Yb+uzu5MC/Tp5oH8nz6st0O7wcHaQIGIiaotmzZqF9evXY/PmzXBzc0N2djYAwN3dHU5OTgCAqVOnomPHjliyZAkAYNGiRbjhhhvQpUsXFBUVYfny5UhLS8Ojjz4q2fsgIqLW72JeGVbE1ky5+MotkQhwd5I4ouZhQU3UCpVXVeNYWhEOpebjaHohEtKLUFalq7NdV19X9O/kiQEhnugf4oEwb1fI5W3vkz0iah1WrVoFABg5cqTR+jVr1mD69OkAgPT0dMjlfz8iUlhYiJkzZyI7Oxuenp4YMGAA/vrrL/To0cNaYRMRURuj1wu8/OMJVGr1GNalA+4dFCx1SM3GgpqoFSgu1+JIWgEOpRbgYGoBTmUUo1ovjLZxVdqjb7AH+od4on8nD/QL9oS7c9t9PoeIWh8hxHW32bNnj9HyypUrsXLlSgtFRERE7dH6Q+k4mFoAJ4Udlt7Ru0129a7FgppIAldKKnE4tRCHUvNx6GIhzmar8c/72EB3RwwO9cLAzl4YEOKJbn5usGPrMxERERG1YRlFFViyLREA8NL4CAR7mW/KRCmwoCaygtwSDf46n4f95/NxKLUAF/LK6mwT5u2CwaFehn9Bnm37jwsRERER0bWEEJj300mUVekwIMQT04Z2ljqkFmNBTWQBJZVaHE0uwJ8pefjrfB7O5ZQavS6TARF+bhgS6oXBoR0wKNQTvm6OEkVLRERERGR5G49ext5zuXCwl2PZnb3bxdg/Fiuo33zzTWzduhUJCQlwcHBAUVGRpU5FJLlKrQ5HLhZiX/IVbD9phxcO7MY/HoFGjwAVosM74IawDhjY2ZOjbxMRERGRzbhcWI5F/3cGAPDCmG7o4usqcUTmYbGCuqqqCnfddReGDh2K1atXW+o0RJLQ6wXOZKnx+7lc7EvOw9H0QlRV184BXfNJW5i3C4aGd8CwLt64IawDvFxYQBMRERGR7akd1btUU43+nTzw2E1hUodkNhYrqBcuXAgAWLt2raVOQWRV+aUa/JGch9/P5eKP5FzklVYZve6vcsTQME84l1zG47ePQidvN4kiJSIiIiJqPdYdTMOfKflwVMix4q4+7WqgXT5DTdSAap0e8ZeK8HtSLvYm5+JkRrHRSNwuDnaI7uKNm7p6Y1gXb4R6u6C6uhrbtl1CgDufhyYiIiIiuphXhre2nQUAzB3fHWE+7aOrd61WVVBrNBpoNBrDslqtBgBotVpotdoWHbt2/5Yex5bYYs7ySjXYnVTTCv3XhQKUVFYbvR7p74abunpjeNcO6BfsAQd7ueG16upqm8xZSzFnpmPOTGfOnDHvRERETaPTC/x743FUaHUYGtYBU9vBqN7/ZFJBPXfuXCxbtqzRbRITE9G9e/dmBbNkyRJDV/FrxcbGwtnZPFMIxcXFmeU4tqQ950wIIKscOFUow+lCOdJKAYG/u6C42AtEuAtEegp0dxdQORQC1YXIT0zGb4kNH7c958xSmDPTMWemM0fOysvLzRAJERFR+7d63wUcSSuEq9Ieb/+rfYzq/U8mFdRz5szB9OnTG90mLKz5D5jPmzcPs2fPNiyr1WoEBwcjJiYGKpWq2ccFaloU4uLiMHbsWCgUihYdy1a015xVVetx6GIhdiXlYvfZK7hcVGn0es9AFW6O8MFN3bzRM1Bl0jMe7TVnlsScmY45M505c1bbe4qIiIgadi6nBCt+PQcAeO3WSAR7maeBtLUxqaD28fGBj4+PpWKBUqmEUqmss16hUJjtptGcx7IV7SFnJZVa7Dp7BbGnc/D7uVyUav7uyq20l2NYF2+MifTD6Ehf+Kla/vxze8iZtTFnpmPOTGeOnDHnREREjdPq9Jjz/XFU6fQYFeGDuwcGSx2SxVjsGer09HQUFBQgPT0dOp0OCQkJAIAuXbrA1bV9PYhOrVNReRXizuRgx6ls/JGchyqd3vCat6sSo7v7YkwPPwzr0gHODq1qOAEiIiIiojZr1Z7zOJlRDHcnBZbe2RsyWfvr6l3LYlXE66+/jq+++sqw3K9fPwDA7t27MXLkSEudlmxcXqkGsadzsP1UFvafz0e1/u9hucN8XDChpz/GRPqhT5BHu3yGg4iIiIhISqcyivHfnckAgEW3RZml92drZrGCeu3atZyDmqwiv1SDbSezsPVkFg6lFuCaGhrd/d0woWcAJvTyR1df13b96RgRERERkZQqtTq88F0CqvUCE3r6Y3KfQKlDsjj2c6U2qVRTjdjT2dickIl9KXnQXVNF9w5yx/ie/pjQMwCh3i4SRklEREREZDuWbj+L5Cul8HFT4o0pPW2iMYsFNbUZmmodfk/KxebjmfjtTA401X8/E907yB2TegdifE//djuCIBERERFRa7Un6QrW/nURALD8X73RwbXuYNPtEQtqatX0eoFDFwuwKT4D205mQV359+jcYd4umNw3EJP7BCLMhwPdERERERFJIb9Ugxd/OAEAmB7dGSMjfCWOyHpYUFOrdLmwHD8ezcCPxy4jvaDcsN5PpcTkPoG4rW9HRAWqbKIbCRERERFRayWEwNyfTiK3RIOuvq6YO6G71CFZFQtqajUqqnT49XQ2Nh69hL/O50NcfSzaVWmPW3oFYEq/jhgc6gU7js5NRERERNQqbDh8CXFncqCwk+G9e/vCUWEndUhWxYKaJCWEwPHLxfju8CVsOZ6JEs3fXbqjwzvgroFBGB8VACcH2/rFJCIiIiJq7VLzyrDo/84AAF4cF4GoQHeJI7I+FtQkiTJNNX45non/HUjD6Uy1YX2QpxP+NSAId/YP4uBiREREREStlFanx/Mb4lGh1WFoWAc8emOY1CFJggU1WVVSdgnWHUzDz8cyDK3RDvZy3NIrAHcNDMINoR0gZ5duIiIiIqJW7b87k3H8cjFUjvZ45+4+NnsPz4KaLE5TrcOOU9n434E0HL5YaFgf6u2CB4Z0wp39g+Dp4iBhhERERERE1FRHLhbgo90pAIC37uiFQA8niSOSDgtqspj8Ug3WHUzHNwfSkFuiAQDYyWWI6eGHB4aEIDqcrdFERERERG2JulKL579LgF4Ad/TriFt7B0odkqRYUJPZncspwZf7UvFzfAY01XoANdNd3T84BPcODoafylHiCImIiIiIyFRCCMz76SQuF1YgyNMJC26LkjokybGgJrMQQuD3c7lYvS8VfyTnGdb3DnLHIzeGYmKvACjs5BJGSERERERELfHd4UvYeiIL9nIZPrivH1SOCqlDkhwLamoRnV5g68ksfLw7BWezSwAAchkQ08MfjwwPxcAQT8hk7NZNRERERNSWJeeUYMH/nQYA/HtcBPp18pQ4otaBBTU1i6Zah5+PZeCT38/jYn45AMDFwQ73DOqE6dGd0akDp7wiIiIiImoPKrU6PL0+HpVaPYZ39cZjw21ziqz6sKAmk5RXVePbQ5fw+d4LyFZXAgA8nBWYER2KadEh8HDmaN1ERERERO3J4i1nkJRTAm9XJd69uy8HFr4GC2pqkkqtDv87kIZVe84jv6wKQM1AYzOHh+G+wZ3gouSPEhERERFRe7P9ZBbWHUwHAKy8pw983JQSR9S6sAqiRlXrgf8dTMeq31Nx5erUV528nPHkyHDc0b8jlPZ2EkdIRERERESWcKmgHC/9eAIA8OTIcAzv6iNxRK0PC2qql1anx8ajl7E83g6FVWcBAB09nPDc6K64o39H2HPEbiIiIiKidkur0+O5DfEoqaxGv04emD22m9QhtUosqMmIEAI7TmVj2Y6zVwcbk8HPTYmnb+6CuwcFs0WaiIiIiMgGvPfbORxLL4Kboz3+e28/ToHbABbUZBCfXog3tybiSFohAMDLRYGbvCuxeNqNcHN2lDg6IiIiIiKyhj+Sc/HxnvMAgKV39EawF2fwaQgLasKlgnIs23EWW05kAQAcFXI8dlM4ZgwNxt6dsXBUsFWaiIiIiMgWZBVX4rkNCRACuG9wJ9zSO0DqkFo1FtQ2rFRTjQ92JWPNvouo0ukhkwH/6h+EOTER8Hd3hFarlTpEIiIiIiKyEp0eeOH7Eygoq0JUoArzJ/WQOqRWjwW1DRJCYOvJLLyxJdEwl/SNXbzxysRI9AhUSRwdERERERFJYUu6HEeziuCmtMfHD/RnT9UmYEFtY1KulGL+L6fwZ0o+gJopsBZM7oFREb6QyThBOxERERGRLfot8Qp2ZdUMPLb8rt4I6eAicURtAwtqG1FRpcP7O5Oxet8FaHUCSns5nhrZBY+PCOMnT0RERERENiw9vxwv/XQKADAjOgTje/K56aZiQW0DDlzIx8s/nkBafjkA4ObuvlgwKQqdOnC0PiIiIiIiW1ap1eGp9UdRUlmNzq4CL8Z0lTqkNoUFdTtWUqnF0u1nse5gOgDAX+WIRbdFISbKX+LIiIiIiIioNVi85QxOZajh6azA9G4VnG/aRCyo26ndSVfwyk8nkVVcM+jY/UM6Ye6E7lA5KiSOjIiIiIiIWoPNCRlYdzAdMhnwzr96oST5kNQhtTksqNuZ8qpqLN5yBt8eugSgZtCxpXf2QnS4t8SRERERERFRa5FypQTzfjoJAHhmVBcM7+qNbckSB9UGsaBuR45fKsLz3yUgNa8MMhkwIzoU/x7XDc4O/DYTEREREVGNkkotHv/mKMqrdIgO74DnxnSDXlctdVhtksU6yF+8eBGPPPIIQkND4eTkhPDwcMyfPx9VVVWWOqXN0ukFPtyVjDtX/YXUvDL4qxyx7pEheH1SDxbTRERERERkoNcL/HvjcZzPrakb3r+3H+zknD63uSxWbZ09exZ6vR6ffvopunTpglOnTmHmzJkoKyvDihUrLHVam3NFXYmnv43HodQCAMAtvQLw5u094eHsIHFkRERERETU2qz6/Tx+PZ0DBzs5PnloAHzclFKH1KZZrKAeP348xo8fb1gOCwtDUlISVq1axYLaTP46n4dnv01AXqkGLg52WHhbT9zZvyNkMn7CRERERERExn4/l4sVsUkAgEW3RaFvsIe0AbUDVu0PXFxcDC8vrwZf12g00Gg0hmW1Wg0A0Gq10Gq1LTp37f4tPU5roNcLfPZHKlbuTIFeABF+rvjg3j4I9XZBdbX5nn1oTzmzFubMdMyZ6Zgz05kzZ8w7ERG1RZcKyvHst/EQArhvcDDuHdxJ6pDaBasV1CkpKfjggw8abZ1esmQJFi5cWGd9bGwsnJ2dzRJHXFycWY4jlYpq4JsUOU4X1jz+PthHj7tCipB46HckWuicbT1nUmDOTMecmY45M505clZeXm6GSFqnJUuW4KeffsLZs2fh5OSE6OhoLFu2DBEREY3ut3HjRrz22mu4ePEiunbtimXLlmHixIlWipqIiK6nokqHx785iuIKLfoEe2DB5CipQ2o3TC6o586di2XLljW6TWJiIrp3725YzsjIwPjx43HXXXdh5syZDe43b948zJ4927CsVqsRHByMmJgYqFQqU0M1otVqERcXh7Fjx0KhaJtzMacVlOPx/8XjfGEZHOzlWHBrd/zLgl2820POrI05Mx1zZjrmzHTmzFlt76n26Pfff8esWbMwaNAgVFdX45VXXkFMTAzOnDkDFxeXevf566+/cN9992HJkiW49dZbsX79ekyZMgXHjh1Dz549rfwOiIjon4QQeOXnkziTpYa3qwM+ebA/lPZ2UofVbphcUM+ZMwfTp09vdJuwsDDD15mZmRg1ahSio6Px2WefNbqfUqmEUln3oXiFQmG2m0ZzHsua9p/Px5PrjqKoXAt/lSM+nzoQvYLcrXLutpozKTFnpmPOTMecmc4cOWvPOd+xY4fR8tq1a+Hr64ujR4/ipptuqnef999/H+PHj8eLL74IAFi8eDHi4uLw4Ycf4pNPPrF4zERE1Liv/rqIn+MzYCeX4cP7+yPA3UnqkNoVkwtqHx8f+Pj4NGnbjIwMjBo1CgMGDMCaNWsgl1tslq527dtD6Xht0ylU6wX6BLnj86kD4atylDosIiJq54qLiwGg0fFP9u/fb9S7DADGjRuHTZs2WTI0IiJqgkOpBXhja82Doa9MjMQNYR0kjqj9sdgz1BkZGRg5ciRCQkKwYsUK5ObmGl7z9/e31GnbFSEElu1Iwie/nwcATOoTiOX/6g1HBbtoEBGRZen1ejz//PMYNmxYo123s7Oz4efnZ7TOz88P2dnZ9W5vyQFIa49z7f9tDeOXFuOXFuM3r8yiCjz5v6Oo1gtM6u2PhwZ3bDS21ha/qaQagNRiBXVcXBxSUlKQkpKCoKAgo9eEEJY6bbtRrdNj3k8nsfHoZQDAC2O64dnRXTglFhERWcWsWbNw6tQp7Nu3z6zHtcYApEDbH7SP8UuL8UuL8becRge8f8oO+eUydHQWGO54Gdu3X27Svq0h/paw9gCkFiuop0+fft1nral+FVU6PL3+GHaevQK5DFh6R2/cPShY6rCIiMhGPP3009iyZQv27t1b50Pxf/L390dOTo7RupycnAZ7o1lyAFKg7Q/ax/ilxfilxfjNQwiB5747gYzyHHi5KLD+iRsQ6HH956ZbS/zNJdUApFadh5qur7hCi0fWHsaRtEIo7eX48P7+GNvD7/o7EhERtZAQAs888wx+/vln7NmzB6GhodfdZ+jQodi5cyeef/55w7q4uDgMHTq03u2tMQCpJY5nbYxfWoxfWoy/Zf67MxnbT+dAYSfDpw8NRIiPaR9WSh1/S1l7AFIW1K1IcYUWU1cfxPHLxVA52uOLaYMwOLThgWCIiIjMadasWVi/fj02b94MNzc3w3PQ7u7ucHKqad2YOnUqOnbsiCVLlgAAnnvuOYwYMQLvvPMObrnlFmzYsAFHjhy57sweRERkfjtOZePduHMAgDem9MSgzqwlLI3DbrcSReVVePCLmmLa01mBDY8NZTFNRERWtWrVKhQXF2PkyJEICAgw/Pvuu+8M26SnpyMrK8uwHB0djfXr1+Ozzz5Dnz598MMPP2DTpk2cg5qIyMoSs9SY/X0CAGB6dGfcM6iTtAHZCLZQtwJF5VV44IuDOJ2phpeLA9bPHILu/i1/joyIiMgUTRk0dM+ePXXW3XXXXbjrrrssEBERETVFfqkGj351BOVVOtzYxRv/uSVS6pBsBgtqiZVUavHQ6kM4nalGBxcHrJ95AyL83aQOi4iIiIiI2oCqaj2eXHcMGUUV6NzBGR/e3w/2duyIbC0sqCVUqdXhsa+P4mRGMbxcHLDhsRvQ1Y/FNBERERERXZ8QAgv+7zQOpRbAVWmPL6YNhIezg9Rh2RR+dCGRap0ez34bj/0X8uGqtMdXMwazmCYiIiIioiZb8+dFrD+YDpkM+O99fdHFl/WEtbGgloAQAvN+OonYMzlwsJfj86kD0SvIXeqwiIiIiIiojfjtTA4Wbz0DAJg3oTtu7s6pdqXAgloC/92Zgo1HL0MuAz68rx+GhneQOiQiIiIiImojTmUU49kN8RACuG9wMGYOD5M6JJvFgtrKfjmeiZW/1cwN9+btvRAT5S9xRERERERE1FZkF1cajei96LaekMlkUodls1hQW9Gx9EL8e+NxAMDM4aG4bzDnhiMiIiIioqYp01Tjka8OI1tdia6+rvjogf5QcERvSTH7VpJZVIHHvj6Kqmo9xkT6Yu4Ezg1HRERERERNo9MLPLchwTDd7pfTB8HdSSF1WDaPBbUVVFXrMWv9MeSVatDd3w3v39sPdnJ2yyAiIiIioqZ5a1sifkusGdT4s6kDEezlLHVIBBbUVvHWtkTEpxdB5WiPz6cOhIuS038TEREREVHTfHMgDav3pQIA3rmrDwaEeEocEdViQW1h/3c8E2v/uggAePfuvvwkiYiIiIiImmxP0hUs+OU0AODfMd0wqU+gxBHRtVhQW9CF3FLM/fEEAODJkeEY04NzwxERERERUdOcvFyMp9Ydg04vcEf/jpg1qovUIdE/sKC2EK1Ojxe+S0BZlQ5DQr0wZ2w3qUMiIiIiIqI24lJBOWasPWyYHmvpHb05PVYrxILaQj7clYLjl4uhcrTHe/f2hT2HsyciIiIioiYoKKvCtC8PIa9Ug8gAFVY92B8O9qwnWiN+VywgPr0QH+5OAQC8cXsvBLg7SRwRERERERG1BRVVOjzy1WFcyCtDRw8nrJ0xCG6OnB6rtWJBbWaVWh1mf38cOr3AbX0DMZmDBhARERERURPo9ALPbog3zBC0dsYg+KkcpQ6LGsGC2sz+uzMZqXll8Fc5YtHknlKHQ0REREREbYAQAvN/OYW4MzVzTX8xbRC6+rlJHRZdBwtqMzqbrcZney8AABbdFgV3Z3bNICIiIiKi6/t4z3n870A6ZDLg/Xv6YnCol9QhUROwoDYTnV5g7o8nUa0XGB/lj5gof6lDIiIiIiKiNuDHo5ex/NckAMDrt/bAhF4BEkdETcWC2kzWH0xDwqUiuCrtsWBylNThEBERERFRG7A76Qpe/vEEAOCxm8IwY1ioxBGRKVhQm0FhWZXhE6WXxkfA350DBxARERERUeOOphXgyf8dRbVeYHKfQMwd313qkMhELKjN4L3fzkFdWY3IABUeGBIidThERERERNTKJWapMWPNYVRq9RgZ4YMVd/WBXC6TOiwyEQvqFkq5UoL/HUwHALx2ayTs+EtARERERESNSM8vx9QvD0FdWY0BIZ5Y9cAAONizNGuL+F1roTe2JkKnF4jp4YfocG+pwyEiIiIiolbsSkklHlx9ELklGnT3d8OX0wbBycFO6rComVhQt8CBC/nYk5QLhZ0Mr0yMlDocIiIiIiJqxYortJi6+hDSC8rRycsZXz88mFPttnEWLagnT56MTp06wdHREQEBAXjooYeQmZlpyVNajRAC78aeAwDcO6gTOnu7SBwRERERERG1VhVVOjyy9jDOZpfAx02J/z0yBL4qDmbc1lm0oB41ahS+//57JCUl4ccff8T58+fxr3/9y5KntJp9KXk4dLEADvZyzBrVRepwiIiIiIioldLq9Hhq3VEcSSuEm6M9vn54MDp1cJY6LDIDe0se/IUXXjB8HRISgrlz52LKlCnQarVQKNpu1wYhBN652jr94JAQTpNFRERERET10ukF/r3xOHYn5cJRIcea6YMQGaCSOiwyE6s9Q11QUIB169YhOjq6TRfTALDnXC4SLhXBUSHHkyPDpQ6HiIiIiIhaIb1e4NWfT2JzQibs5TKsemAABnb2kjosMiOLtlADwMsvv4wPP/wQ5eXluOGGG7Bly5YGt9VoNNBoNIZltVoNANBqtdBqtS2Ko3b/lh4HAD7ZkwIAuH9QMDwc5WY5ZmtkzpzZCubMdMyZ6Zgz05kzZ8w7ERE1hRACi7acwYbDlyCXAe/d2xejuvtKHRaZmckF9dy5c7Fs2bJGt0lMTET37t0BAC+++CIeeeQRpKWlYeHChZg6dSq2bNkCmazufM1LlizBwoUL66yPjY2Fs7N5njGIi4tr0f7ppcDBVHvIZQKdKs9j27bzZomrNWtpzmwRc2Y65sx0zJnpzJGz8vJyM0RCRETtmRACS3ecxdq/LgIAlv+rD27tHShtUGQRJhfUc+bMwfTp0xvdJiwszPC1t7c3vL290a1bN0RGRiI4OBgHDhzA0KFD6+w3b948zJ4927CsVqsRHByMmJgYqFQte85Aq9UiLi4OY8eObVGX8+e/OwEgG5N7B+KB23u1KKbWzlw5syXMmemYM9MxZ6YzZ85qe08RERE15P2dyfj09wsAgDdv74k7BwRJHBFZiskFtY+PD3x8fJp1Mr1eDwBG3bqvpVQqoVQq66xXKBRmu2lsybEuFZRj++lsAMBjI7rYzI2sOfNvK5gz0zFnpmPOTGeOnDHnRETUmM/+SMV7vyUDAF67tQceGBIicURkSRZ7hvrgwYM4fPgwbrzxRnh6euL8+fN47bXXEB4eXm/rdFvw5Z+p0AtgeFdv9AjkyHxERERERPS3vVky/Li/pph+cVwEHrkxVOKIyNIsNsq3s7MzfvrpJ4wePRoRERF45JFH0Lt3b/z+++/1tkK3dhVVOvxw9DIA8BeDiIiIiIiMbDx6GT9etAMAPHNzF8wa1UXiiMgaLNZC3atXL+zatctSh7e6LScyUVJZjU5ezripa/O6vBMRERERUfuzKT4Dr24+AwB4ODoEs8d2kzgisharzUPd1q07mA4AuHdwMOTyuiOUExERERGR7dkUn4HZ3ydACGCYnx5zx3erd0Yjap8sPg91e3AmU42ES0Wwl8tw14BgqcMhIiIiIqJWoLaY1gvgnoEdcYN9GotpG8MW6ibYcLimdXpclD983Nre899ERERERGRe1xbT9w0OxqJJPcCOrLaHBfV1aHV6bDmRBQC4ayDnjyMiIiIisnX/LKbfnNKLj4XaKBbU1/FHci4Kyqrg7eqAG7t4Sx0OERERERFJiMU0XYsF9XVsis8EANzaOxD2dkwXEREREZGtYjFN/8QKsRFlmmrEnckBAEzp11HiaIiIiIiISCospqk+LKgbEXcmBxVaHTp3cEafIHepwyEiIiIiIgn8cPQyi2mqF6fNasT2UzWDkU3uE8jh74mIiIiIbNA3B9Lw2qZTAID7BnfCm1N6spgmAxbUDajU6rD3XB4AICbKX+JoiIiIiIjI2r744wLe2JoIAJge3RnzJ/VgQxsZYUHdgD9T8lCh1SHQ3RFRgSqpwyEiIiIiIiv6YGcy3ok7BwB4amQ4XhwXwWKa6mBB3YDawcjG9PDjLw4RERERkY0QQmD5r0n4eM95AMC/Y7rh6Zu7ShwVtVYsqOuh1wv8lngFADC2h5/E0RARERERkTUIIbDw/85g7V8XAQD/uSUSjw4PkzYoatU4ync94i8VIa9UAzelPYaEdpA6HCIiIqvYu3cvJk2ahMDAmsE4N23a1Oj2e/bsgUwmq/MvOzvbOgETEZmRXi/wys8nDcX04ik9WUzTdbGgrseepJrW6ZsifOBgzxQREZFtKCsrQ58+ffDRRx+ZtF9SUhKysrIM/3x9fS0UIRGRZWh1eszZeBzfHroEuQxYcVcfPHRDiNRhURvALt/12JdSM7r3iG4+EkdCRERkPRMmTMCECRNM3s/X1xceHh7mD4iIyAoqtTrMWncMO89egb1chvfu7YtbewdKHRa1ESyo/0FdqcXxS0UAgGFdvKUNhoiIqA3o27cvNBoNevbsiQULFmDYsGENbqvRaKDRaAzLarUaAKDVaqHValscS+0xzHEsKTB+aTF+aUkRv7pCi8fXxeNIWhGU9nJ8cG8fjIrwaVYMzL+0zBm/KcdgQf0PBy8UQC+AUG8XdPRwkjocIiKiVisgIACffPIJBg4cCI1Ggy+++AIjR47EwYMH0b9//3r3WbJkCRYuXFhnfWxsLJydnc0WW1xcnNmOJQXGLy3GLy1rxa+uAj5JtENGuQxOdgIzI6pQcf4wtp1v2XGZf2mZI/7y8vImb8uC+h/+vNrde1gXDkZGRETUmIiICERERBiWo6Ojcf78eaxcuRLffPNNvfvMmzcPs2fPNiyr1WoEBwcjJiYGKpWqxTFptVrExcVh7NixUCgULT6etTF+aTF+aVkz/vSCcsz46igyyivg7eqANdMGoLu/W4uOyfxLy5zx1/aeagoW1P9Q+/z0jezuTUREZLLBgwdj3759Db6uVCqhVCrrrFcoFGa9gTP38ayN8UuL8UvL0vGfzVbjodWHkVuiQbCXE/73yBCEdHAx2/GZf2mZI35T9mdBfY28Ug1SrpQCAG4IYws1ERGRqRISEhAQECB1GERE9TqaVoAZaw5DXVmN7v5u+PrhwfBVOUodFrVhLKivcSytEADQzc8VHs4OEkdDRERkXaWlpUhJSTEsp6amIiEhAV5eXujUqRPmzZuHjIwMfP311wCA9957D6GhoYiKikJlZSW++OIL7Nq1C7GxsVK9BSKiBu1OuoIn/3cUlVo9BoZ4YvW0QXB3brstsdQ6sKC+xtH0moJ6QIinxJEQERFZ35EjRzBq1CjDcu2zztOmTcPatWuRlZWF9PR0w+tVVVWYM2cOMjIy4OzsjN69e+O3334zOgYRUWuw8cglzP3pJHR6gZERPlj1wAA4OdhJHRa1Ayyor1HbQt2/EwtqIiKyPSNHjoQQosHX165da7T80ksv4aWXXrJwVEREzSeEwAe7UvBu3DkAwO39OuLtf/WGwk4ucWTUXrCgvqqqWo/jl4sBsIWaiIiIiKitq9bp8Z9Np7Dh8CUAwFMjw/HiuAjIZDKJI6P2hAX1Vacyi1FVrYeXiwNCvc03yh8REREREVlXmaYaT68/ht1JuZDLgIW39cRDN4RIHRa1Qyyor7q2uzc/tSIiIiIiaptySzR45KvDOHG5GI4KOf57bz/ERPlLHRa1UyyorzqZUdPdu2+wu8SREBERERFRc1zILcW0NYdwqaACXi4O+GLaQI6PRBbFgvqqM5lqAEBUIAtqIiIiIqK25mhaAR796ggKy7UI6eCMtTMG81FOsjgW1AAqqnQ4n1sKAIgKVEkcDRERERERmWLLiUzM+f44NNV69Alyx+rpg+DtqpQ6LLIBVhkvXqPRoG/fvpDJZEhISLDGKU2SlFMCvQC8XR3g48ZfPCIiIiKitkAIgQ92JuPp9fHQVOsxJtIX3z52A4tpshqrFNQvvfQSAgMDrXGqZjmdWfP8dGSAigOSERERERG1AZpqHWZ/fxzvXJ1j+pEbQ/HpQwPh7MBOuGQ9Fv9p2759O2JjY/Hjjz9i+/btlj5ds/D5aSIiIiKitiO/VIPHvzmKI2mFsJPLsHByFB7ktFgkAYsW1Dk5OZg5cyY2bdoEZ2fn626v0Wig0WgMy2p1TaGr1Wqh1WpbFEvt/vUd59TVEb4jfJ1bfJ72pLGcUf2YM9MxZ6Zjzkxnzpwx70RE0kq5UoKH1x5BekE53Bzt8fED/TG8q4/UYZGNslhBLYTA9OnT8cQTT2DgwIG4ePHidfdZsmQJFi5cWGd9bGxskwrypoiLizNa1gvgTKYdABlyk+Ox7XK8Wc7TnvwzZ3R9zJnpmDPTMWemM0fOysvLzRAJERE1x77kPDy57ihKKqsR7OWENdMHoYuvm9RhkQ0zuaCeO3culi1b1ug2iYmJiI2NRUlJCebNm9fkY8+bNw+zZ882LKvVagQHByMmJgYqVctG39ZqtYiLi8PYsWOhUCgM69Pyy6E9sA9Kezmm3j4BdnI+Q12roZxRw5gz0zFnpmPOTGfOnNX2niIiIutadzANr28+DZ1eYGCIJz59aAA6cPAxkpjJBfWcOXMwffr0RrcJCwvDrl27sH//fiiVxj/kAwcOxAMPPICvvvqqzn5KpbLO9gCgUCjMdtP4z2OlFVbWxOzjCkelg1nO0d6YM/+2gjkzHXNmOubMdObIGXNORGRdWp0ei7ecwdf70wAAt/friKV39oLS3k7iyIiaUVD7+PjAx+f6zyj897//xRtvvGFYzszMxLhx4/Ddd99hyJAhpp7WYmrnnw7z4aTvREREREStSX6pBrPWH8OBCwUAgDlju+Hpm7twZh5qNSz2DHWnTp2Mll1dXQEA4eHhCAoKstRpTXYhtwwAEO7jKnEkRERERERU60ymGjO/PoKMogq4ONhh5T19ERPlL3VYREZsfpK22hbqcLZQExERERG1CltPZOHfG4+jQqtDSAdnfD51ILr5cfAxan2sVlB37twZQghrna7JzrOFmoiIiIioVdAL4N3fkrHq91QAwPCu3vjgvn7wcOZYR9Q62XQLdUFZFQrKqgDwGWoiIiIiIimVVFZjdZIcpwpriumZw0Px8vjusLeTSxwZUcNsuqBOzavp7h3o7ghnB5tOBRERERGRZM7nluKxr4/gfKEcDvZyLL2jF+7o33rGXSJqiE1XkZcKKgAAnTo4SxwJEREREZFt2n4yCy/+cAKlmmq4KwS+fHgQBoR6Sx0WUZPYeEFdDgAI8mRBTURERERkTdU6Pd7+NQmf7b0AABjU2ROTO+Sid5C7xJERNZ1NP5BwqbCmoA5mQU1EREREZDW5JRo8uPqgoZieOTwUX00fABXHHqM2xqZbqC8X1nT5DvZykjgSIiIiIiLbcDStAE+tO4YctQYuDnZYflcfTOwVAK1WK3VoRCaz6YLa0ELtxRZqIiIiIiJLEkJg7V8X8ebWRFTrBbr4uuKTBwegiy+nr6W2y2YL6mqdHplFlQCAIE+2UBMRERERWUqZphrzfjqJX45nAgBu7R2AZXf2hovSZssRaids9ic4W10JnV7AwU4OPzdHqcMhIiIiImqXzuWUYNa6Y0i+Ugp7uQyvTIzEjGGdIZPJpA6NqMVstqCunTKro6cT5HL+MhMRERERmZMQAt8fuYT5v5xGpVYPXzclPnqgPwZ19pI6NCKzsdmC+nJh7ZRZ7O5NRERERGROpZpq/Ofnk9iUUNPF+6ZuPlh5dx90cFVKHBmRedlsQX2lRAMA8FOxuzcRERERkbmczizGM+vjcSGvDHZyGebEdMMTN4WzVyi1S7ZbUKtrBiTzU/FTMiIiIiKilhJC4H8H07F4yxlUVesR4O6ID+7rh4Hs4k3tmO0W1FdbqH05IBkRERERUYuoK7WY99NJbD2RBQAY3d0XK+7qA08XB4kjI7Ismy2oc662UPu6sYWaiIiIiKi54tML8dyGBKQXlMNeLsPcCd3xyI2hHMWbbILNFtSGFmo+Q01EREREZDKdXuDj3Sl4b2cydHqBjh5O+PD+fujXyVPq0IisxiYLaiHENV2+2UJNRERERGSKy4XleOG7BBy+WAgAmNQnEG9M6Ql3J4XEkRFZl00W1MUVWlRV6wEAPiyoiYiIiIiabHNCBv7z8ymUaKrhqrTHotuicHu/juziTTbJJgvq2tZpD2cFHBV2EkdDRERERNT6lVRq8frm0/g5PgMA0L+TB967px86dXCWODIi6dhmQa1md28iIiIioqY6mlaA579LwKWCCshlwDM3d8UzN3eBvZ1c6tCIJGWTBfXfI3xzQDIiIiIiooZUVevx4a5kfLg7BXoBBHk64b17+nJuaaKrbLKg/nuEb7ZQExERERHVJym7BLO/T8DpTDUA4PZ+HbHwtiioHDnwGFEtmyyo80prCmofVxbURERERETX0ukFPv/jAt6NPYcqnR4ezgosvq0nJvUJlDo0olbHJgvqwvIqAICni4PEkRARERERtR4X88owZ+NxHE2rmQ5rdHdfLLmzFx+VJGqAbRbUZVcLamd2VyEiIiIi0usF/ncwDUu2nUWFVgdXpT1ev7UH7hoYxOmwiBphmwV1uRYA4OnMFmoiIiIism0ZRRV4+YcT2JeSBwAYGtYBy+/qjSBPTodFdD02WVAXscs3EREREdk4IQS+P3IJb2xJRImmGo4KOeaO746pQztDLmerNFFT2GRBXcAu30RERERkw9LzyzHv5xP4MyUfANCvkwfeuasPwnxcJY6MqG2xuYK6WqeHurIaALt8ExEREZFt0ekF1v51ESt+TUKFVgelvRz/jonAjGGdYW8nlzo8ojbH5grq4qvFNAC4O7GFmoiIiIhsQ8qVErz0wwkcSy8CAAwJ9cKyO3ujs7eLtIERtWEW/Riqc+fOkMlkRv+WLl1qyVNeV9HVAclUjvb8FI6IiIiI2j2tTo8PdyVj4vv7cCy9CK5Ke7x5e098O/MGFtNELWTxFupFixZh5syZhmU3NzdLn7JRnIOaiIiIiGzFqYxivPjDCSRmqQEAoyJ88ObtvRDo4SRxZETtg8WbaN3c3ODv72/45+Ii7adgxVdbqD34/DQREZGRvXv3YtKkSQgMDIRMJsOmTZuuu8+ePXvQv39/KJVKdOnSBWvXrrV4nER0fWWaary59Qxu++hPJGap4emswHv39MWX0wexmCYyI4u3UC9duhSLFy9Gp06dcP/99+OFF16AvX39p9VoNNBoNIZltbrmkzStVgutVtuiOGr3LyqvOb6rg12Lj9ne1eaHeWo65sx0zJnpmDPTmTNn7TnvZWVl6NOnDx5++GHccccd190+NTUVt9xyC5544gmsW7cOO3fuxKOPPoqAgACMGzfOChETUX1+S7yCxVvPIrO4EgBwa+8ALJgcBW9XpcSREbU/Fi2on332WfTv3x9eXl7466+/MG/ePGRlZeHdd9+td/slS5Zg4cKFddbHxsbC2dk8E8sfOX4KgB3KCnOxbds2sxyzvYuLi5M6hDaHOTMdc2Y65sx05shZeXm5GSJpnSZMmIAJEyY0eftPPvkEoaGheOeddwAAkZGR2LdvH1auXMmCmkgCWcWV+OKsHCf3JwAAgjydsPi2nhjV3VfawIjaMZML6rlz52LZsmWNbpOYmIju3btj9uzZhnW9e/eGg4MDHn/8cSxZsgRKZd1PyObNm2e0j1qtRnBwMGJiYqBSqUwN1YhWq0VcXBw6du4CpKaiS+dgTJwY1aJjtne1ORs7diwUCo6I3hTMmemYM9MxZ6YzZ85qe08RsH//fowZM8Zo3bhx4/D88883uI8le6PVHufa/9saxi+tthp/tU6Pbw5ewsrfUlChlcNeLsMjwzpj1sgwOLWhXpltNf+1GL+0pOqNZnJBPWfOHEyfPr3RbcLCwupdP2TIEFRXV+PixYuIiIio87pSqay30FYoFGa7aayoFgAAd2cH3og2kTnzbyuYM9MxZ6Zjzkxnjpwx53/Lzs6Gn5+f0To/Pz+o1WpUVFTAyanuc5rW6I0GtP0eHIxfWm0p/rQS4LsLdsgolwEAQt0E7g6rRmB1Mnb/lixxdM3TlvJfH8YvLWv3RjO5oPbx8YGPj4+puwEAEhISIJfL4esrXbeTkqvzULsqbW4KbiIiIslZsjca0PZ7cDB+abWl+IsrtFj5WwrWn74EIQB3J3vMGR0Ot7zTGBfT+uOvT1vKf30Yv7Sk6o1msapy//79OHjwIEaNGgU3Nzfs378fL7zwAh588EF4enpa6rTXVaqpKajdHFlQExERtYS/vz9ycnKM1uXk5EClUtXbOg1YpzeaJY5nbYxfWq05fr1eYOPRS1i2IwkFZTXTwd7RryNeuSUS7ko5tm073arjbwrGLy3Gb1pvNItVlUqlEhs2bMCCBQug0WgQGhqKF154wehTaSnUtlCzoCYiImqZoUOH1hngMy4uDkOHDpUoIqL2LeFSEeZvPoXjl4sBAF18XbFochSiu3gDaLvPvhK1ZRarKvv3748DBw5Y6vDNVttC7apsu5+6EBERWUJpaSlSUlIMy6mpqUhISICXlxc6deqEefPmISMjA19//TUA4IknnsCHH36Il156CQ8//DB27dqF77//Hlu3bpXqLRC1S/mlGry9IwnfHbkEoObRxefHdMW06M5Q2Mkljo7IttlcM20pW6iJiIjqdeTIEYwaNcqwXNurbNq0aVi7di2ysrKQnp5ueD00NBRbt27FCy+8gPfffx9BQUH44osvOGUWkZlU6/RYdzAd78QmQX31HvaO/h0xd0J3+Lo5ShwdEQG2WFDXtlCzoCYiIjIycuRICCEafH3t2rX17hMfH2/BqIhs08EL+Zj/y2mczS4BAEQFqrDotigMCPGSODIiupbNVZUlVwtqFQtqIiIiImplLhWUY+mOs9h6IgsA4OGswL9jInDf4E6wk8skjo6I/smmqkohgFKNDgCfoSYiIiKi1qOkUouPdp/Hl3+moqpaD7kMuHdwJ7wYEwFPFwepwyOiBthUQV2lB3T6mq5sfIaaiIiIiKRWrdPjuyOX8G7sOeRfnQZrWJcO+M8tPRAZ0PK52YnIsmyqqrzaOA0AcFLYSRcIEREREdm8vedy8ebWRCTl1DwnHebjglcnRuLm7r6Qydi9m6gtsKmCWquv+d9RIYecz6AQERERkQRSrpTgza2J2J2UC6DmOennR3fFAzeEcBosojbGRgtqtk4TERERkXVdKanEf3cm49tDl6DTC9jLZZg6tDOeHd0FHs58TpqoLbLJgprdvYmIiIjIWkoqtfhs7wV88UcqKrQ1zyCO7eGHeRO6I8zHVeLoiKglbKqgrmILNRERERFZiaZah3UH0vHh7hQUXB1wrG+wB+ZO6I4bwjpIHB0RmYNNFdRafc1z0yyoiYiIiMhS9HqBzccz8E7sOVwurABQM+DYS+O6Y1yUHwccI2pHbKqgrrpmUDIiIiIiInMSQmDPuVy8vSMJiVlqAICfSonnx3TDXQOCYM8Bx4jaHZsqqPkMNRERERFZwtG0Aiz/NQkHLhQAANwc7fHkyHDMiA6FkwPvPYnaK5sqqKuuzkPNLt9EREREZA7HLxXh3bhz+P1czRRYDvZyTI/ujCdHhMPThSN3E7V3NlVQs4WaiIiIiMzhdGYxVsadw2+JVwAA9nIZ7hoYhKdv7oqOHk4SR0dE1mKTBbWSz1ATERERUTOcyynByrhz2H4qGwAglwG39wvCc6O7olMHZ4mjIyJrs6mCuoot1ERERETUDBdyS/Heb8n4vxOZEAKQyYBJvQPx3JiuCOdc0kQ2y6YKak6bRURERESmOJ9bio92p2BTfAb0ombdhJ7+eH5MN0T4u0kbHBFJzsYK6pr/2UJNRERERI05m63Gh7tSsPVkFsTVQnpMpC+eH9MNPTu6SxscEbUaNlVQcx5qIiIiImrMpVLgqfUJiLs62BgAjIn0wzM3d0GfYA/pAiOiVsmmCmqtoaBmCzURERER/e1oWiH+u/Mcfj9nD+AKZDJgYs8AzBrVBT0CVVKHR0StlE0V1LXzUDs5sKAmIiIisnVCCBy4UIAPdiXjr/P5AAAZBCb3CcTTN3dFVz8+I01EjbOpgtrQQm3PgpqIiIjIVun1AjvPXsEnv5/H0bRCADXzSN/eLxARujRMu7MXFAqFxFESUVtgYwV1zSjfbKEmIiIisj2aah02x2fi073ncT63DADgYCfHPYOC8fiIMPi5KrBtW5rEURJRW2JTBTUHJSMiIiKyPepKLdYfTMeX+1JxpUQDAHBztMeDN4RgRnRn+KocAQBarVbKMImoDbKpgppdvomIiIhsR466El/uS8W6g+ko1VQDAPxVjnjkxlDcOzgYbo7s1k1ELWNTBXX11TkElWyhJiIiImq3knNK8PkfF/BzfAa0upobwK6+rnh8RDgm9wmEgz3vBYnIPGyqoNZdbaFW2PGPKBEREVF7otcL7E3OxZd/XsTec7mG9YNDvfDEiDCM7OYLuVwmYYRE1B7ZVkF9tYWaBTURERFR+1BeVY2fjmVgzZ+phoHG5DIgpoc/HhsRhv6dPCWOkIjaM4sW1Fu3bsWiRYtw4sQJODo6YsSIEdi0aZMlT9moahbURERERO1CZlEFvt6fhm8PpaO4omYwMTelPe4ZFIxp0Z0R7OUscYREZAssVlD/+OOPmDlzJt566y3cfPPNqK6uxqlTpyx1uiap7fLtwIKaiIiIqE06ll6IL/elYvupbOj0Na0lIR2cMSO6M/41MBiuSpvqgElEErPIX5zq6mo899xzWL58OR555BHD+h49eljidE1maKG25/MzRERERG1FpVaH7aey8NVfaUi4VGRYPzSsAx6+MRQ3d/eFHZ+PJiIJWKSgPnbsGDIyMiCXy9GvXz9kZ2ejb9++WL58OXr27NngfhqNBhqNxrCsVqsB1MwJ2NJ5AauqqqATNX9oZXod5xlsgtocMVdNx5yZjjkzHXNmOnPmjHknsp5LBeVYdzAd3x+5hIKyKgA1PQ1v6xuIGcNC0SNQJXGERGTrLFJQX7hwAQCwYMECvPvuu+jcuTPeeecdjBw5EufOnYOXl1e9+y1ZsgQLFy6ssz42NhbOzi17Dqamu3fN2929ayec2RuoyeLi4qQOoc1hzkzHnJmOOTOdOXJWXl5uhkiIqCE6vcDec7n45kAadiddgbjawzDQ3REP3BCCuwcGw8dNKW2QRERXmVRWzp07F8uWLWt0m8TEROj1NQ8rv/rqq7jzzjsBAGvWrEFQUBA2btyIxx9/vN59582bh9mzZxuW1Wo1goODERMTA5WqZZ9AFpdVAgf3AgBuGT8OTg52LTqeLdBqtYiLi8PYsWOhUCikDqdNYM5Mx5yZjjkznTlzVtt7iojMq6CsCt8fuYR1B9NwqaDCsH54V29MHdoZoyJ8YM9xcIiolTGpoJ4zZw6mT5/e6DZhYWHIysoCYPzMtFKpRFhYGNLT0xvcV6lUQqms+4mjQqFo+U2j/O8ues6ODvyDbAKz5N/GMGemY85Mx5yZzhw5Y86JzEcIgWPpRVh3IA1bTmahqrqmUcbdSYG7BgThgRtCEOrtInGUREQNM6mg9vHxgY+Pz3W3GzBgAJRKJZKSknDjjTcCqGkduHjxIkJCQpoXaQtpa4f4BjhoBREREZGECsqq8HN8Br47nI5zOaWG9b06uuOhoSGY1DuQvQmJqE2wyJPEKpUKTzzxBObPn4/g4GCEhIRg+fLlAIC77rrLEqe8Lq2u5gEchZ0MMhkLaiIiIiJr0usF/jqfjw2H0xF7OgdVVxs7HBVy3NIrEFOHhqBPsIe0QRIRmchiQ3MtX74c9vb2eOihh1BRUYEhQ4Zg165d8PT0tNQpG1X7R5tzUBMRERFZT3ZxJX44egnfHblk9Gx0z44q3DOoE27rGwiVIx+lIKK2yWIFtUKhwIoVK7BixQpLncIk2qvP5ChYUBMRERFZVLVOjz3JOdhwKB27k65Af3WkbjdHe0zp2xH3DApGz47u0gZJRGQGNjN51LVdvomIiIjI/BKzSvDzRTkWr9iLvNIqw/rBnb1wz6BgTOwVwGejiahdsaGCmi3UREREROaWW6LB5oQM/HgsA4lZagByAFXo4OKAfw0Iwt2DghHu4yp1mEREFmEzBXX11b5G9myhJiIiImqRSq0OOxOv4Mdjl/H7uVzo9H/3BOzhrsOTE/pjdI8ANmQQUbtnMwV17R96e06ZRURERGSy2jmjfzp2Gf93PBPqymrDa32DPXBn/44Y38MXf+2Jw+juviymicgm2FxBLeeUWURERERNlnKlBL8kZOKX45m4mF9uWB/g7ojb+3XEHf2D0MW3pku3VquVKkwiIknYTkEt2EJNRERE1BQZRRX4v+OZ+CUhE2ey1Ib1Tgo7TOjpjzsHBOGGsA6w430VEdk42ymoa1uo+YefiIiIqI78Ug22nczC5oRMHEkrNKy3l8twUzcfTO4TiLE9/OCitJnbRyKi67KZv4h8hpqIiIjIWEmlFr+ezsEvxzPxZ0qe4X5JJgOGhHphcp+OmNDTH54uDhJHSkTUOtlcQc0WaiIiIrJl6kotdiVewbaTWdhzLhdV1XrDa72D3DG5TyBu7R0If3dHCaMkImobbK6gZgs1ERER2Zrici3iEnOw/WQW/kjOQ5Xu7yI63McFt/XtiEl9AhHq7SJhlEREbY/NFdQc5ZuIiKhxH330EZYvX47s7Gz06dMHH3zwAQYPHlzvtmvXrsWMGTOM1imVSlRWVlojVGpEQVkVYk9nY/upbPyZkofqq/dCANDF1xUTe/pjfM8ARAa4Qcb7IyKiZrGdgpqjfBMREV3Xd999h9mzZ+OTTz7BkCFD8N5772HcuHFISkqCr69vvfuoVCokJSUZllmcSSe3RINfT2dj+6ksHLhQYGhQAIDu/m6Y0DMAE3v5o6ufm4RREhG1H7ZTUPMZaiIiout69913MXPmTEOr8yeffIKtW7fiyy+/xNy5c+vdRyaTwd/f35ph0jUu5JYi7kwO4s7k4Gh6IcTfNTSiAlWY2CsA43v6I9zHVbogiYjaKZsrqDlfIhERUf2qqqpw9OhRzJs3z7BOLpdjzJgx2L9/f4P7lZaWIiQkBHq9Hv3798dbb72FqKgoa4Rsk/R6gfhLRVeL6Gyczy0zer1PkDsm9ArAhJ7+COnAZ6KJiCzJ9gpqdkMjIiKqV15eHnQ6Hfz8/IzW+/n54ezZs/XuExERgS+//BK9e/dGcXExVqxYgejoaJw+fRpBQUF1ttdoNNBoNIZltVoNANBqtdBqtS1+D7XHMMexpNBQ/JVaHf66UICdiVewKykXeaVVhtfs5TIMCfXCmEgfjO7ui4BrRue2dh7aa/7bCsYvLcYvLXPGb8oxbKegFmyhJiIiMrehQ4di6NChhuXo6GhERkbi008/xeLFi+tsv2TJEixcuLDO+tjYWDg7O5strri4OLMdSwpxcXEo0QKJhTKcLJThbJEMVfq/72Ec7QR6eAj08hKI9BBwss8B8nMQ/ycQL2HctdpD/tsyxi8txi8tc8RfXl7e5G1tp6Bml28iIqJGeXt7w87ODjk5OUbrc3JymvyMtEKhQL9+/ZCSklLv6/PmzcPs2bMNy2q1GsHBwYiJiYFKpWp+8FdptVrExcVh7NixUCgULT6eNen1AvHpBVj762Fk6N1xMrPE6PUAd0eM7l7TCj24sycc7OUSRdqwtpx/gPFLjfFLi/H/rbb3VFOwoCYiIiIAgIODAwYMGICdO3diypQpAAC9Xo+dO3fi6aefbtIxdDodTp48iYkTJ9b7ulKphFKprLNeoVCY9QbO3MezlOJyLfYm52L32Sv4/Vwu8suqAMgB1BTTUYEqjI70Q0wPP0QFqtrMCOptJf8NYfzSYvzSYvwwaX/bK6jbyIWIiIhICrNnz8a0adMwcOBADB48GO+99x7KysoMo35PnToVHTt2xJIlSwAAixYtwg033IAuXbqgqKgIy5cvR1paGh599FEp30arJYRAYlYJdiddwZ6kKziaVohrZraCi9IOXVy0uHdEL4yO9IevyrHhgxERkeRsp6CufYbajgU1ERFRQ+655x7k5ubi9ddfR3Z2Nvr27YsdO3YYBipLT0+HXP53V+PCwkLMnDkT2dnZ8PT0xIABA/DXX3+hR48eUr2FVie/VIM/z+fjj3O5+CM5D9nqSqPXu/m5YlSEL0ZG+KJPR1fE/boDE/t3bNMtREREtsJ2CmodW6iJiIia4umnn26wi/eePXuMlleuXImVK1daIaq2o1Krw9G0QvyRnIc/knNxOtP4WTxHhRzDwr0xqrsvRkb4IMjz78HY2uroukREtsp2Cuqr3an4DDURERGZkxACSTkl2Jech73JeTiUmo9Krd5om+7+bhje1Rs3dvXBkFAvOCrsJIqWiIjMyXYKan3Nhc2u9Q2ISURERG1MdnEl/jqfh33JefgjJQ+5JRqj133clBje1RvDu3pjWBdv+LrxWWgiovbIhgrqmv/t5KyoiYiIyDRXSipx4EIB9p/Px4EL+UjNKzN63VEhx5DQDleLaB9083NtMyNyExFR89lQQV37DLXEgRAREVGrl1+qwcHUmgJ6/4V8pFwpNXpdLgN6dnTHsC7eGN7FG/1DPNmNm4jIBtlOQS04DzURERHVr7hciwOp+TUF9Pl8JOWUGL0ukwGR/ioMDe+AoWEdMCjUC+5OHIWbiMjW2U5BrWdBTURERDWyiitwKLUARy4W4vDFAiTllEAI420i/NwwNLwDbgjrgBvCvODh7CBNsERE1GqxoCYiIqJ2Ta8XOJ9bikMXawroQ6kFyCiqqLNduI/L1RZob9wQ5oUOrkoJoiUiorbE9gpqDhBCRETUrlXrgfhLRYi/pMbhi4U4mlaAwnLj+Z3lMiAq0B2DOnthUGdPDOzsBR83FtBERGQa2ymo+Qw1ERFRu/b7uVx8vDsZx9LsoD14yOg1R4Uc/YI9MSi0poDu18kTrkqbuQ0iIiILsdiVZM+ePRg1alS9rx06dAiDBg2y1KnrxS7fRERE7ZtGq8PB1EIAMng6KzCwsxcGd/bCwM6e6NnRHQo7Tp1JRETmZbGCOjo6GllZWUbrXnvtNezcuRMDBw601Gkb9MDgYLiVpCOmh5/Vz01ERESWNzjUC4sn90B52glMv2MslEoOIkZERJZlsYLawcEB/v7+hmWtVovNmzfjmWeegUyC55ijAlVI6yAQ7uNi9XMTERGR5Xk4O+DeQUHYlnsCcvZIIyIiK7Daw0O//PIL8vPzMWPGjAa30Wg00Gg0hmW1Wg2gphjXarUN7dYktfu39Di2hDkzHXNmOubMdMyZ6cyZM+adiIiIalmtoF69ejXGjRuHoKCgBrdZsmQJFi5cWGd9bGwsnJ2dzRJHXFycWY5jS5gz0zFnpmPOTMecmc4cOSsvLzdDJERERNQemFxQz507F8uWLWt0m8TERHTv3t2wfPnyZfz666/4/vvvG91v3rx5mD17tmFZrVYjODgYMTExUKlUpoZqRKvVIi4uDmPHjoVCoWjRsWwFc2Y65sx0zJnpmDPTmTNntb2niIiIiEwuqOfMmYPp06c3uk1YWJjR8po1a9ChQwdMnjy50f2USiWUyrpzQCoUCrPdNJrzWLaCOTMdc2Y65sx0zJnpzJEz5pyIiIhqmVxQ+/j4wMfHp8nbCyGwZs0aTJ06lTchRERERERE1G5YfELGXbt2ITU1FY8++qilT0VERERERERkNRYvqFevXo3o6GijZ6qJiIiIiIiI2jqLj/K9fv16S5+CiIiIiIiIyOos3kJNRERERERE1B6xoCYiIiIiIiJqBot3+W4JIQQA88z5qdVqUV5eDrVazdHGm4g5Mx1zZjrmzHTMmenMmbPaa1LtNYpaxpzXeqDt/34wfmkxfmkxfmkx/r+Zcq1v1QV1SUkJACA4OFjiSIiIiIyVlJTA3d1d6jDaPF7riYiotWrKtV4mWvFH7Hq9HpmZmXBzc4NMJmvRsdRqNYKDg3Hp0iWoVCozRdi+MWemY85Mx5yZjjkznTlzJoRASUkJAgMDIZfzyamWMue1Hmj7vx+MX1qMX1qMX1qM/2+mXOtbdQu1XC5HUFCQWY+pUqna5A+IlJgz0zFnpmPOTMecmc5cOWPLtPlY4loPtP3fD8YvLcYvLcYvLcZfo6nXen60TkRERERERNQMLKiJiIiIiIiImsFmCmqlUon58+dDqVRKHUqbwZyZjjkzHXNmOubMdMyZ7Wjr32vGLy3GLy3GLy3G3zytelAyIiIiIiIiotbKZlqoiYiIiIiIiMyJBTURERERERFRM7CgJiIiIiIiImoGFtREREREREREzdCuCuqPPvoInTt3hqOjI4YMGYJDhw41uv3GjRvRvXt3ODo6olevXti2bZuVIm09TMnZ559/juHDh8PT0xOenp4YM2bMdXPcHpn6c1Zrw4YNkMlkmDJlimUDbIVMzVlRURFmzZqFgIAAKJVKdOvWzeZ+P03N2XvvvYeIiAg4OTkhODgYL7zwAiorK60UrbT27t2LSZMmITAwEDKZDJs2bbruPnv27EH//v2hVCrRpUsXrF271uJxknm8+eabiI6OhrOzMzw8PJq0jxACr7/+OgICAuDk5IQxY8YgOTnZaJuCggI88MADUKlU8PDwwCOPPILS0lKzx2/qeS5evAiZTFbvv40bNxq2q+/1DRs2SB4/AIwcObJObE888YTRNunp6bjlllvg7OwMX19fvPjii6iurpY8/oKCAjzzzDOGv6+dOnXCs88+i+LiYqPtLJV/c9/bNuV3wZzMfZ85ffr0OnkeP358q4h/7dq1dWJzdHQ02sba+Tf1PdT3uyqTyXDLLbcYtrHW98BS1/bm3sc3SrQTGzZsEA4ODuLLL78Up0+fFjNnzhQeHh4iJyen3u3//PNPYWdnJ95++21x5swZ8Z///EcoFApx8uRJK0cuHVNzdv/994uPPvpIxMfHi8TERDF9+nTh7u4uLl++bOXIpWNqzmqlpqaKjh07iuHDh4vbbrvNOsG2EqbmTKPRiIEDB4qJEyeKffv2idTUVLFnzx6RkJBg5cilY2rO1q1bJ5RKpVi3bp1ITU0Vv/76qwgICBAvvPCClSOXxrZt28Srr74qfvrpJwFA/Pzzz41uf+HCBeHs7Cxmz54tzpw5Iz744ANhZ2cnduzYYZ2AqUVef/118e6774rZs2cLd3f3Ju2zdOlS4e7uLjZt2iSOHz8uJk+eLEJDQ0VFRYVhm/Hjx4s+ffqIAwcOiD/++EN06dJF3HfffWaP39TzVFdXi6ysLKN/CxcuFK6urqKkpMSwHQCxZs0ao+2ufX9SxS+EECNGjBAzZ840iq24uNjoPfbs2VOMGTNGxMfHi23btglvb28xb948yeM/efKkuOOOO8Qvv/wiUlJSxM6dO0XXrl3FnXfeabSdJfJviXvbpvwumIsl7jOnTZsmxo8fb5TngoICs8fenPjXrFkjVCqVUWzZ2dlG21gz/815D/n5+Ubxnzp1StjZ2Yk1a9YYtrHW98AS1/bm3sdfT7spqAcPHixmzZplWNbpdCIwMFAsWbKk3u3vvvtuccsttxitGzJkiHj88cctGmdrYmrO/qm6ulq4ubmJr776ylIhtjrNyVl1dbWIjo4WX3zxhZg2bZrNFdSm5mzVqlUiLCxMVFVVWSvEVsfUnM2aNUvcfPPNRutmz54thg0bZtE4W6OmXHRfeuklERUVZbTunnvuEePGjbNgZGRua9asaVJBrdfrhb+/v1i+fLlhXVFRkVAqleLbb78VQghx5swZAUAcPnzYsM327duFTCYTGRkZZovZXOfp27evePjhh43WNeVnv6WaG/+IESPEc8891+Dr27ZtE3K53Kj4WLVqlVCpVEKj0ZgldiHMl//vv/9eODg4CK1Wa1hnifyb+962Kb8LUsb/T/XdZ1rzPsrU+K/3N8na+Rei5d+DlStXCjc3N1FaWmpYJ8W9rLmu7S3NR0PaRZfvqqoqHD16FGPGjDGsk8vlGDNmDPbv31/vPvv37zfaHgDGjRvX4PbtTXNy9k/l5eXQarXw8vKyVJitSnNztmjRIvj6+uKRRx6xRpitSnNy9ssvv2Do0KGYNWsW/Pz80LNnT7z11lvQ6XTWCltSzclZdHQ0jh49aui2dOHCBWzbtg0TJ060Ssxtja3//bc1qampyM7ONvqeu7u7Y8iQIYbv+f79++Hh4YGBAwcathkzZgzkcjkOHjxotljMcZ6jR48iISGh3mvKrFmz4O3tjcGDB+PLL79EzX2o+bQk/nXr1sHb2xs9e/bEvHnzUF5ebnTcXr16wc/Pz7Bu3LhxUKvVOH36dKuI/1rFxcVQqVSwt7c3Wm/O/Fvi3rYpvwvmYsn7zD179sDX1xcRERF48sknkZ+fb9bYgebHX1paipCQEAQHB+O2224z+vm1Zv5b8h6utXr1atx7771wcXExWm+N74Gprvfzb458NMT++pu0fnl5edDpdEZ/iAHAz88PZ8+erXef7OzserfPzs62WJytSXNy9k8vv/wyAgMD6/zwtlfNydm+ffuwevVqJCQkWCHC1qc5Obtw4QJ27dqFBx54ANu2bUNKSgqeeuopaLVazJ8/3xphS6o5Obv//vuRl5eHG2+8EUIIVFdX44knnsArr7xijZDbnIb+/qvValRUVMDJyUmiyMgSaq/rjV3zs7Oz4evra/S6vb09vLy8zHpfYI7zrF69GpGRkYiOjjZav2jRItx8881wdnZGbGwsnnrqKZSWluLZZ5+VPP77778fISEhCAwMxIkTJ/Dyyy8jKSkJP/30k+G49X1/al+TOv5r5eXlYfHixXjssceM1ps7/5a4t23K74K5WOo+c/z48bjjjjsQGhqK8+fP45VXXsGECROwf/9+2NnZSRp/REQEvvzyS/Tu3RvFxcVYsWIFoqOjcfr0aQQFBVk1/819D9c6dOgQTp06hdWrVxutt9b3wFTXu7YXFha2+GeyIe2ioCbrW7p0KTZs2IA9e/bUGXCBapSUlOChhx7C559/Dm9vb6nDaTP0ej18fX3x2Wefwc7ODgMGDEBGRgaWL19uEwV1c+zZswdvvfUWPv74YwwZMgQpKSl47rnnsHjxYrz22mtSh0d0XXPnzsWyZcsa3SYxMRHdu3e3UkSmaWr8LVVRUYH169fX+3t97bp+/fqhrKwMy5cvb1JBZ+n4ry0+e/XqhYCAAIwePRrnz59HeHh4s49by1r5V6vVuOWWW9CjRw8sWLDA6LWW5J/qaug+89577zV83atXL/Tu3Rvh4eHYs2cPRo8eLUWoBkOHDsXQoUMNy9HR0YiMjMSnn36KxYsXSxhZ86xevRq9evXC4MGDjda35u+BVNpFQe3t7Q07Ozvk5OQYrc/JyYG/v3+9+/j7+5u0fXvTnJzVWrFiBZYuXYrffvsNvXv3tmSYrYqpOTt//jwuXryISZMmGdbp9XoANZ+IJyUlmeVGojVrzs9ZQEAAFAqF0aeckZGRyM7ORlVVFRwcHCwas9Sak7PXXnsNDz30EB599FEANRe4srIyPPbYY3j11Vchl7eLp3vMpqG//yqViq3TEpkzZw6mT5/e6DZhYWHNOnbt701OTg4CAgIM63NyctC3b1/DNleuXDHar7q6GgUFBU26L2hq/C09zw8//IDy8nJMnTr1utsOGTIEixcvhkajgVKpbBXxXxsbAKSkpCA8PBz+/v51Rtqt/R1tLfkvKSnB+PHj4ebmhp9//hkKhaLR7U3Jf30scW/blN8Fc7HWfWZYWBi8vb2RkpJi1mKuJfHXUigU6NevH1JSUgBYN/9Ay95DWVkZNmzYgEWLFl33PJb6Hpjqetd2Ozu7Fn9PG9Iu7rIcHBwwYMAA7Ny507BOr9dj586dRp8UXWvo0KFG2wNAXFxcg9u3N83JGQC8/fbbWLx4MXbs2GH0DJItMDVn3bt3x8mTJ5GQkGD4N3nyZIwaNQoJCQkIDg62ZviSaM7P2bBhw5CSkmL48AEAzp07h4CAgHZfTAPNy1l5eXmdorn2AwlzP0PZHtj63//WyMfHB927d2/0X3N//0NDQ+Hv72/0PVer1Th48KDhez506FAUFRXh6NGjhm127doFvV5vKP7MEX9Lz7N69WpMnjwZPj4+1902ISEBnp6eTSrmrBX/tbEBMBQVQ4cOxcmTJ42K3bi4OKhUKvTo0UPy+NVqNWJiYuDg4IBffvmlST3zTMl/fSxxb9uU3wVzsdZ95uXLl5Gfn29UoJpDc+O/lk6nw8mTJw2xWTP/QMvew8aNG6HRaPDggw9e9zyW+h6Y6no//+b4njaoRUOatSIbNmwQSqVSrF27Vpw5c0Y89thjwsPDwzBi5EMPPSTmzp1r2P7PP/8U9vb2YsWKFSIxMVHMnz/fJqfNMiVnS5cuFQ4ODuKHH34wGir/2mk72jtTc/ZPtjjKt6k5S09PF25ubuLpp58WSUlJYsuWLcLX11e88cYbUr0FqzM1Z/Pnzxdubm7i22+/FRcuXBCxsbEiPDxc3H333VK9BasqKSkR8fHxIj4+XgAQ7777roiPjxdpaWlCCCHmzp0rHnroIcP2tVNrvPjiiyIxMVF89NFHnDarDUlLSxPx8fGGqaNqv/fXXosiIiLETz/9ZFheunSp8PDwEJs3bxYnTpwQt912W73TZvXr108cPHhQ7Nu3T3Tt2tVi02Y1dp7Lly+LiIgIcfDgQaP9kpOThUwmE9u3b69zzF9++UV8/vnn4uTJkyI5OVl8/PHHwtnZWbz++uuSx5+SkiIWLVokjhw5IlJTU8XmzZtFWFiYuOmmmwz71E6bFRMTIxISEsSOHTuEj4+PxabNMiX+4uJiMWTIENGrVy+RkpJidP9TXV0thLBc/i1xb9uU3wVzMfd9ZklJifj3v/8t9u/fL1JTU8Vvv/0m+vfvL7p27SoqKyslj3/hwoXi119/FefPnxdHjx4V9957r3B0dBSnT582eo/Wyn9z3kOtG2+8Udxzzz111lvze2CJa/v18tFc7aagFkKIDz74QHTq1Ek4ODiIwYMHiwMHDhheGzFihJg2bZrR9t9//73o1q2bcHBwEFFRUWLr1q1Wjlh6puQsJCREAKjzb/78+dYPXEKm/pxdyxYLaiFMz9lff/0lhgwZIpRKpQgLCxNvvvmm4cbFVpiSM61WKxYsWCDCw8OFo6OjCA4OFk899ZQoLCy0fuAS2L17d71/m2pzNG3aNDFixIg6+/Tt21c4ODiIsLAwozk2qXWbNm1avd/v3bt3G7bB1TmBa+n1evHaa68JPz8/oVQqxejRo0VSUpLRcfPz88V9990nXF1dhUqlEjNmzLDIB8bXO09qamqd9yOEEPPmzRPBwcFCp9PVOeb27dtF3759haurq3BxcRF9+vQRn3zySb3bWjv+9PR0cdNNNwkvLy+hVCpFly5dxIsvvmg0D7UQQly8eFFMmDBBODk5CW9vbzFnzhyjaamkir+hvy8ARGpqqhDCsvk3971tU34XzMmc95nl5eUiJiZG+Pj4CIVCIUJCQsTMmTNbXAyZK/7nn3/esK2fn5+YOHGiOHbsmNHxrJ1/U9+DEEKcPXtWABCxsbF1jmXN74Glru2N5aO5ZEKwPyARERERERGRqdrFM9RERERERERE1saCmoiIiIiIiKgZWFATERERERERNQMLaiIiIiIiIqJmYEFNRERERERE1AwsqImIiIiIiIiagQU1ERERERERUTOwoCYiIiIiIiJqBhbURERERERERM3AgpqIiIiIiIioGVhQExERERERETUDC2oiIiIiIiKiZvh/i6K8Glt8+m0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that we are using the natural logarithm with this log likelihood function.\n",
    "import math \n",
    "print(f\"We are using the natural logarithm for this log likelihood:\\n\")\n",
    "print(f\"torch.log(torch.tensor(math.exp(1)))\".ljust(40) + f\"= {torch.log(torch.tensor(math.exp(1)))}\")\n",
    "print(f\"torch.log10(torch.tensor(math.exp(1)))\".ljust(40) + f\"= {torch.log10(torch.tensor(math.exp(1))):.4f}\")\n",
    "print(f\"torch.log(torch.tensor(10))\".ljust(40) + f\"= {torch.log(torch.tensor(10)):.4f}\")\n",
    "print(f\"torch.log10(torch.tensor(10))\".ljust(40) + f\"= {torch.log10(torch.tensor(10)):.4f}\")\n",
    "\n",
    "# Draw the logarithm and exponential functions \n",
    "x1 = torch.linspace(0.001, 1, 1000)\n",
    "x2 = torch.linspace(-1, 1, 1000)\n",
    "fig=plt.figure(figsize=(12,4)); plt.subplot(1,2,1); plt.title(\"The log function\"); plt.plot(x1, torch.log(x1)); plt.grid(True); plt.subplot(1,2,2); plt.title(\"The exp function\"); plt.plot(x2, torch.exp(x2)); plt.grid(True); plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: maximize likelihood of the data w.r.t. model parameters (statistical modeling)\n",
    "# equivalent to maximizing the log likelihood (because log is monotonic)\n",
    "# equivalent to minimizing the negative log likelihood\n",
    "# equivalent to minimizing the average negative log likelihood\n",
    "\n",
    "# log(a*b*c) = log(a) + log(b) + log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have the bigram (., e)\n",
      "Now we have the bigram (e, m)\n",
      "Now we have the bigram (m, m)\n",
      "Now we have the bigram (m, a)\n",
      "Now we have the bigram (a, .)\n",
      "From this 1 word we have 5 bigrams\n",
      "These 5 bigrams are: \n",
      "tensor([ 0,  5, 13, 13,  1])\n",
      "tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w_count, w in enumerate(words[:1]):\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = s_to_i[ch1]\n",
    "    ix2 = s_to_i[ch2]\n",
    "    print(f\"Now we have the bigram ({ch1}, {ch2})\")\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "\n",
    "# Convert the bigrams into tensors \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"From this {w_count+1} word{'s' if w_count else ''} we have {xs.shape[0]} bigrams\")\n",
    "print(f\"These {xs.shape[0]} bigrams are: \\n{xs}\\n{ys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have the xenc of shape torch.Size([5, 27]) and data type torch.float32:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()    # Note that we cast this encoding to a float \n",
    "print(f\"Now we have the xenc of shape {xenc.size()} and data type {xenc.dtype}:\")\n",
    "plt.imshow(xenc); plt.draw(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first row of logits that sum to 7.2981:\n",
      "tensor([ 0.6864,  1.3338, -0.1414, -0.7356,  0.3093,  0.8409,  2.6342,  0.5260,\n",
      "         0.5368,  0.8954,  1.0906, -0.9103, -0.1501, -1.0778,  1.3961, -0.1602,\n",
      "         0.7263, -0.6999, -0.2933, -1.2533, -1.4244,  1.4685, -0.1302,  1.0654,\n",
      "         0.1282,  0.6283,  0.0084])\n",
      "This is the first row of counts that sum to 56.0421:\n",
      "tensor([ 1.9866,  3.7954,  0.8682,  0.4792,  1.3624,  2.3184, 13.9316,  1.6921,\n",
      "         1.7106,  2.4483,  2.9761,  0.4024,  0.8606,  0.3403,  4.0395,  0.8520,\n",
      "         2.0675,  0.4966,  0.7458,  0.2856,  0.2407,  4.3428,  0.8779,  2.9020,\n",
      "         1.1368,  1.8745,  1.0084])\n",
      "This is the first row of probs  that sum to 1.0000:\n",
      "tensor([0.0354, 0.0677, 0.0155, 0.0086, 0.0243, 0.0414, 0.2486, 0.0302, 0.0305,\n",
      "        0.0437, 0.0531, 0.0072, 0.0154, 0.0061, 0.0721, 0.0152, 0.0369, 0.0089,\n",
      "        0.0133, 0.0051, 0.0043, 0.0775, 0.0157, 0.0518, 0.0203, 0.0334, 0.0180])\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27, 27))                   # The hidden layer\n",
    "logits = xenc @ W                           # Our computed log-counts \n",
    "# This below is equal to the softmax function\n",
    "counts = logits.exp()                       # Equivalent to N\n",
    "probs = counts / counts.sum(1,keepdim=True) # Probabilities\n",
    "\n",
    "# The first output row is the result from the first input bigram example \n",
    "print(f\"This is the first row of logits that sum to {logits[0].sum():.4f}:\\n{logits[0]}\")\n",
    "print(f\"This is the first row of counts that sum to {counts[0].sum():.4f}:\\n{counts[0]}\")\n",
    "print(f\"This is the first row of probs  that sum to {probs[0].sum():.4f}:\\n{probs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0354, 0.0677, 0.0155, 0.0086, 0.0243, 0.0414, 0.2486, 0.0302, 0.0305,\n",
      "        0.0437, 0.0531, 0.0072, 0.0154, 0.0061, 0.0721, 0.0152, 0.0369, 0.0089,\n",
      "        0.0133, 0.0051, 0.0043, 0.0775, 0.0157, 0.0518, 0.0203, 0.0334, 0.0180])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.04136824607849121\n",
      "log likelihood: -3.18524169921875\n",
      "negative log likelihood: 3.18524169921875\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0474, 0.0492, 0.0160, 0.0363, 0.2042, 0.0464, 0.0055, 0.0077, 0.0069,\n",
      "        0.0531, 0.0336, 0.0742, 0.0059, 0.0590, 0.0174, 0.0433, 0.0308, 0.0124,\n",
      "        0.0125, 0.0492, 0.0338, 0.0193, 0.0104, 0.0327, 0.0154, 0.0221, 0.0549])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.059031784534454346\n",
      "log likelihood: -2.829679250717163\n",
      "negative log likelihood: 2.829679250717163\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0388, 0.0215, 0.0071, 0.0153, 0.0058, 0.0034, 0.0430, 0.0563, 0.0210,\n",
      "        0.1001, 0.0234, 0.0385, 0.0120, 0.0182, 0.0100, 0.0406, 0.0180, 0.0130,\n",
      "        0.1705, 0.0381, 0.1016, 0.0092, 0.0855, 0.0101, 0.0421, 0.0381, 0.0187])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018185822293162346\n",
      "log likelihood: -4.007112979888916\n",
      "negative log likelihood: 4.007112979888916\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0388, 0.0215, 0.0071, 0.0153, 0.0058, 0.0034, 0.0430, 0.0563, 0.0210,\n",
      "        0.1001, 0.0234, 0.0385, 0.0120, 0.0182, 0.0100, 0.0406, 0.0180, 0.0130,\n",
      "        0.1705, 0.0381, 0.1016, 0.0092, 0.0855, 0.0101, 0.0421, 0.0381, 0.0187])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.02147568389773369\n",
      "log likelihood: -3.840833902359009\n",
      "negative log likelihood: 3.840833902359009\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0389, 0.0135, 0.0093, 0.0103, 0.0380, 0.0217, 0.1445, 0.1695, 0.0075,\n",
      "        0.0337, 0.1163, 0.0127, 0.0130, 0.0364, 0.0029, 0.0032, 0.0121, 0.0221,\n",
      "        0.0269, 0.0309, 0.1060, 0.0154, 0.0076, 0.0161, 0.0185, 0.0466, 0.0265])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.03891542926430702\n",
      "log likelihood: -3.2463643550872803\n",
      "negative log likelihood: 3.2463643550872803\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.421846389770508\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### Here we run a forward pass for one example at a time - a very inefficient way \n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {i_to_s[x]}{i_to_s[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the forward pass in vectorized form gives the loss 3.7693.\n",
      "Before in the 'one sample at a time' for loop we had a loss of 3.4218\n"
     ]
    }
   ],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)  # Remember to set the requires_grad=True \n",
    "\n",
    "# Forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float()                # input to the network: one-hot encoding\n",
    "logits = xenc @ W                                           # predict log-counts\n",
    "counts = logits.exp()                                       # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)               # probabilities for next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean()             # Compute the loss in a vectorized manner \n",
    "print(f\"Now the forward pass in vectorized form gives the loss {loss.item():.4f}.\\nBefore in the 'one sample at a time' for loop we had a loss of {nlls.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None               # set to zero the gradient\n",
    "loss.backward()             # Compute the gradients for all parameters \n",
    "W.data += -0.1 * W.grad     # Update the parameters of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset with all the words in the list \n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = s_to_i[ch1]\n",
    "    ix2 = s_to_i[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(f'number of examples: {num}')\n",
    "\n",
    "# initialize the 'network' (i.e. the single, hidden linear layer) \n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((len(s_to_i), len(s_to_i)), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the 1. iteration we gain a loss of 3.7686190605163574\n",
      "At the 25. iteration we gain a loss of 2.5668609142303467\n",
      "At the 50. iteration we gain a loss of 2.5157084465026855\n",
      "At the 75. iteration we gain a loss of 2.5010147094726562\n",
      "At the 100. iteration we gain a loss of 2.4945943355560303\n",
      "At the 125. iteration we gain a loss of 2.491189479827881\n",
      "At the 150. iteration we gain a loss of 2.489206552505493\n",
      "At the 175. iteration we gain a loss of 2.488002061843872\n",
      "At the 200. iteration we gain a loss of 2.4872772693634033\n",
      "At the 225. iteration we gain a loss of 2.4868834018707275\n",
      "At the 250. iteration we gain a loss of 2.4867498874664307\n"
     ]
    }
   ],
   "source": [
    "### Gradient descent approach to minimize the NLL loss \n",
    "# As we are still only using a bigram model with only a single layer, we will expect the loss to go down to a minima of \n",
    "num_iterations = 250\n",
    "for k in range(num_iterations):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=len(s_to_i)).float()                   # input to the network: one-hot encoding\n",
    "  logits = xenc @ W                                                       # predict log-counts\n",
    "  counts = logits.exp()                                                   # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True)                           # probabilities for next character (softmax)\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()  # NLL loss + regularization (weight decay) loss \n",
    "  if (k+1) % 25 == 0 or k==0:\n",
    "    print(f\"At the {k+1}. iteration we gain a loss of {loss.item()}\")\n",
    "  \n",
    "  # Backward pass\n",
    "  W.grad = None             # Set to zero the gradient, W.grad=None should be more efficient than W.grad = 0\n",
    "  loss.backward()           # Compute gradients for all the parameters of the model \n",
    "  \n",
    "  # Update the parameters of the model\n",
    "  W.data += -(1 - k/num_iterations) * 50 * W.grad    # In this simple example we can affort a super high learning rate of 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot_product_computation and indexed row are equal\n"
     ]
    }
   ],
   "source": [
    "# Because of the way the dot product works with onehot encoded vectors we can see that multiplying a \n",
    "# onehot encoded vector is just similar to indexing into a matrix with the row at the corresponding index \n",
    "index_num = 4\n",
    "dot_product_computation = W @ F.one_hot(torch.tensor(index_num), num_classes=len(s_to_i)).float()\n",
    "index_row = W[:,4]\n",
    "print(f\"The dot_product_computation and indexed row are {'NOT' if not torch.equal(dot_product_computation, index_row) else ''}equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bigram counts probability matrix P has a mean value of 0.03704\n",
      "The Neural Network probability matrix P_NN has a mean value of 0.03704\n",
      "The MSE between the two matrices is as small as 0.000152\n"
     ]
    }
   ],
   "source": [
    "### Compute the probabilities from the linear layer of our trained neural network\n",
    "# Notice that P and P_NN are almost identical \n",
    "P_NN = W.exp() / W.exp().sum(dim=1,keepdim=True)\n",
    "P_NN_mean = P_NN.mean()\n",
    "P_mean = P.mean()\n",
    "P_NN_and_P_diff = ((P-P_NN)**2).mean()\n",
    "print(f\"The bigram counts probability matrix P has a mean value of {P_mean:.5f}\")\n",
    "print(f\"The Neural Network probability matrix P_NN has a mean value of {P_NN_mean:.5f}\")\n",
    "print(f\"The MSE between the two matrices is as small as {P_NN_and_P_diff:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the probabilities\n",
    "plot_counts(N_matrix_list=[P, P_NN.detach()], i2s=i_to_s, figsize=(60,120), fontsize=30, colormap=\"jet\", draw=False, title_list=[\"Bigram counted probabilities\", \"Neural Network probabilities\"], tit_fontsize=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    # p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(i_to_s[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
