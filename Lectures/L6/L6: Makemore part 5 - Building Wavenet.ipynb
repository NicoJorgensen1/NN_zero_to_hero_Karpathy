{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## makemore: part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import math \n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt         # for making figures\n",
    "%matplotlib inline\n",
    "from typing import Union, Dict, Tuple, List, Any \n",
    "from IPython.display import clear_output\n",
    "from tqdm.autonotebook import tqdm\n",
    "import capra_standard_functions as csf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will import the list with all the words/names \n",
    "import os \n",
    "words_txt_file_path = os.path.join(os.path.expanduser(\"~\"), \"NN_zero_to_hero\", \"Lectures\", \"Makemore_repo\", \"names.txt\")\n",
    "words = open(words_txt_file_path, 'r').read().splitlines()\n",
    "\n",
    "print(f\"Now we have read the {len(words)} words from the file. Here we display the first 8 words\\n{words[:8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we are building the vocabulary and mapping from char to integer and vice versa \n",
    "chars = sorted(list(set(''.join(words))))           # Read all unique characters in all words \n",
    "s_to_i = {s:i+1 for i,s in enumerate(chars)}        # Create a mapping from char to index integer\n",
    "s_to_i['.'] = 0                                     # Assign the index of our special start/end token '.' to 0\n",
    "i_to_s = {i:s for s,i in s_to_i.items()}            # Reverse the mapping from integer to char \n",
    "num_classes = len(s_to_i)                           # Read the different number of characters available in our dataset \n",
    "vocab_size = num_classes                            # Vocabulary = number of different tokens available for the model \n",
    "\n",
    "print(f\"Now we have created the mappings between chars and integers and vice versa, which yields to {num_classes} different, possible characters (i.e. classes) in our dataset:\\n{s_to_i=}\\n{i_to_s=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build the dataset\n",
    "\n",
    "# Create the dataset arrays from the words list\n",
    "def build_dataset(words, s2i: Dict = s_to_i, block_size: int = 3, num_words: Union[int, None] = None, ):  \n",
    "  X, Y = [], []\n",
    "  for w in (words if num_words is None else words[:num_words] if isinstance(words[:num_words], list) else [words[:num_words]]): \n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = s2i[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix]                              # crop and append the next character to the context\n",
    "  return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "# Create datasets \n",
    "random.seed(42)                                                 # Choose a seed for deterministic shuffling \n",
    "random.shuffle(words)                                           # inplace shuffling of the words list \n",
    "n1 = int(0.8*len(words))                                        # Extract an integer for the 80% of the dataset \n",
    "n2 = int(0.9*len(words))                                        # Extract an integer for the 90% of the dataset \n",
    "\n",
    "block_size = 8                                                  # context length: how many characters do we take to predict the next one?\n",
    "Xtr,  Ytr  = build_dataset(words[:n1], block_size=block_size)   # Build the training split \n",
    "Xdev, Ydev = build_dataset(words[n1:n2], block_size=block_size) # Build the validation (dev) split \n",
    "Xte,  Yte  = build_dataset(words[n2:], block_size=block_size)   # Build the test split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 20 sequences \n",
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "  print(''.join(i_to_s[ix.item()] for ix in x), '-->', i_to_s[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Near copy paste of the layers we have developed in Part 3 - Batch Norm lecture \n",
    "# We want to use these differenct class instances in order to be able to stack them as different blocks on top of each other \n",
    "# This makes is very easy to simply instantiate new instances from each of these classes to create or extend a new network \n",
    "# PyTorch has similar layers (with similar names) for all these new layers that we are using here \n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init, not necessary due to BatchNorm \n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \"\"\"\n",
    "  Notice that the running mean and variance is \"trained\" as an exponential moving average during the training, i.e. not trained with backprop \n",
    "  Notice that the layer acts differently during eval and training, this makes BatchNorm vulnerable to bugs and issues \n",
    "  Notice that batch norm requires a training batch size of bs>=2, as we cannot calculate the variance of a single sample ... \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.05):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    \"\"\"\n",
    "    Notice how we need to edit the dimensions that we are measuring the statistics across based on the number of inputs \n",
    "    \"\"\"\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim==2: dim = 0\n",
    "      elif x.ndim==3: dim = (0,1)\n",
    "      else: raise NotImplementedError(f\"Only 2D or 3D inputs are accepted. Now the input is {tuple(x.ndim)}!!\") \n",
    "      xmean = x.mean(dim, keepdim=True)     # batch mean\n",
    "      xvar = x.var(dim, keepdim=True)       # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean \n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \"\"\"\n",
    "  We have an embedding table, which we want to index into using:        emb = C[Xb]\n",
    "  This we can do with this simple layer.\n",
    "  We simply say that we want to initiate a random embedding matrix of the specified dimension\n",
    "  The forward pass is then simply indexing into the embedding matrix \n",
    "  \"\"\"\n",
    "  ### Randomly initialize an embedding matrix from a normal distribution\n",
    "  # This is the embedding matrix that will hold vectors that are corresponding to our vocabulary \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "  \n",
    "  # The call method for this class is simply indexing into the embedding matrix \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \"\"\"\n",
    "  This is a Flatten layer, that will flatten consecutive inputs\n",
    "    Initially we would simply have a single Flatten layer with only the forward pass: self.out = x.view(x.shape[0], -1) and no parameters\n",
    "  Flatten consecutive can view the input into multiple dimensions\n",
    "    I.e. if we have an input array of (N, block_size, emb_dim) then a regular flatten would view this as a (N, block_size*emb_dim) --> i.e. our (32, 8, 10) would become (32, 80)\n",
    "    However, now we don't want to squeeze all the blocks into the first linear layer at the same time, we want to fuse only two consecutive blocks at a time\n",
    "  This means that as the input to FlattenConsecutive is still (32, 8, 10) then we now want the output to be (32, 4, 20)\n",
    "    This is compatible with our Linear layer, which is simply a powerful Python matrix multiplication:\n",
    "          (32, 80) @ (80, 200)    = (32, 200)\n",
    "          (32, 4, 20) @ (20, 200) = (32, 4, 200) \n",
    "      Hence, notice here, that Python is performing a broadcast as the multiplication will only happen along the last dimension of the first matrix and the first dimension of the last matrix \n",
    "  \"\"\"\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape                       # Read the batch_size, fusing size and channel size \n",
    "    x = x.view(B, T//self.n, C*self.n)      # View the input as a [N, fusing_size//consecutive_fusing, channels*consecutive_fusing] tensor\n",
    "    if x.shape[1] == 1: x = x.squeeze(1)    # Remove the fusing_dimension if that dimension is a spurious '1' (i.e. if x.shape == (32, 1, 80) we squeeze it into x.shape == (32, 80)) \n",
    "    self.out = x \n",
    "    return self.out\n",
    "  \n",
    "  # We have no parameters in the flattening layer \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \"\"\"\n",
    "  This is a Sequential class => this is what's called a PyTorch container --> https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \n",
    "  This container can simply contain all the layers added sequentially \n",
    "  This code is straight forward:\n",
    "    We pass in a list of layers\n",
    "    In the forward pass we then call all these layers sequentially one by one \n",
    "    The parameters are then the parameters for all the layers inside the \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # Get parameters of all layers and stretch them out into one list \n",
    "    return [p for layer in self.layers for p in layer.parameters()] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The output of a simple flattening: {tuple((torch.randn(32, 80) @ torch.randn(80, 200) + torch.randn(200)).shape)}\")\n",
    "print(f\"The output of a consecutive flattning: {tuple((torch.randn(32, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### However, when implementing FlatteningConsecutive, we need to look at the batch norm ...\n",
    "# Even though everything runs and the output x_hat has the wanted dimensions (due to broadcasting) the mean and the sigma does NOT have the wanted dimensions!\n",
    "# As now the mean and the variance is calculated only across the first batch dimension and not the fusing dimension as well\n",
    "# As we are using a channel size of 20, we only want 20 mean values, we don't want 4*20 mean values...\n",
    "emb = torch.randn(32, 4, 20)            # (32, 4, 20)\n",
    "# ------------------------------------------------------------------------------\n",
    "mu = emb.mean(0, keepdim=True)          # (1, 4, 20)\n",
    "sigma = emb.var(0, keepdim=True)        # (1, 4, 20)\n",
    "x_hat = (emb-mu)/sigma                  # (32, 4, 20)\n",
    "# ------------------------------------------------------------------------------\n",
    "mu = emb.mean((0,1), keepdim=True)      # (1, 1, 20)\n",
    "sigma = emb.var((0,1),keepdim=True)     # (1, 1, 20)\n",
    "x_hat = (emb-mu)/sigma                  # (32, 4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discarded the generator object, now we simply set the manual seed for the entire notebook here instead \n",
    "torch.manual_seed(42);                  # seed rng for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original network\n",
    "# n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "# n_hidden = 300 # the number of neurons in the hidden layer of the MLP\n",
    "# model = Sequential([\n",
    "#   Embedding(vocab_size, n_embd),\n",
    "#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(n_hidden, vocab_size),\n",
    "# ])\n",
    "\n",
    "### hierarchical network with the FlatteningConsecutive layers \n",
    "# Notice that we are not using a bias in the Linear layers due to the bias in the batch norm layers \n",
    "n_embd = 30           # the dimensionality of the character embedding vectors\n",
    "n_hidden = 350        # the number of neurons in the hidden layer of the MLP\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1              # Make the final layer of the model less confident \n",
    "\n",
    "parameters = model.parameters()               # Use model.parameters instead of \n",
    "print(f\"Now we have a model with {len(model.layers)} layers and {sum(p.nelement() for p in parameters):,} parameters\")\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "batch_size = 64\n",
    "num_epochs = 10 \n",
    "steps_pr_epoch = Xtr.shape[0] // batch_size \n",
    "lossi = []\n",
    "lr = 0.15 \n",
    "\n",
    "# Assure the model is ready for training \n",
    "for layer in model.layers:\n",
    "  layer.training = True\n",
    "\n",
    "# Create the indices for each training epoch \n",
    "indices_epoch = torch.randint(low=0, high=Xtr.shape[0], size=(num_epochs, Xtr.shape[0]))\n",
    "\n",
    "epochs_progress_bar = tqdm(range(num_epochs), leave=True, total=num_epochs, desc=\"Training the NN\")\n",
    "for i in epochs_progress_bar:\n",
    "  epochs_progress_bar.set_description_str(f\"Training epoch {i+1:d}/{num_epochs}\")\n",
    "  for j in range(steps_pr_epoch):\n",
    "    \n",
    "    # minibatch construct\n",
    "    # ix = torch.randint(0, Xtr.shape[0], (batch_size,))  # Using this, we cannot be sure that we are using all examples in the dataset equally \n",
    "    ix = indices_epoch[i, j*batch_size:(j+1)*batch_size]  # Using this we assure that all samples are seen only once for each epoch \n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]               # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    logits = model(Xb)                      # This is the logits, i.e. outputs of the entire model \n",
    "    loss = F.cross_entropy(logits, Yb)      # loss function, compute CCE loss from the logits \n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    loss.backward()\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    # update: simple SGD\n",
    "    for p in parameters:\n",
    "      p.data += -lr * p.grad\n",
    "\n",
    "  # track stats after every epoch \n",
    "  if i%2==0:\n",
    "    lr *= 0.1                               # Decrease the learning rate by a factor of 10 every second epoch \n",
    "  loss_avg = csf.moving_average(inp_array=[10**x for x in lossi[-steps_pr_epoch:]], n=25) if lossi else [loss.item()]\n",
    "  tqdm.write(f\"{i+1:d}/\".rjust(7) + f\"{num_epochs:d}:\".ljust(10) + f\"lr: {lr:.1e}\".ljust(13) + f\"Loss: {loss_avg[-1]:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.plot(torch.linspace(start=1, end=num_epochs, steps=len(lossi)), [10**x for x in lossi], color=\"blue\", alpha=0.5, label=\"Loss\")\n",
    "avg_run_mean_loss = csf.moving_average(inp_array=[10**x for x in lossi], n=50)\n",
    "plt.plot(torch.linspace(start=1, end=num_epochs, steps=len(avg_run_mean_loss)), avg_run_mean_loss, color=\"red\", label=\"Loss running mean\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(f\"{layer.__class__.__name__}:\".ljust(23) + f\"{tuple(layer.out.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "for layer in model.layers:\n",
    "  layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n",
    "### When the losses are very close to each other, we have a feeling that we are not overfitting the model, hence we might be able to increase capacity and still get a gain from that \n",
    "# Hence, we need to find the optimal way to increase the capacity of the network \n",
    "# At the moment it seems silly to squash the entire input sequence into the first layer with a massive receptive field -> we simply loose to much information in that way \n",
    "  # Hence, we want to implement a Wavenet like architecture with dilated convolutions \n",
    "  # At each layer we only want to merge two consecutive elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance log\n",
    "\n",
    "- original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n",
    "- context: 3 -> 8 (22K params): train 1.918, val 2.027\n",
    "- flat -> hierarchical (22K params): train 1.941, val 2.029\n",
    "- fix bug in batchnorm: train 1.912, val 2.022\n",
    "- scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.769, val 1.993\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      logits = model(torch.tensor([context]))\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(i_to_s[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next time:\n",
    "Why convolutions? Brief preview/hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(Xtr[7:15], Ytr[7:15]):\n",
    "  print(''.join(i_to_s[ix.item()] for ix in x), '-->', i_to_s[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward a single example:\n",
    "logits = model(Xtr[[7]])\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward all of them\n",
    "logits = torch.zeros(8, 27)\n",
    "for i in range(8):\n",
    "  logits[i] = model(Xtr[[7+i]])\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution is a \"for loop\"\n",
    "# allows us to forward Linear layers efficiently over space\n",
    "# The convolutions are implemented in CUDA vs the upper for loop running in Python ... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
