{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libaries \n",
        "import os \n",
        "import torch \n",
        "import tiktoken\n",
        "import urllib.request\n",
        "import capra_standard_functions as csf\n",
        "from matplotlib import pyplot as plt \n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "ccc60f0c-fd78-4dbe-8598-0512d1036aad"
      },
      "outputs": [],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "save_dir = \"/home/nico/NN_zero_to_hero/Lectures/L7\" \n",
        "full_path = os.path.join(save_dir, os.path.basename(url))\n",
        "if not os.path.isfile(full_path):\n",
        "    _=urllib.request.urlretrieve(url, full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In this Shakespeare dataset we have 1,115,394 characters. Let's look at the first 250 characters:\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the Shakespeare file \n",
        "with open(full_path, 'r', encoding='utf-8') as f:\n",
        "    shakespeare_text = f.read()                                         # Read the file as an entire string \n",
        "\n",
        "# txt = csf.read_lblfile(\"input.txt\")                       # Read the file as a list \n",
        "watch_num = 250\n",
        "print(f\"In this Shakespeare dataset we have {len(shakespeare_text):,} characters. Let's look at the first {watch_num:d} characters:\\n\\n{shakespeare_text[:watch_num]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the Shakespeare dataset we have 65 unique characters:\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(shakespeare_text)))         # Create a sorted list of all single characters that occur in the dataset \n",
        "vocab_size = len(chars)                             # The vocabulary size is the possible characters in our dataset, i.e. the number of classes \n",
        "\n",
        "# Decode to utf-8 in order to be specific about that we also have '\\n' and ' ' (i.e. lineskip and a space) as possible characters in the dataset \n",
        "print(f\"In the Shakespeare dataset we have {vocab_size} unique characters:\\n{''.join(chars)}\")  # Note the lineskip before printing the characters and that we start with a space of the new line "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
      },
      "outputs": [],
      "source": [
        "### Create a mapping from characters to integers\n",
        "# This is our tokenizer --> i.e. our translation from characters into indices, as a NN model cannot take characters directly \n",
        "\n",
        "### This is a VERY simply tokenizer --> one of many different tokenizers out there \n",
        "# Google uses 'Sentence-Piece' tokenizer => which is a sub-word unit-level tokenizer \n",
        "# OpenAI has a library called 'tiktoken' which uses a BPE (Byte-Pair Encoding), which is then used by all GPT models \n",
        "stoi = { ch:i for i,ch in enumerate(chars) }                # Dictionary mapping characters to integers \n",
        "itos = { i:ch for i,ch in enumerate(chars) }                # Dictionary mapping integers to characters\n",
        "encode = lambda s: [stoi[c] for c in s]                     # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])            # decoder: take a list of integers, output a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using our own, simple encoder <-> decoder with a vocabulary size of 65 and the test string: 'hii there', we get:\n",
            "Our simple tokenizer-encoder outputs:                       '[46, 47, 47, 1, 58, 46, 43, 56, 43]'\n",
            "Our simple tokenizer-decoding outputs:                      'hii there'\n",
            "\n",
            "The OpenAI encoder for GPT2 has a vocavulary size of 50,257\n",
            "Encoding our test string using OpenAI tokenizer gets:       [71, 4178, 612]\n",
            "Decoding that back using the OpenAI tokenizer gets:         hii there\n"
          ]
        }
      ],
      "source": [
        "### Encode and decode our test string using our own encoder and decoder tokenizer \n",
        "encode_decode_txt_test_str = \"hii there\"\n",
        "ljust_length = 60\n",
        "print(f\"Using our own, simple encoder <-> decoder with a vocabulary size of {vocab_size} and the test string: '{encode_decode_txt_test_str}', we get:\")\n",
        "print(f\"Our simple tokenizer-encoder outputs:\".ljust(ljust_length) +f\"'{encode(encode_decode_txt_test_str)}'\")\n",
        "print(f\"Our simple tokenizer-decoding outputs:\".ljust(ljust_length) + f\"'{decode(encode(encode_decode_txt_test_str))}'\")\n",
        "\n",
        "### Encode and decode our test string using the encoder used for GPT2 by OpenAI \n",
        "openai_encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "print(f\"\\nThe OpenAI encoder for GPT2 has a vocavulary size of {openai_encoder.n_vocab:,d}\")\n",
        "print(f\"Encoding our test string using OpenAI tokenizer gets:\".ljust(ljust_length) + f\"{openai_encoder.encode(encode_decode_txt_test_str)}\")\n",
        "print(f\"Decoding that back using the OpenAI tokenizer gets:\".ljust(ljust_length) + f\"{openai_encoder.decode(openai_encoder.encode(encode_decode_txt_test_str))}\")\n",
        "\n",
        "### Notice how the vocabulary size of the OpenAI tokenizer is much larger than our simple character level approach \n",
        "# This means that they can distinguish between 50k different tokens, whereas we can only distinguish between 65 different characters\n",
        "# This makes their task way heavier, as that means their final linear layer will have to output a logits vector of length 50k \n",
        "# This then means that a model used with such a vocabulary size will have way more parameters than our simple approach model\n",
        "# However, distinguishing between that many tokens will also allow a model to learn more representative combinations of tokens\n",
        "# Hence, the OpenAI tokenizer will allow the OpenAI models to become better than our simple character-level based models \n",
        "# Notice that in the bottom code block our simple character based tokenizer outputs a list of 9 integers for the two words 'hii there', where as the OpenAI tokenizer only outputs a list of three integers\n",
        "    # Hence, we can see that there is a tradeoff between having small sequence lengths but large vocabularies (as OpenAI) or longer sequence lengths but with smaller vocabularies (as with our simple approach)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now we have our data of torch.Size([1115394]) of type torch.int64\n",
            "This is the tokens input that our GPT model gets for the first 250 tokens:\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0])\n"
          ]
        }
      ],
      "source": [
        "### Let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "data = torch.tensor(encode(shakespeare_text), dtype=torch.long)                 # Here we encode the entire Shakespeare text dataset and convert these encodings into a torch.tensor \n",
        "print(f\"Now we have our data of {data.shape} of type {data.dtype}\")\n",
        "print(f\"This is the tokens input that our GPT model gets for the first {watch_num} tokens:\\n{data[:watch_num]}\")   # The first 'watch_num' characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "### Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data))          # First 90% will be train, rest validation \n",
        "train_data = data[:n]           # Read the first 90% of the dataset     \n",
        "val_data = data[n:]             # Read the final 10% of the dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hence, the string 'First Cit' translates into [18, 47, 56, 57, 58, 1, 15, 47, 58]\n",
            "\n",
            "When input is tensor([18]) the target is:                             47\n",
            "When input is tensor([18, 47]) the target is:                         56\n",
            "When input is tensor([18, 47, 56]) the target is:                     57\n",
            "When input is tensor([18, 47, 56, 57]) the target is:                 58\n",
            "When input is tensor([18, 47, 56, 57, 58]) the target is:             1\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]) the target is:         15\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is:     47\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
          ]
        }
      ],
      "source": [
        "### When we are training the Transformer based model, we will never send in the entire dataset\n",
        "# We will instead feed smaller chunks of data --> the mini-batches \n",
        "# In our earlier lectures we have denoted the size of each sequence as our \"block_size\"\n",
        "    # This is what's denoted as context_length etc. in other NLP transformer papers \n",
        "block_size = 8\n",
        "print(f\"Hence, the string '{decode(train_data[:block_size+1].tolist())}' translates into {train_data[:block_size+1].tolist()}\\n\")\n",
        "\n",
        "### Hence, we can create sequences of different lengths\n",
        "    # We feed in a sequences of as little as 1 token and then all the way up to a sequence of length 'block_size' \n",
        "    # This is done in order for the Transformer to learn how to handle sequences of varying sizes \n",
        "# When we are going to the inference stage with the Transformer model, the model will then know how to sample and continue on\n",
        "# from sequences as small as only one token input and all the way up to a sequence of length 'block_size' \n",
        "    # If the sequence gets longer than 'block_size', then that's where we start truncating the sequence and remove the first in favour of the last tokens \n",
        "x = train_data[:block_size]             # Creating a bunch of sequences from the training data \n",
        "y = train_data[1:block_size+1]          # Creating their corresponding targets from the training data, i.e. what character will follow the input sequence \n",
        "for t in range(block_size):             # Iterating through all these possible sequence lengths. From 0 all the way to 'block_size' \n",
        "    context = x[:t+1]                   # The context length of the sequence will then be the earlier 't' characters \n",
        "    target = y[t]                       # The target is then going to be the following character \n",
        "    print(f\"When input is {context} the target is:\".ljust(70) + f\"{target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs of shape torch.Size([4, 8]) => Strings: [\"Let's he\", 'for that', 'nt that ', 'MEO:\\nI p']:\n",
            "Integers: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "\n",
            "Targets of shape torch.Size([4, 8]) => Strings: [\"et's hea\", 'or that ', 't that h', 'EO:\\nI pa']:\n",
            "Integers: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "When the input is [24] the target is:                            43\n",
            "When the input is [24, 43] the target is:                        58\n",
            "When the input is [24, 43, 58] the target is:                    5\n",
            "When the input is [24, 43, 58, 5] the target is:                 57\n",
            "When the input is [24, 43, 58, 5, 57] the target is:             1\n",
            "When the input is [24, 43, 58, 5, 57, 1] the target is:          46\n",
            "When the input is [24, 43, 58, 5, 57, 1, 46] the target is:      43\n",
            "When the input is [24, 43, 58, 5, 57, 1, 46, 43] the target is:  39\n",
            "\n",
            "When the input is [44] the target is:                            53\n",
            "When the input is [44, 53] the target is:                        56\n",
            "When the input is [44, 53, 56] the target is:                    1\n",
            "When the input is [44, 53, 56, 1] the target is:                 58\n",
            "When the input is [44, 53, 56, 1, 58] the target is:             46\n",
            "When the input is [44, 53, 56, 1, 58, 46] the target is:         39\n",
            "When the input is [44, 53, 56, 1, 58, 46, 39] the target is:     58\n",
            "When the input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)         # Set a manual seed for deterministic reproduction \n",
        "batch_size = 4                  # how many independent sequences will we process in parallel?\n",
        "block_size = 8                  # what is the maximum context length for predictions?\n",
        "\n",
        "# Generate a small batch of data of inputs x and targets y\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data                         # Sample from either the train_data or val_data \n",
        "    ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,))    # Generate a random integer array of indices of size 'batch_size', sampled from integers between [0, len(data)-block_size]\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])                         # Stack the data along the first dimension in a new tensor \n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])                     # Stack the targets, as the sequences taken 1 step further in a new tensor\n",
        "    return x, y                                                                 # Return both the sequence batch and their corresponding targets \n",
        "\n",
        "# Create inputs \n",
        "xb, yb = get_batch('train')\n",
        "print(f\"Inputs of shape {xb.shape} => Strings: {[decode(xb[i,:].tolist()) for i in range(xb.shape[0])]}:\\nIntegers: {xb}\")\n",
        "print(f\"\\nTargets of shape {yb.shape} => Strings: {[decode(yb[i,:].tolist()) for i in range(yb.shape[0])]}:\\nIntegers: {yb}\")\n",
        "print('-'*100)\n",
        "\n",
        "### Print the different sequences we are capable of creating from this batch\n",
        "# We will only print the sequences for the first two rows (half the batch) to save output space ... \n",
        "# Hence, this \n",
        "for b in range(batch_size//2):      # batch dimension\n",
        "    for t in range(block_size):     # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"When the input is {context.tolist()} the target is:\".ljust(65) + f\"{target}\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "### This is the most simple model we can build for a language model\n",
        "# This is a bigram language model, where each token is directly predicted from the previous token\n",
        "# This is a very simple model, but it is a good starting point to understand the basic concepts of language modeling\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"\n",
        "        Here we are simply creating an embedding table we can use for embeddings for all our of possible characters \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        We are simply taking the indices and passing them into the Embedding table \n",
        "        Each token we is then only concerned about it-self, as it cannot see any context in this bigram modelling scenario \n",
        "        However, we are still in a relatively good position to predict the next token, just from knowing the current token,\n",
        "        as we know that some tokens are more likely to follow than others\n",
        "        E.g. if we look at the 'n' token, then we know that a 'g' token is more likely than a 'z' token, hence even though\n",
        "        the context is very limited, we still have some information to base our predictions on \n",
        "        We are then going to use the Negative Log Likelihood (implemented as cross entropy in torch) loss as that is the \n",
        "        standard loss function for language modeling \n",
        "        \n",
        "        In this example we have a multi-dimensional input, as we are using (B, T, C) arrays \n",
        "        For the cross entropy loss to work, we then have to view the logits and targets to be proper shaped \n",
        "        \"\"\"\n",
        "\n",
        "        # Idx and targets are both (B, T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx)                        # (B,T,C)  --> (Batch_size, Time, Channels), where time is the block_size (i.e. context length) of the model \n",
        "        loss = None                                                     # As a default, the loss is None in order to allow for logits computations without computing a loss in the generate function\n",
        "\n",
        "        if targets is not None:                                         # However, if the targets are provided (if we are training the model) ... \n",
        "            B, T, C = logits.shape                                      # ... extract the dimensions of the logits (which is the embedded inputs) i.e. (4, 8, 65)\n",
        "            logits = logits.view(B*T, C)                                # ... view the logits as a 2D array of (B*T, C), i.e. (32, 65), i.e. the two \"batch\" dimensions multiplied and the embedding dim \n",
        "            targets = targets.view(B*T)                                 # ... view the targets as a 1D array of (B*T) i.e. (65)\n",
        "            loss = F.cross_entropy(logits, targets)                     # ... compute the cross entropy loss, i.e. this will measure the \"quality\" of the logits compared to the targets \n",
        "        \n",
        "        return logits, loss                                             # Return the logits and the computed loss \n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Sample from the model using a generation function\n",
        "        idx is (B, T) array of indices in the current context\n",
        "            The idx is the current context of some characters in a batch, hence the size (B, T)\n",
        "        The job for this generate function is to generate new samples, i.e. expanding the context to (B, T+1), then (B, T+2) ... (B, T+max_new_tokens) \n",
        "            I.e. we are going to generate new characters from the model in the time dimension for the batch \n",
        "        However, notice here that this is a very simple bigram model, where we feed in the entire context history, \n",
        "        even though we for the bigram model will only use the last character in the context\n",
        "        However, we want to be able to keep this function somewhat fixed, when the model is extended, hence we just keep in feeding the entire list of context (=past characters) \n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):                                 # Iterating over the maximum number of tokens allowed \n",
        "            logits, loss = self(idx)                                    # Make a prediction with the model \n",
        "            logits = logits[:, -1, :]                                   # Focus only on the last time step of the logits --> the logits becomes (B, C), this is because we only focus on the final predicted character \n",
        "            probs = F.softmax(logits, dim=-1)                           # apply softmax to get probabilities --> probs are of shape (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)          # sample from the distribution --> idx_next is of shape (B, 1), because we only have a single new character prediction we predict \n",
        "            idx = torch.cat((idx, idx_next), dim=1)                     # append sampled index to the running sequence --> idx is of shape (B, T+1), as we appended the new character to the sequence \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We can see that we are expecting an initial loss of about 4.1744 from an estimated initial probability of 0.0154\n",
            "Now we have logits of shape (32, 65) and the loss of 4.8786\n",
            "This means that the average probability for the predicted first character is 0.00761\n"
          ]
        }
      ],
      "source": [
        "### Initiate the simple bigram language model and try a simple forward pass \n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "### Compute the expected initial loss \n",
        "# We have a vocabulary of 65. \n",
        "# Initially we expect each of the characters to be equally likely, as the model is simply randomly initialized \n",
        "# Hence, we can compute the negative log likelihood loss of a uniform distribution \n",
        "    # The NLL loss: log(-1 * p)\n",
        "    # The probability for each of the characters are assumed to be 1/vocab_size \n",
        "initial_expected_probability = 1/65\n",
        "expected_initial_loss = -torch.log(torch.as_tensor(initial_expected_probability))\n",
        "actual_prob_for_example_character = torch.exp(-loss)\n",
        "print(f\"We can see that we are expecting an initial loss of about {expected_initial_loss:.4f} from an estimated initial probability of {initial_expected_probability:.4f}\")\n",
        "print(f\"Now we have logits of shape {tuple(logits.shape)} and the loss of {loss.item():.4f}\") \n",
        "print(f\"This means that the average probability for the predicted first character is {actual_prob_for_example_character:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ],
      "source": [
        "### Sample from the model \n",
        "initial_idx = torch.zeros((1,1), dtype=torch.long)                                  # As decode([0])=\\n, this seems like a reasonably, fair starting token \n",
        "max_tokens = 100                                                                    # The number of new tokens/characters we want to generate from the model \n",
        "print(decode(m.generate(idx = initial_idx, max_new_tokens=max_tokens)[0].tolist())) # Print the 100 generated characters from the untrained model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "### Create a PyTorch optimizer\n",
        "# The Adam optimizer is way more advanced than SGD, as we have been using in earlier lectures\n",
        "# This is because ADAM uses momentum for the first and second order statistics of the gradients\n",
        "# This will allow for \"varying learning rates\" for each of the parameters, as the momentum will take \n",
        "# care of \"scaling\" the learning rate appropriately \n",
        "### A typical good starting learning rate would be around 1e-4 or perhaps 1e-3, but for such a small\n",
        "# network as what we have for the bigram model here, 1e-3 is probably a bit too low, and we could \n",
        "# probably afford starting with a higher learning rate \n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32                             # Initial batch size \n",
        "num_iterations = 10000                      # The number of iterations to train for\n",
        "loss_list = list()                          # Create a list to store the loss for each iteration "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After 10000 iterations, loss = 2.47019\n"
          ]
        }
      ],
      "source": [
        "### Define a small training loop, where we train the bigram model \n",
        "for steps in range(num_iterations): \n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step() \n",
        "    loss_list.append(loss.item())\n",
        "print(f\"After {len(loss_list)} iterations, loss = {csf.moving_average(loss_list,150)[-1]:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = plt.figure(figsize=(10,3))\n",
        "plt.plot(torch.linspace(start=1,end=len(loss_list), steps=len(loss_list)), loss_list, color=\"blue\", alpha=0.5, label=\"Loss\")\n",
        "plt.plot(torch.linspace(start=1,end=len(loss_list), steps=len(loss_list)), csf.moving_average(inp_array=loss_list, n=150), color=\"red\", label=\"Mov_avg loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.title(\"Training loss\")\n",
        "plt.draw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo \n"
          ]
        }
      ],
      "source": [
        "### Sample from the now trained bigram model \n",
        "initial_idx = torch.zeros((1,1), dtype=torch.long)                                  # As decode([0])=\\n, this seems like a reasonably, fair starting token \n",
        "max_tokens = 200                                                                    # The number of new tokens/characters we want to generate from the model \n",
        "print(decode(m.generate(idx = initial_idx, max_new_tokens=max_tokens)[0].tolist())) # Print the 100 generated characters from the untrained model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### v1) Running average of features - for loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\"\"\"\n",
        "they are currently not talking to each other, but we want to couple them in a very specific way\n",
        "e.g. the tokens in e.g. position 5 should not be talking to tokens in position 6 or 7, as they are future tokens\n",
        "but tokens in position 5 should only be talking to earlier tokens, in position 1,2,3,4 ... \n",
        "    Information is only allowed to flow from earlier context to the current time-steps \n",
        "So, what is the easiest way for tokens to communicate? \n",
        "    - The simplest weight to communicate with the previous tokens is simply to average the earlier steps.\n",
        "        - i.e. for the 5th token, the easiest way to communicate with the previous context would be to take the mean\n",
        "        of the four earlier embedded tokens, hence that could be some kind of feature vector to use \n",
        "        - However, the average feature vector is a noisy context -> we can't tell which are the important ones,\n",
        "        and we lost a lot of information by averaging all the earlier tokens \n",
        "    - For every single batch element (i.e. for every t'th element) we want to compute the feature vectors for earlier tokens\n",
        "        - This is what we are calling the bag-of-words features for the current token\n",
        "\"\"\"\n",
        "\n",
        "B,T,C = 4,8,2                       # batch, time, channels => \n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "\"\"\"\n",
        "- Here we will compute the bag of words\n",
        "    - We run through each sample in the batch ...\n",
        "        - For each sample:\n",
        "            - we extract the previous tokens => into a tensor of shape (t, C), where t is the point\n",
        "                        in time we have reached and C is the number of channels (embedding dimension) \n",
        "            - we take the mean of those previous tokens along the time dimension and\n",
        "                        add that to our bag-of-words feature array => this is an array of shape (C)\n",
        "\"\"\"\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] \n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first sample of x:\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "\n",
            "The first sample of xbow:\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ],
      "source": [
        "### Print the x array and the x_bow tensor array in order to inspect them and visualize what we just did\n",
        "print(f\"The first sample of x:\\n{x[0]}\\n\\nThe first sample of xbow:\\n{xbow[0]}\")\n",
        "\n",
        "\"\"\"\n",
        "Notice that the first rows of the two tensors are equal [0.1808, -0.0700].\n",
        "    Because at the first row (i.e. first time step) we have only one context token to compute the mean over \n",
        "From the second row and on the two tensors differ\n",
        "    As now the xbow becomes a rolling mean of the previous inputs of x \n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### v2) Running average of features - matrix multiplication with summing normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a:\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "-----------------------\n",
            "b:\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "-----------------------\n",
            "c:\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "### toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "# This is a toy example to use to get used to what self-attention is and how it works \n",
        "\"\"\"\n",
        "Notice here that we create the three tensor matrices a, b, c\n",
        "    We run through three examples:\n",
        "        1) Triangular = False, normalization = False \n",
        "        2) Triangular = True, normalization = False\n",
        "        3) Triangular = True, normalization = True \n",
        "    In each example we create the a (which may or may not be edited) and b matrices.\n",
        "    Then we form c by taking the dot product between a and b.\n",
        "        The values of c is then what's interesting to us here \n",
        "    *** Remember the dot product m*n is computed by multiplying each element in the columns of the first matrix by each of the elements in the rows of the second matrix *** \n",
        "Example 1)\n",
        "    - If a is all ones => the resulting c matrix will be equal in each row:\n",
        "        - torch.equal(c[0,:], c[1,:]) and torch.equal(c[0,:], c[2,:]) \n",
        "        >>> True \n",
        "Example 2)\n",
        "    - If a is a lower triangular matrix of all ones => the resulting c matrix will be the cumulative sum of the b matrix \n",
        "        - i.e. the first row of c will be identical to the first row of b, as the cumulative sum of a single row is just that row\n",
        "        - the second row of c is then equal to the first and second row of b summed together \n",
        "        - the third row of c is then equal to all the rows of example 1), as it is the sum of all three rows of b\n",
        "Example 3)\n",
        "    - If a is a lower triangular matrix of all ones, and then it gets normalized to make the sum of each row '1', the resulting c matrix will be the moving average of b\n",
        "        - i.e. the first row of c will be identical to the first row of b, as the moving average of a single row is just that row\n",
        "        - the second row of c is then equal to the average of the first and second row of b \n",
        "        - the third row of c is then equal to the average of the first three rows of b \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "triangular = True \n",
        "normalization = True \n",
        "\n",
        "\n",
        "### Running our experiments \n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3)) if triangular else torch.ones(3, 3) \n",
        "a = a / torch.sum(a, 1, keepdim=True) if normalization else a \n",
        "b = torch.randint(0,10,(3,2)).float() \n",
        "c = a @ b \n",
        "print(f\"a:\\n{a}\\n{'-'*23}\\nb:\\n{b}\\n{'-'*23}\\nc:\\n{c}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The xbow and xbow2 are equal\n"
          ]
        }
      ],
      "source": [
        "### version 2: using matrix multiply for a weighted aggregation\n",
        "# Hence, now we are doing weighted sums, as with the for loop before,\n",
        "# we are just doing it using matrix multiplication, which is way more efficient \n",
        "wei = torch.tril(torch.ones(T, T))              # torch.tril will give us the lower triangular part of the torch.ones() tensor (earlier denoted a)\n",
        "wei = wei / wei.sum(1, keepdim=True)            # Normalize the weight matrix (earlier denoted b)\n",
        "xbow2 = wei @ x                                 # (B, T, T) @ (B, T, C) ----> (B, T, C), because, broadcasting happens along the batch dimension (earlier denoted c)\n",
        "\n",
        "print(f\"The xbow and xbow2 are {'' if torch.allclose(xbow, xbow2, rtol=1e-15, atol=1e-6) else 'NOT '}equal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### v3) Running average of features - matrix multiplication with softmax normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The xbow and xbow3 are equal\n"
          ]
        }
      ],
      "source": [
        "### version 3: use Softmax\n",
        "# Now we are going to use the weighted sum of features as in version 2\n",
        "# However, now we are going to use a softmax along the rows in order to assure that \n",
        "# the sum of each row will be 1\n",
        "# Hence, using the softmax is equal to what was done before in version 2\n",
        "# as the weight matrix used here for example 3 will be equal to what was done in version 2\n",
        "\n",
        "# The weight matrix here will tell us how much each of the tokens from the past will contribute to the value of the new prediction\n",
        "# Setting some values to -inf before the softmax will make sure that the current tokens will not be able to communicate with future tokens\n",
        "# This trick with taking some average of the earlier inputs will then allow for tokens to interact with earlier inputs\n",
        "# Later (the self-attention trick) will allow us to make weighted averages of the earlier tokens\n",
        "    # Hence, with a weigted average, we can interprete it as the tokens will tell which earlier tokens are important, and which are not\n",
        "    # Thus, the important earlier tokens will be attended to (i.e. getting a high value) where as the non-important tokens will get a low value \n",
        "# Remember, in a Transformer, we will have different heads to compute attention, similar to having different kernels in each layer in a CNN\n",
        "    # These different heads will attend to different parts of the input, i.e. find different input parts interesting \n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))                                                                                                         # Create a lower triangular matrix of shape (T, T)\n",
        "wei = torch.zeros((T,T))                                                                                                                    # Initiate the weight matrix as all zeros of the same shape \n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))                                                                                             # Read all the indices where the lower triangular matrix is 0 and fill those with -inf values in the weights matrix\n",
        "wei = F.softmax(wei, dim=-1)                                                                                                                # Compute a softmax for each row. The filled -inf values turns to 0 in the softmax (as exp(-inf)=0). Hence this is equal to what was done before in example 2.\n",
        "xbow3 = wei @ x                                                                                                                             # Compute the normalized bag of features for the x input using the softmax-normalized weight matrix \n",
        "\n",
        "print(f\"The xbow and xbow3 are {'' if torch.allclose(xbow, xbow3, rtol=1e-15, atol=1e-6) else 'NOT '}equal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### v4) BoW features - matrix multiplication turns to self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the weight matrix of shape (4, 8, 8) with the first parameters\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n",
            "This is the output of shape (4, 8, 16) of the attention head with the selected parameters:\n",
            "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
            "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
            "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
            "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
            "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
            "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
            "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
            "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
            "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
            "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
            "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
            "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
            "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
            "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
            "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
            "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "### version 4: self-attention!\n",
        "# This is self-attention for a single attention-head!\n",
        "\"\"\"\n",
        "Every single token will emit two vectors => a query (what am I looking for) and a key (what do I contain) \n",
        "    We will then find similarities by taking the dot product between the query and the key \n",
        "Hence, to begin with, we have to create the key and the value\n",
        "    - This is done by multiplying the input with a separate layer, one for the key, one for the query \n",
        "    - Our input is of shape (4, 8, 32). The layers are of shape (16, 32). Hence --> out = x @ w.T \n",
        "Now, the communication starts --> now we can start to find affinities (=likings) between keys and queries\n",
        "    - We will take the dot product between the keys and the queries \n",
        "        However, notice the shapes of the keys (4,8,16) and the queries (4,8,16)\n",
        "        As the first one is the batch dimension, which we don't want to multiply over, we need to transpose the keys ...\n",
        "        Hence, we transpose the keys from (4,8,16) into (4,16,8)\n",
        "        Then we can compute the dot product between the queries and the keys \n",
        "    - Hence, we can interprete this as the weight matrix and then do as before out = wei @ x\n",
        "        This will create an output of both positive and negative values \n",
        "            This we can interprete as the affinities between all the tokens (nodes)\n",
        "            However, we don't like negative affinities, hence we normalize and use the softmax to get numbers between 0 and 1, that sum to 1 \n",
        "    - However, we don't want any node/token to be able to communicate with future tokens (tokens with higher timesteps)\n",
        "        Hence, if we use the masking future information/tokens are washed out\n",
        "Now we have the attention weights for the inputs\n",
        "    - Though, we are not quite done, as we still need the final layer of the attention head --> the values \n",
        "        The values (v tensor) are what is returned to the inputs, if the queries and keys are matched \n",
        "Attention is a communication mechanism - we have a number of connections between each of the nodes corresponding to the tokens \n",
        "    - In this scenario we have a block size of 8, which means we always have 8 nodes\n",
        "        The first node will only point to itself\n",
        "        The second node points to itself and the first node \n",
        "        The third node points to itself, the second and the first node \n",
        "        ....\n",
        "        The eighth node points to all the other nodes in our context \n",
        "There is NO communication across the batch dimension\n",
        "    - No two samples in the batch are talking to each other, there are only communication across the tokens in the input sequence in each batch \n",
        "At the moment, as we have implemented it now, the tokens cannot talk to future tokens \n",
        "    - In a Transformer decoder block, where we might want to perform text generation by always predicting the next token, we will have to mask future tokens in our training sequence \n",
        "    - However, it doesn't necessarily has to be like that.\n",
        "        For encoder based applications we will remove the masking ---> remove: wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "            If we are doing sentiment analysis, we may want to make all tokens in the sentence able to talk to each other across the time dimension\n",
        "            This we would do in order for the model to come up with a single sentiment score for the entire sequence \n",
        "    - However, we can remove the masking, and then everything will just run and work fine\n",
        "        The attention mechanishm itself doens't care whether or not it has been masked ... \n",
        "Notice here that we have no notion of positional space ---> hence, by default the tokens/nodes have no notion of where they are located in space\n",
        "    This is different from e.g. convolution, where the very definition of a filter sliding across an image will incorporate the positional knowledge\n",
        "    - Hence, in order to gain positional knowledge we have to make positional embeddings as well\n",
        "        We will usually simply make these positional embeddings learnable as well, like the token embeddings\n",
        "            token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "            position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "            x = token_embedding_table + position_embedding_table\n",
        "        Hence, the embeddings for the tokens and the positions will start of by being random, but will be learnable through training \n",
        "Multiple kinds of attention:\n",
        "    - Self-Attention:\n",
        "        Self attention simply means that the keys, queries and values arrive from the same source\n",
        "        Hence, the inputs are attending to itself \n",
        "    - Cross-Attention:\n",
        "        In encoder-decoder transformers we can have a case where the queries are from the inputs, where the keys and values are from a separate source\n",
        "        Cross-attention happens when we have multiple sources where we want to incorporate some information from into our system\n",
        "            These other sources might be from other encoder blocks, where their outputs arrive into a decoder input \n",
        "Scaled dot product attention:\n",
        "    - If we look at the Attention Is All You Need paper we can see that the attention equation is scaled by the sqrt(dk)\n",
        "        This is for a similar reason as why weight initialization for neural networks are important\n",
        "    - If we don't scale the attention weights we can get a super high variance in our \"wei\" matrix \n",
        "        As the keys and queries can be unit variance, the variance of their dot product can be pretty high\n",
        "    - The problem is that if the wei matrix takes on too high numbers, the softmax will converge to a onehot encoding \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "use_triangular_matrix = True                    # If True we use a decoder (masked future), if False we use an encoder \n",
        "use_softmax = True                              # If True, normalize the weight matrix along the rows \n",
        "use_value_layer = True                          # If true, use a Value layer to compute the outputs \n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32                                  # batch, time, channels\n",
        "x = torch.randn(B,T,C)                          # Create an input of random numbers \n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16                                  # The number of elements in the key, query, value tokens \n",
        "key = nn.Linear(C, head_size, bias=False)       # key: \"what do the token contain\" \n",
        "query = nn.Linear(C, head_size, bias=False)     # query: \"what do the token want\"\n",
        "value = nn.Linear(C, head_size, bias=False)     # value: \"what do the token receive\"\n",
        "k = key(x)                                      # (B, T, head_size)   --->   nn.Linear()(x) ==> x @ w --->  multiplying the input of...\n",
        "q = query(x)                                    # (B, T, head_size)   --->   ... shape (4,8,32) with the linear layer of shape (16, 30) ...\n",
        "v = value(x)                                    # (B, T, head_size)   --->   ... which gets transposed to (32, 16), yields a (4,8,16) \n",
        "k_transposed = k.transpose(-2, -1)              # Transpose from shape (4,8,16) --> (4,16,8).                                                               Remember that torch.transpose(source_dim, target_dim), hence we take the -2dim and swap with the -1 dim \n",
        "wei =  q @ k_transposed                         # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "### Now comes the trick, weighted averages of the inputs \n",
        "if use_triangular_matrix: \n",
        "    wei = wei.masked_fill(torch.tril(torch.ones(T, T))  == 0, float('-inf'))\n",
        "if use_softmax:\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "### Compute the output from either the orig input or the values \n",
        "out = wei @ (v if use_value_layer else x)\n",
        "\n",
        "print(f\"This is the weight matrix of shape {tuple(wei.shape)} with the first parameters\\n{wei[0]}\")\n",
        "print(f\"\\nThis is the output of shape {tuple(out.shape)} of the attention head with the selected parameters:\\n{out[0,]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Scaling the weight matrix with sqrt(dk) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without scaling the variance of the weights matrix is 17.801.\n",
            "With scaling the variance of the weights matrix is 1.113.\n"
          ]
        }
      ],
      "source": [
        "### The reason why we need to scale the weights matrix before the softmax \n",
        "torch.manual_seed(4)\n",
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2,-1)\n",
        "wei_scaled = wei * head_size**-0.5\n",
        "print(f\"Without scaling the variance of the weights matrix is {wei.var():.3f}.\")\n",
        "print(f\"With scaling the variance of the weights matrix is {wei_scaled.var():.3f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Scaled dot product attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the output of the torch.softmax(test_tensor):    tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "This is the output of the torch.softmax(test_tensor)*8:  tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ],
      "source": [
        "### The softmax will converge towards a one hot encoding \n",
        "# Especially at initialization we don't want the softmax outputs to be too peaky/sharp, as that would mean \n",
        "# that we would have only one node contributing to the attention mechanism, as a onehot encoded output\n",
        "# from the softmax would be equal to plugging out a single node of the weight matrix \n",
        "\n",
        "test_tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
        "test_scale = 8 \n",
        "print(f\"This is the output of the torch.softmax(test_tensor):\".ljust(57) + f\"{torch.softmax(test_tensor, dim=-1)}\")\n",
        "print(f\"This is the output of the torch.softmax(test_tensor)*{test_scale}:\".ljust(57) + f\"{torch.softmax(test_tensor*test_scale, dim=-1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing the Language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(shakespeare_text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(shakespeare_text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))            # Not a weight parameter, hence in PyTorch we need to \"register_buffer\" \n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
