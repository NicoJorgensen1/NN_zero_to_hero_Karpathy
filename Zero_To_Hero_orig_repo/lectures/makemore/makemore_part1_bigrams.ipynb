{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will simply import the list with all the words \n",
    "import os \n",
    "words_txt_file_path = os.path.join(os.path.expanduser(\"~\"), \"NN_zero_to_hero\", \"Lectures\", \"Makemore_repo\", \"names.txt\")\n",
    "words = open(words_txt_file_path, 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "for w in words:\n",
    "  chs = ['<S>'] + list(w) + ['<E>']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    bigram = (ch1, ch2)\n",
    "    b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    N[ix1, ix2] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "p = torch.rand(3, generator=g)\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(p, num_samples=100, replacement=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27, 27\n",
    "# 27,  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27, 27\n",
    "#  1, 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    p = P[ix]\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: maximize likelihood of the data w.r.t. model parameters (statistical modeling)\n",
    "# equivalent to maximizing the log likelihood (because log is monotonic)\n",
    "# equivalent to minimizing the negative log likelihood\n",
    "# equivalent to minimizing the average negative log likelihood\n",
    "\n",
    "# log(a*b*c) = log(a) + log(b) + log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is index: 0 with row: tensor([3.1192e-05, 1.3759e-01, 4.0767e-02, 4.8129e-02, 5.2745e-02, 4.7785e-02,\n",
      "        1.3038e-02, 2.0898e-02, 2.7293e-02, 1.8465e-02, 7.5577e-02, 9.2452e-02,\n",
      "        4.9064e-02, 7.9195e-02, 3.5777e-02, 1.2321e-02, 1.6095e-02, 2.9008e-03,\n",
      "        5.1154e-02, 6.4130e-02, 4.0830e-02, 2.4641e-03, 1.1759e-02, 9.6070e-03,\n",
      "        4.2109e-03, 1.6719e-02, 2.9008e-02])\n",
      "\n",
      "This is index: 1 with row: tensor([0.1958, 0.0164, 0.0160, 0.0139, 0.0308, 0.0204, 0.0040, 0.0050, 0.0688,\n",
      "        0.0487, 0.0052, 0.0168, 0.0746, 0.0482, 0.1604, 0.0019, 0.0024, 0.0018,\n",
      "        0.0963, 0.0330, 0.0203, 0.0113, 0.0246, 0.0048, 0.0054, 0.0605, 0.0129])\n",
      "\n",
      "This is index: 2 with row: tensor([0.0430, 0.1205, 0.0146, 0.0007, 0.0247, 0.2455, 0.0004, 0.0004, 0.0157,\n",
      "        0.0816, 0.0007, 0.0004, 0.0389, 0.0004, 0.0019, 0.0397, 0.0004, 0.0004,\n",
      "        0.3155, 0.0034, 0.0011, 0.0172, 0.0004, 0.0004, 0.0004, 0.0314, 0.0004])\n",
      "\n",
      "This is index: 3 with row: tensor([0.0275, 0.2293, 0.0003, 0.0121, 0.0006, 0.1551, 0.0003, 0.0008, 0.1869,\n",
      "        0.0764, 0.0011, 0.0891, 0.0329, 0.0003, 0.0003, 0.1071, 0.0006, 0.0034,\n",
      "        0.0216, 0.0017, 0.0101, 0.0101, 0.0003, 0.0003, 0.0011, 0.0295, 0.0014])\n",
      "\n",
      "This is index: 4 with row: tensor([9.3609e-02, 2.3610e-01, 3.6212e-04, 7.2424e-04, 2.7159e-02, 2.3248e-01,\n",
      "        1.0864e-03, 4.7076e-03, 2.1546e-02, 1.2222e-01, 1.8106e-03, 7.2424e-04,\n",
      "        1.1045e-02, 5.6129e-03, 5.7940e-03, 6.8622e-02, 1.8106e-04, 3.6212e-04,\n",
      "        7.6951e-02, 5.4318e-03, 9.0531e-04, 1.6839e-02, 3.2591e-03, 4.3455e-03,\n",
      "        1.8106e-04, 5.7577e-02, 3.6212e-04])\n",
      "\n",
      "This is index: 5 with row: tensor([0.1948, 0.0333, 0.0060, 0.0075, 0.0188, 0.0622, 0.0041, 0.0062, 0.0075,\n",
      "        0.0400, 0.0027, 0.0088, 0.1589, 0.0377, 0.1309, 0.0132, 0.0041, 0.0007,\n",
      "        0.0958, 0.0422, 0.0284, 0.0034, 0.0227, 0.0025, 0.0065, 0.0524, 0.0089])\n",
      "\n",
      "This is index: 6 with row: tensor([0.0869, 0.2607, 0.0011, 0.0011, 0.0011, 0.1330, 0.0483, 0.0021, 0.0021,\n",
      "        0.1727, 0.0011, 0.0032, 0.0225, 0.0011, 0.0054, 0.0655, 0.0011, 0.0011,\n",
      "        0.1234, 0.0075, 0.0204, 0.0118, 0.0011, 0.0054, 0.0011, 0.0161, 0.0032])\n",
      "\n",
      "This is index: 7 with row: tensor([0.0558, 0.1694, 0.0020, 0.0005, 0.0102, 0.1714, 0.0010, 0.0133, 0.1847,\n",
      "        0.0977, 0.0020, 0.0005, 0.0169, 0.0036, 0.0143, 0.0430, 0.0005, 0.0005,\n",
      "        0.1034, 0.0159, 0.0164, 0.0440, 0.0010, 0.0138, 0.0005, 0.0164, 0.0010])\n",
      "\n",
      "This is index: 8 with row: tensor([3.1532e-01, 2.9373e-01, 1.1775e-03, 3.9252e-04, 3.2710e-03, 8.8316e-02,\n",
      "        3.9252e-04, 3.9252e-04, 2.6168e-04, 9.5512e-02, 1.3084e-03, 3.9252e-03,\n",
      "        2.4336e-02, 1.5439e-02, 1.8187e-02, 3.7682e-02, 2.6168e-04, 2.6168e-04,\n",
      "        2.6822e-02, 4.1868e-03, 9.4204e-03, 2.1850e-02, 5.2335e-03, 1.4392e-03,\n",
      "        1.3084e-04, 2.7999e-02, 2.7476e-03])\n",
      "\n",
      "This is index: 9 with row: tensor([0.1405, 0.1380, 0.0063, 0.0288, 0.0249, 0.0933, 0.0058, 0.0242, 0.0054,\n",
      "        0.0047, 0.0043, 0.0252, 0.0759, 0.0241, 0.1200, 0.0332, 0.0030, 0.0030,\n",
      "        0.0479, 0.0743, 0.0306, 0.0062, 0.0152, 0.0005, 0.0051, 0.0440, 0.0157])\n",
      "\n",
      "This is index: 10 with row: tensor([2.4599e-02, 5.0359e-01, 6.8329e-04, 1.7082e-03, 1.7082e-03, 1.5067e-01,\n",
      "        3.4165e-04, 3.4165e-04, 1.5716e-02, 4.0998e-02, 1.0249e-03, 1.0249e-03,\n",
      "        3.4165e-03, 2.0499e-03, 1.0249e-03, 1.6399e-01, 6.8329e-04, 3.4165e-04,\n",
      "        4.0998e-03, 2.7332e-03, 1.0249e-03, 6.9354e-02, 2.0499e-03, 2.3915e-03,\n",
      "        3.4165e-04, 3.7581e-03, 3.4165e-04])\n",
      "\n",
      "This is index: 11 with row: tensor([7.1837e-02, 3.4182e-01, 5.9207e-04, 5.9207e-04, 5.9207e-04, 1.7683e-01,\n",
      "        3.9471e-04, 1.9736e-04, 6.0785e-02, 1.0065e-01, 5.9207e-04, 4.1445e-03,\n",
      "        2.7630e-02, 1.9736e-03, 5.3286e-03, 6.8088e-02, 1.9736e-04, 1.9736e-04,\n",
      "        2.1709e-02, 1.8946e-02, 3.5524e-03, 1.0065e-02, 5.9207e-04, 6.9074e-03,\n",
      "        1.9736e-04, 7.4995e-02, 5.9207e-04])\n",
      "\n",
      "This is index: 12 with row: tensor([9.4029e-02, 1.8763e-01, 3.7898e-03, 1.8591e-03, 9.9392e-03, 2.0894e-01,\n",
      "        1.6446e-03, 5.0054e-04, 1.4301e-03, 1.7740e-01, 5.0054e-04, 1.7876e-03,\n",
      "        9.6246e-02, 4.3618e-03, 1.0726e-03, 4.9553e-02, 1.1441e-03, 2.8602e-04,\n",
      "        1.3586e-03, 6.7930e-03, 5.5774e-03, 2.3239e-02, 5.2199e-03, 1.2156e-03,\n",
      "        7.1505e-05, 1.1362e-01, 7.8656e-04])\n",
      "\n",
      "This is index: 13 with row: tensor([7.7523e-02, 3.8851e-01, 1.6944e-02, 7.7973e-03, 3.7487e-03, 1.2281e-01,\n",
      "        2.9990e-04, 1.4995e-04, 8.9969e-04, 1.8848e-01, 1.1996e-03, 2.9990e-04,\n",
      "        8.9969e-04, 2.5341e-02, 3.1489e-03, 6.7926e-02, 5.8480e-03, 1.4995e-04,\n",
      "        1.4695e-02, 5.3981e-03, 7.4974e-04, 2.0993e-02, 5.9979e-04, 4.4984e-04,\n",
      "        1.4995e-04, 4.3185e-02, 1.7994e-03])\n",
      "\n",
      "This is index: 14 with row: tensor([3.6853e-01, 1.6225e-01, 4.9036e-04, 1.1660e-02, 3.8411e-02, 7.4098e-02,\n",
      "        6.5381e-04, 1.4929e-02, 1.4711e-03, 9.4039e-02, 2.4518e-03, 3.2146e-03,\n",
      "        1.0679e-02, 1.0897e-03, 1.0390e-01, 2.7079e-02, 3.2690e-04, 1.6345e-04,\n",
      "        2.4518e-03, 1.5201e-02, 2.4191e-02, 5.2850e-03, 3.0511e-03, 6.5381e-04,\n",
      "        3.8139e-04, 2.5390e-02, 7.9547e-03])\n",
      "\n",
      "This is index: 15 with row: tensor([0.1075, 0.0188, 0.0177, 0.0144, 0.0240, 0.0167, 0.0044, 0.0057, 0.0216,\n",
      "        0.0088, 0.0021, 0.0087, 0.0779, 0.0329, 0.3030, 0.0146, 0.0121, 0.0005,\n",
      "        0.1331, 0.0634, 0.0149, 0.0347, 0.0222, 0.0144, 0.0058, 0.0131, 0.0069])\n",
      "\n",
      "This is index: 16 with row: tensor([0.0323, 0.1994, 0.0028, 0.0019, 0.0009, 0.1880, 0.0019, 0.0009, 0.1947,\n",
      "        0.0589, 0.0019, 0.0019, 0.0161, 0.0019, 0.0019, 0.0570, 0.0380, 0.0009,\n",
      "        0.1443, 0.0161, 0.0171, 0.0047, 0.0009, 0.0009, 0.0009, 0.0123, 0.0009])\n",
      "\n",
      "This is index: 17 with row: tensor([0.0970, 0.0468, 0.0033, 0.0033, 0.0033, 0.0067, 0.0033, 0.0033, 0.0033,\n",
      "        0.0468, 0.0033, 0.0033, 0.0067, 0.0100, 0.0033, 0.0100, 0.0033, 0.0033,\n",
      "        0.0067, 0.0100, 0.0033, 0.6923, 0.0033, 0.0134, 0.0033, 0.0033, 0.0033])\n",
      "\n",
      "This is index: 18 with row: tensor([0.1083, 0.1852, 0.0033, 0.0079, 0.0148, 0.1334, 0.0008, 0.0061, 0.0096,\n",
      "        0.2384, 0.0020, 0.0072, 0.0325, 0.0128, 0.0111, 0.0684, 0.0012, 0.0013,\n",
      "        0.0335, 0.0150, 0.0164, 0.0199, 0.0064, 0.0017, 0.0003, 0.0608, 0.0019])\n",
      "\n",
      "This is index: 19 with row: tensor([1.4386e-01, 1.4779e-01, 2.7050e-03, 7.5003e-03, 1.2296e-03, 1.0882e-01,\n",
      "        3.6887e-04, 3.6887e-04, 1.5812e-01, 8.4225e-02, 3.6887e-04, 1.0205e-02,\n",
      "        3.4428e-02, 1.1189e-02, 3.0739e-03, 6.5413e-02, 6.3937e-03, 2.4591e-04,\n",
      "        6.8855e-03, 5.6806e-02, 9.4184e-02, 2.2870e-02, 1.8443e-03, 3.0739e-03,\n",
      "        1.2296e-04, 2.6558e-02, 1.3525e-03])\n",
      "\n",
      "This is index: 20 with row: tensor([8.6475e-02, 1.8367e-01, 3.5733e-04, 3.2160e-03, 1.7867e-04, 1.2810e-01,\n",
      "        5.3600e-04, 5.3600e-04, 1.1578e-01, 9.5230e-02, 7.1467e-04, 1.7867e-04,\n",
      "        2.4120e-02, 8.9334e-04, 4.1093e-03, 1.1935e-01, 1.7867e-04, 1.7867e-04,\n",
      "        6.3070e-02, 6.4320e-03, 6.7000e-02, 1.4115e-02, 2.8587e-03, 2.1440e-03,\n",
      "        5.3600e-04, 6.1104e-02, 1.8939e-02])\n",
      "\n",
      "This is index: 21 with row: tensor([0.0493, 0.0519, 0.0329, 0.0329, 0.0433, 0.0538, 0.0063, 0.0152, 0.0187,\n",
      "        0.0386, 0.0047, 0.0297, 0.0955, 0.0490, 0.0873, 0.0035, 0.0054, 0.0035,\n",
      "        0.1312, 0.1502, 0.0262, 0.0013, 0.0120, 0.0275, 0.0111, 0.0044, 0.0145])\n",
      "\n",
      "This is index: 22 with row: tensor([0.0342, 0.2473, 0.0008, 0.0004, 0.0008, 0.2188, 0.0004, 0.0004, 0.0008,\n",
      "        0.3508, 0.0004, 0.0015, 0.0058, 0.0004, 0.0035, 0.0592, 0.0004, 0.0004,\n",
      "        0.0188, 0.0004, 0.0004, 0.0031, 0.0031, 0.0004, 0.0004, 0.0469, 0.0004])\n",
      "\n",
      "This is index: 23 with row: tensor([0.0544, 0.2939, 0.0021, 0.0010, 0.0094, 0.1569, 0.0031, 0.0021, 0.0251,\n",
      "        0.1559, 0.0010, 0.0073, 0.0146, 0.0031, 0.0617, 0.0387, 0.0010, 0.0010,\n",
      "        0.0241, 0.0220, 0.0094, 0.0272, 0.0010, 0.0031, 0.0010, 0.0774, 0.0021])\n",
      "\n",
      "This is index: 24 with row: tensor([0.2279, 0.1436, 0.0028, 0.0069, 0.0083, 0.0511, 0.0055, 0.0014, 0.0028,\n",
      "        0.1423, 0.0014, 0.0014, 0.0552, 0.0028, 0.0028, 0.0580, 0.0014, 0.0014,\n",
      "        0.0014, 0.0442, 0.0981, 0.0083, 0.0014, 0.0055, 0.0539, 0.0428, 0.0276])\n",
      "\n",
      "This is index: 25 with row: tensor([0.2048, 0.2187, 0.0029, 0.0118, 0.0278, 0.0308, 0.0013, 0.0032, 0.0023,\n",
      "        0.0197, 0.0024, 0.0089, 0.1127, 0.0152, 0.1864, 0.0277, 0.0016, 0.0007,\n",
      "        0.0298, 0.0410, 0.0107, 0.0145, 0.0109, 0.0005, 0.0030, 0.0024, 0.0081])\n",
      "\n",
      "This is index: 26 with row: tensor([0.0664, 0.3551, 0.0021, 0.0012, 0.0012, 0.1542, 0.0004, 0.0008, 0.0181,\n",
      "        0.1505, 0.0012, 0.0012, 0.0511, 0.0148, 0.0021, 0.0458, 0.0012, 0.0004,\n",
      "        0.0136, 0.0021, 0.0021, 0.0305, 0.0012, 0.0016, 0.0008, 0.0610, 0.0190])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in enumerate(P):\n",
    "    print(f\"This is index: {idx} with row: {row}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words[:3]:\n",
    "#for w in [\"andrejq\"]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    prob = P[ix1, ix2]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n += 1\n",
    "    print(f'For the bigram: ({ch1}, {ch2}) the probability is: {prob:.4f}, which gives a log_prob: {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 1))\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() # equivalent N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 27) @ (27, 27) -> (5, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY ------------------------------>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- !!! OPTIMIZATION !!! yay --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None # set to zero the gradient\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- !!! OPTIMIZATION !!! yay, but this time actually --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "for k in range(1):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
